{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957488d1",
   "metadata": {},
   "source": [
    "# Flow Field Reconstruction with 8 Edge Sensors\n",
    "\n",
    "This notebook implements a physics-informed machine learning approach to reconstruct flow fields using data from 8 edge sensors. The implementation uses:\n",
    "- Variational Autoencoder (VAE) for dimensionality reduction and flow field reconstruction\n",
    "- Fourier feature embeddings for coordinate information\n",
    "- FLRNet architecture to predict flow fields from sparse sensor measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ded4a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll import the necessary libraries for data manipulation, visualization, and deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e48aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n",
      "Found 1 GPU(s): [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Suppress TensorFlow warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Force reload of modules\n",
    "if 'models_improved' in sys.modules:\n",
    "    importlib.reload(sys.modules['models_improved'])\n",
    "if 'config_manager' in sys.modules:\n",
    "    importlib.reload(sys.modules['config_manager'])\n",
    "if 'data.flow_field_dataset' in sys.modules:\n",
    "    importlib.reload(sys.modules['data.flow_field_dataset'])\n",
    "\n",
    "# Now import everything\n",
    "import models_improved\n",
    "import config_manager\n",
    "from data.flow_field_dataset import FlowFieldDatasetCreator\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Configure GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Found {len(gpus)} GPU(s): {gpus}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"Using GPU: {gpu}\")\n",
    "        # Enable memory growth to avoid allocation issues\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPUs found, using CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22807b8f",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup\n",
    "\n",
    "Let's set up the configuration for our model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "994c56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Configuration Summary\n",
      "==================================================\n",
      "üèóÔ∏è  Model Architecture:\n",
      "   - Fourier Enhancement: True\n",
      "   - Perceptual Loss: True\n",
      "   - Input Shape: [128, 256, 1]\n",
      "   - Latent Dimensions: 8\n",
      "   - Base Features: 64\n",
      "\n",
      "üì° Sensor Configuration:\n",
      "   - Layout: random\n",
      "   - Number of Sensors: 8\n",
      "   - Dataset: data/datasets\\dataset_random_8.npz\n",
      "\n",
      "üöÄ Training Parameters:\n",
      "   - VAE Epochs: 250\n",
      "   - FLRNet Epochs: 150\n",
      "   - VAE Learning Rate: 0.0001\n",
      "   - FLRNet Learning Rate: 0.0001\n",
      "   - Batch Size: 8\n",
      "   - Test Split: 0.2\n",
      "\n",
      "üíæ Output Configuration:\n",
      "   - Model Name: fourierTrue_percepTrue_random_8\n",
      "   - Checkpoints: ./checkpoints\\fourierTrue_percepTrue_random_8\n",
      "   - Logs: ./logs\\fourierTrue_percepTrue_random_8\n",
      "   - Save Best Model: True\n",
      "   - Save Last Model: True\n",
      "\n",
      "\n",
      "üîß Final Configuration (lowercase keys):\n",
      "   Model name: fourierTrue_percepTrue_random_8\n",
      "   Use Fourier: True\n",
      "   Use perceptual loss: True\n",
      "   Input shape: (128, 256, 1)\n",
      "   Number of sensors: 8\n",
      "   Latent dims: 8\n",
      "   Base features: 64\n",
      "   Batch size: 8\n",
      "   VAE epochs: 250\n",
      "   FLRNet epochs: 150\n",
      "   VAE learning rate: 0.0001\n",
      "   FLRNet learning rate: 0.0001\n",
      "   Dataset path: data/datasets\\dataset_random_8.npz\n",
      "   Checkpoint dir: ./checkpoints\\fourierTrue_percepTrue_random_8\n",
      "   Logs dir: ./logs\\fourierTrue_percepTrue_random_8\n",
      "‚úÖ All required configuration keys are present\n"
     ]
    }
   ],
   "source": [
    "# Load configuration using ConfigManager\n",
    "config_name = \"random_8_fourier\"\n",
    "config_mgr = config_manager.ConfigManager()\n",
    "hierarchical_config = config_mgr.load_config(config_name)\n",
    "\n",
    "# Print configuration summary\n",
    "print(config_mgr.create_config_summary(hierarchical_config))\n",
    "\n",
    "# Flatten the config for training (with lowercase keys for Python style)\n",
    "flattened_config = config_manager.flatten_config_for_training(hierarchical_config)\n",
    "\n",
    "# Convert to lowercase keys for consistent Python style\n",
    "config = {}\n",
    "for key, value in flattened_config.items():\n",
    "    config[key.lower()] = value\n",
    "\n",
    "print(\"\\nüîß Final Configuration (lowercase keys):\")\n",
    "print(f\"   Model name: {config['model_name']}\")\n",
    "print(f\"   Use Fourier: {config['use_fourier']}\")\n",
    "print(f\"   Use perceptual loss: {config['use_perceptual_loss']}\")\n",
    "print(f\"   Input shape: {config['input_shape']}\")\n",
    "print(f\"   Number of sensors: {config['n_sensors']}\")\n",
    "print(f\"   Latent dims: {config['latent_dims']}\")\n",
    "print(f\"   Base features: {config['n_base_features']}\")\n",
    "print(f\"   Batch size: {config['batch_size']}\")\n",
    "print(f\"   VAE epochs: {config['vae_epochs']}\")\n",
    "print(f\"   FLRNet epochs: {config['flr_epochs']}\")\n",
    "print(f\"   VAE learning rate: {config['vae_learning_rate']}\")\n",
    "print(f\"   FLRNet learning rate: {config['flr_learning_rate']}\")\n",
    "print(f\"   Dataset path: {config['dataset_path']}\")\n",
    "print(f\"   Checkpoint dir: {config['checkpoint_dir']}\")\n",
    "print(f\"   Logs dir: {config['logs_dir']}\")\n",
    "\n",
    "# Verify essential config keys exist\n",
    "required_keys = ['model_name', 'use_fourier', 'use_perceptual_loss', 'input_shape', \n",
    "                 'n_sensors', 'latent_dims', 'n_base_features', 'batch_size', \n",
    "                 'vae_epochs', 'flr_epochs', 'vae_learning_rate', 'flr_learning_rate',\n",
    "                 'dataset_path', 'checkpoint_dir', 'logs_dir']\n",
    "missing_keys = [key for key in required_keys if key not in config]\n",
    "if missing_keys:\n",
    "    print(f\"‚ö†Ô∏è  Missing required config keys: {missing_keys}\")\n",
    "else:\n",
    "    print(\"‚úÖ All required configuration keys are present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c2fdc",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset\n",
    "\n",
    "We'll load the flow field dataset and the sensor layout for 8 edge sensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f51841e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from: data/datasets\\dataset_random_8.npz\n",
      "üìä Dataset parameters:\n",
      "   Layout type: random\n",
      "   Number of sensors: 8\n",
      "üìÅ Loading dataset file: data/datasets\\dataset_random_8.npz\n",
      "üìã Available keys in dataset: ['sensor_data', 'field_data', 'sensor_positions', 'reynolds_numbers', 'layout_type', 'n_sensors']\n",
      "üìä Dataset loaded successfully:\n",
      "   sensor_data: (28, 8, 39) (float64)\n",
      "   field_data: (28, 128, 256, 39) (float64)\n",
      "   sensor_positions: (8, 2) (float64)\n",
      "   reynolds_numbers: (28,) (int32)\n",
      "   layout_type: () (<U6)\n",
      "   n_sensors: () (int32)\n",
      "üìç Sensor positions shape: (8, 2)\n",
      "üìä Dataset loaded successfully:\n",
      "   sensor_data: (28, 8, 39) (float64)\n",
      "   field_data: (28, 128, 256, 39) (float64)\n",
      "   sensor_positions: (8, 2) (float64)\n",
      "   reynolds_numbers: (28,) (int32)\n",
      "   layout_type: () (<U6)\n",
      "   n_sensors: () (int32)\n",
      "üìç Sensor positions shape: (8, 2)\n",
      "Dataset reshaped:\n",
      "  Original sensor data: (28, 8, 39)\n",
      "  Reshaped sensor data: (1092, 8)\n",
      "  Original field data: (28, 128, 256, 39)\n",
      "  Reshaped field data: (1092, 128, 256, 1)\n",
      "  Total samples: 1092\n",
      "Dataset reshaped:\n",
      "  Original sensor data: (28, 8, 39)\n",
      "  Reshaped sensor data: (1092, 8)\n",
      "  Original field data: (28, 128, 256, 39)\n",
      "  Reshaped field data: (1092, 128, 256, 1)\n",
      "  Total samples: 1092\n",
      "TensorFlow datasets created:\n",
      "  Train samples: 873\n",
      "  Test samples: 219\n",
      "\n",
      "üìä TensorFlow datasets created:\n",
      "   Train dataset: <ShuffleDataset element_spec={'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None)}>\n",
      "   Test dataset: <BatchDataset element_spec={'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None)}>\n",
      "TensorFlow datasets created:\n",
      "  Train samples: 873\n",
      "  Test samples: 219\n",
      "\n",
      "üìä TensorFlow datasets created:\n",
      "   Train dataset: <ShuffleDataset element_spec={'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None)}>\n",
      "   Test dataset: <BatchDataset element_spec={'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None)}>\n",
      "\n",
      "üìä Coordinate-aware datasets created:\n",
      "   Train dataset with coordinates: <MapDataset element_spec={'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), 'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'coordinates': TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)}>\n",
      "   Test dataset with coordinates: <MapDataset element_spec={'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), 'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'coordinates': TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)}>\n",
      "\n",
      "üåä Creating Fourier-aware VAE datasets...\n",
      "‚úÖ Fourier-aware VAE datasets created\n",
      "\n",
      "üåä Creating Fourier-aware FLRNet datasets...\n",
      "‚úÖ Fourier-aware FLRNet datasets created\n",
      "\n",
      "üìä Specialized datasets:\n",
      "   VAE train dataset: <MapDataset element_spec=((TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   VAE test dataset: <MapDataset element_spec=((TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   FLRNet train dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None))>\n",
      "   FLRNet test dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None))>\n",
      "\n",
      "üìä Data shape verification:\n",
      "\n",
      "üìä Coordinate-aware datasets created:\n",
      "   Train dataset with coordinates: <MapDataset element_spec={'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), 'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'coordinates': TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)}>\n",
      "   Test dataset with coordinates: <MapDataset element_spec={'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), 'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'coordinates': TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)}>\n",
      "\n",
      "üåä Creating Fourier-aware VAE datasets...\n",
      "‚úÖ Fourier-aware VAE datasets created\n",
      "\n",
      "üåä Creating Fourier-aware FLRNet datasets...\n",
      "‚úÖ Fourier-aware FLRNet datasets created\n",
      "\n",
      "üìä Specialized datasets:\n",
      "   VAE train dataset: <MapDataset element_spec=((TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   VAE test dataset: <MapDataset element_spec=((TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   FLRNet train dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None))>\n",
      "   FLRNet test dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None))>\n",
      "\n",
      "üìä Data shape verification:\n",
      "   Sensor data shape: (8, 8)\n",
      "   Field data shape: (8, 128, 256, 1)\n",
      "   Coordinates shape: (8, 128, 256, 2)\n",
      "\n",
      "üìä VAE dataset structure verification:\n",
      "   Sensor data shape: (8, 8)\n",
      "   Field data shape: (8, 128, 256, 1)\n",
      "   Coordinates shape: (8, 128, 256, 2)\n",
      "\n",
      "üìä VAE dataset structure verification:\n",
      "   VAE field input shape: (8, 128, 256, 1)\n",
      "   VAE coordinate input shape: (8, 128, 256, 2)\n",
      "   VAE target shape: (8, 128, 256, 1)\n",
      "   VAE field input shape: (8, 128, 256, 1)\n",
      "   VAE coordinate input shape: (8, 128, 256, 2)\n",
      "   VAE target shape: (8, 128, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare dataset created from data_creation_and_viz.ipynb\n",
    "print(f\"üìÇ Loading dataset from: {config['dataset_path']}\")\n",
    "\n",
    "# Parse the dataset filename to get layout and n_sensors\n",
    "dataset_filename = Path(config['dataset_path']).name\n",
    "# Expected format: dataset_edge_8.npz\n",
    "parts = dataset_filename.split('_')\n",
    "layout_type = parts[1]  # 'edge'\n",
    "n_sensors = int(parts[2].split('.')[0])  # 8\n",
    "\n",
    "print(f\"üìä Dataset parameters:\")\n",
    "print(f\"   Layout type: {layout_type}\")\n",
    "print(f\"   Number of sensors: {n_sensors}\")\n",
    "\n",
    "# Load the dataset directly from the NPZ file\n",
    "print(f\"üìÅ Loading dataset file: {config['dataset_path']}\")\n",
    "data = np.load(config['dataset_path'])\n",
    "\n",
    "# Check what keys are available in the dataset\n",
    "print(f\"üìã Available keys in dataset: {list(data.keys())}\")\n",
    "\n",
    "# Create dataset dictionary\n",
    "dataset = {key: data[key] for key in data.keys()}\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"üìä Dataset loaded successfully:\")\n",
    "for key, value in dataset.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"   {key}: {value.shape} ({value.dtype})\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "# Extract sensor positions\n",
    "sensor_positions = dataset['sensor_positions']\n",
    "print(f\"üìç Sensor positions shape: {sensor_positions.shape}\")\n",
    "\n",
    "# Create dataset creator instance for TensorFlow dataset creation\n",
    "creator = FlowFieldDatasetCreator(\n",
    "    output_path=\"./data/\",\n",
    "    domain_shape=config['input_shape'][:2],  # (height, width)\n",
    "    use_synthetic_data=False  # Don't create synthetic data, just use for TF dataset creation\n",
    ")\n",
    "\n",
    "# Create TensorFlow datasets using the creator's method\n",
    "train_dataset, test_dataset = creator.create_tensorflow_dataset(\n",
    "    dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    test_split=config['test_split']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä TensorFlow datasets created:\")\n",
    "print(f\"   Train dataset: {train_dataset}\")\n",
    "print(f\"   Test dataset: {test_dataset}\")\n",
    "\n",
    "# Function to add coordinate grids to field data for Fourier-aware VAE training\n",
    "def add_coordinate_grid(batch):\n",
    "    \"\"\"Add coordinate grid to field data for Fourier VAE training\"\"\"\n",
    "    field_data = batch['field_data']\n",
    "    \n",
    "    # Get dimensions\n",
    "    batch_size = tf.shape(field_data)[0]\n",
    "    height = tf.shape(field_data)[1]\n",
    "    width = tf.shape(field_data)[2]\n",
    "    \n",
    "    # Create normalized coordinate grids [0, 1] - match field dimensions\n",
    "    x_coords = tf.linspace(0.0, 1.0, width)   # Width corresponds to x\n",
    "    y_coords = tf.linspace(0.0, 1.0, height)  # Height corresponds to y\n",
    "    \n",
    "    # Create meshgrid to match image indexing: [height, width, 2]\n",
    "    y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    # Stack to create coordinate grid (height, width, 2)\n",
    "    coord_grid = tf.stack([x_grid, y_grid], axis=-1)\n",
    "    \n",
    "    # Expand to batch size (batch_size, height, width, 2)\n",
    "    coord_batch = tf.tile(tf.expand_dims(coord_grid, 0), [batch_size, 1, 1, 1])\n",
    "    \n",
    "    # Update batch to include coordinates\n",
    "    return {\n",
    "        'field_data': field_data,\n",
    "        'sensor_data': batch['sensor_data'],\n",
    "        'coordinates': coord_batch\n",
    "    }\n",
    "\n",
    "# Add coordinate grids to datasets\n",
    "coord_train_dataset = train_dataset.map(add_coordinate_grid)\n",
    "coord_test_dataset = test_dataset.map(add_coordinate_grid)\n",
    "\n",
    "print(f\"\\nüìä Coordinate-aware datasets created:\")\n",
    "print(f\"   Train dataset with coordinates: {coord_train_dataset}\")\n",
    "print(f\"   Test dataset with coordinates: {coord_test_dataset}\")\n",
    "\n",
    "# Create specialized datasets for VAE and FLRNet training\n",
    "# VAE trains on field reconstruction WITH coordinates for Fourier features\n",
    "if config['use_fourier']:\n",
    "    print(\"\\nüåä Creating Fourier-aware VAE datasets...\")\n",
    "    # For Fourier VAE: input = (field, coordinates), output = field\n",
    "    vae_train_dataset = coord_train_dataset.map(\n",
    "        lambda batch: ((batch['field_data'], batch['coordinates']), batch['field_data'])\n",
    "    )\n",
    "    vae_test_dataset = coord_test_dataset.map(\n",
    "        lambda batch: ((batch['field_data'], batch['coordinates']), batch['field_data'])\n",
    "    )\n",
    "    print(\"‚úÖ Fourier-aware VAE datasets created\")\n",
    "else:\n",
    "    print(\"\\nüîÑ Creating standard VAE datasets...\")\n",
    "    # Standard VAE: input = field, output = field\n",
    "    vae_train_dataset = coord_train_dataset.map(\n",
    "        lambda batch: (batch['field_data'], batch['field_data'])\n",
    "    )\n",
    "    vae_test_dataset = coord_test_dataset.map(\n",
    "        lambda batch: (batch['field_data'], batch['field_data'])\n",
    "    )\n",
    "    print(\"‚úÖ Standard VAE datasets created\")\n",
    "\n",
    "# FLRNet trains on sensor-to-field reconstruction \n",
    "if config['use_fourier']:\n",
    "    print(\"\\nüåä Creating Fourier-aware FLRNet datasets...\")\n",
    "    # For Fourier FLRNet: input = (sensor, field, coordinates), output = field\n",
    "    flrnet_train_dataset = coord_train_dataset.map(\n",
    "        lambda batch: (batch['sensor_data'], batch['field_data'], batch['coordinates'])\n",
    "    )\n",
    "    flrnet_test_dataset = coord_test_dataset.map(\n",
    "        lambda batch: (batch['sensor_data'], batch['field_data'], batch['coordinates'])\n",
    "    )\n",
    "    print(\"‚úÖ Fourier-aware FLRNet datasets created\")\n",
    "else:\n",
    "    print(\"\\nüîÑ Creating standard FLRNet datasets...\")\n",
    "    # Standard FLRNet: input = (sensor, field), output = field\n",
    "    flrnet_train_dataset = coord_train_dataset.map(\n",
    "        lambda batch: (batch['sensor_data'], batch['field_data'])\n",
    "    )\n",
    "    flrnet_test_dataset = coord_test_dataset.map(\n",
    "        lambda batch: (batch['sensor_data'], batch['field_data'])\n",
    "    )\n",
    "    print(\"‚úÖ Standard FLRNet datasets created\")\n",
    "\n",
    "print(f\"\\nüìä Specialized datasets:\")\n",
    "print(f\"   VAE train dataset: {vae_train_dataset}\")\n",
    "print(f\"   VAE test dataset: {vae_test_dataset}\")\n",
    "print(f\"   FLRNet train dataset: {flrnet_train_dataset}\")\n",
    "print(f\"   FLRNet test dataset: {flrnet_test_dataset}\")\n",
    "\n",
    "# Get a sample to verify data shapes\n",
    "print(f\"\\nüìä Data shape verification:\")\n",
    "for batch in coord_train_dataset.take(1):\n",
    "    print(f\"   Sensor data shape: {batch['sensor_data'].shape}\")\n",
    "    print(f\"   Field data shape: {batch['field_data'].shape}\")\n",
    "    print(f\"   Coordinates shape: {batch['coordinates'].shape}\")\n",
    "    break\n",
    "\n",
    "# Verify VAE dataset structure\n",
    "print(f\"\\nüìä VAE dataset structure verification:\")\n",
    "for vae_batch in vae_train_dataset.take(1):\n",
    "    if config['use_fourier']:\n",
    "        inputs, targets = vae_batch\n",
    "        if isinstance(inputs, tuple):\n",
    "            field_input, coord_input = inputs\n",
    "            print(f\"   VAE field input shape: {field_input.shape}\")\n",
    "            print(f\"   VAE coordinate input shape: {coord_input.shape}\")\n",
    "            print(f\"   VAE target shape: {targets.shape}\")\n",
    "        else:\n",
    "            print(f\"   Unexpected VAE input format: {type(inputs)}\")\n",
    "    else:\n",
    "        inputs, targets = vae_batch\n",
    "        print(f\"   VAE input shape: {inputs.shape}\")\n",
    "        print(f\"   VAE target shape: {targets.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61349511",
   "metadata": {},
   "source": [
    "## 5. Train VAE and FLRNet Models\n",
    "\n",
    "Now we'll train our models using the FLRTrainer:\n",
    "1. First, the Variational Autoencoder (VAE) to learn a compressed representation of flow fields\n",
    "2. Then, the FLRNet to reconstruct flow fields from sparse sensor measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "566a6830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Clearing TensorFlow session...\n",
      "üöÄ Initializing FLRTrainer...\n",
      "‚úÖ FLRTrainer initialized:\n",
      "   Input shape: (128, 256, 1)\n",
      "   Use Fourier: True\n",
      "   Model name: fourierTrue_percepTrue_random_8\n",
      "   Gradient clipping: 2.0\n",
      "   Checkpoints: ./checkpoints\\fourierTrue_percepTrue_random_8\n",
      "   Logs: ./logs\\fourierTrue_percepTrue_random_8\n",
      "   Save best model: True\n",
      "   Save last model: True\n",
      "\n",
      "üîß Training configuration:\n",
      "   Train VAE: True\n",
      "   Train FLRNet: True\n",
      "\n",
      "üîç Verifying dataset shapes...\n",
      "   Field data shape: (8, 128, 256, 1)\n",
      "   Coordinates shape: (8, 128, 256, 2)\n",
      "   Field data shape: (8, 128, 256, 1)\n",
      "   Coordinates shape: (8, 128, 256, 2)\n",
      "   VAE field input shape: (8, 128, 256, 1)\n",
      "   VAE coordinate input shape: (8, 128, 256, 2)\n",
      "   VAE target shape: (8, 128, 256, 1)\n",
      "   ‚úÖ Coordinate shapes now match field shapes!\n",
      "   VAE field input shape: (8, 128, 256, 1)\n",
      "   VAE coordinate input shape: (8, 128, 256, 2)\n",
      "   VAE target shape: (8, 128, 256, 1)\n",
      "   ‚úÖ Coordinate shapes now match field shapes!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# Clear TensorFlow session to fix any shape mismatches\n",
    "print(\"üîÑ Clearing TensorFlow session...\")\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Initialize FLRTrainer from models_improved.py\n",
    "print(\"üöÄ Initializing FLRTrainer...\")\n",
    "\n",
    "# Create the FLRTrainer instance\n",
    "trainer = models_improved.FLRTrainer(\n",
    "    input_shape=config['input_shape'],\n",
    "    use_fourier=config['use_fourier'],\n",
    "    checkpoint_dir=config['checkpoint_dir'],\n",
    "    logs_dir=config['logs_dir'],\n",
    "    model_name=config['model_name'],\n",
    "    save_best_model=config['save_best_model'],\n",
    "    save_last_model=config['save_last_model'],\n",
    "    gradient_clip_norm=config['gradient_clip_norm']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ FLRTrainer initialized:\")\n",
    "print(f\"   Input shape: {config['input_shape']}\")\n",
    "print(f\"   Use Fourier: {config['use_fourier']}\")\n",
    "print(f\"   Model name: {config['model_name']}\")\n",
    "print(f\"   Gradient clipping: {config['gradient_clip_norm']}\")\n",
    "print(f\"   Checkpoints: {config['checkpoint_dir']}\")\n",
    "print(f\"   Logs: {config['logs_dir']}\")\n",
    "print(f\"   Save best model: {config['save_best_model']}\")\n",
    "print(f\"   Save last model: {config['save_last_model']}\")\n",
    "\n",
    "# Set training flags\n",
    "train_vae_model = True  # Set to False if you want to skip VAE training\n",
    "train_flrnet_model = True  # Set to True to test FLRNet with proper Fourier features\n",
    "\n",
    "print(f\"\\nüîß Training configuration:\")\n",
    "print(f\"   Train VAE: {train_vae_model}\")\n",
    "print(f\"   Train FLRNet: {train_flrnet_model}\")\n",
    "\n",
    "# Quick test to verify dataset shapes are correct\n",
    "print(f\"\\nüîç Verifying dataset shapes...\")\n",
    "for batch in coord_train_dataset.take(1):\n",
    "    print(f\"   Field data shape: {batch['field_data'].shape}\")\n",
    "    print(f\"   Coordinates shape: {batch['coordinates'].shape}\")\n",
    "    break\n",
    "\n",
    "# Test VAE dataset format\n",
    "for vae_batch in vae_train_dataset.take(1):\n",
    "    inputs, targets = vae_batch\n",
    "    if isinstance(inputs, tuple):\n",
    "        field_input, coord_input = inputs\n",
    "        print(f\"   VAE field input shape: {field_input.shape}\")\n",
    "        print(f\"   VAE coordinate input shape: {coord_input.shape}\")\n",
    "        print(f\"   VAE target shape: {targets.shape}\")\n",
    "        \n",
    "        # Check if shapes match now\n",
    "        if field_input.shape[1:3] == coord_input.shape[1:3]:\n",
    "            print(f\"   ‚úÖ Coordinate shapes now match field shapes!\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Shape mismatch: field {field_input.shape[1:3]} vs coord {coord_input.shape[1:3]}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50931c46",
   "metadata": {},
   "source": [
    "## Load Trained Models from Checkpoint\n",
    "\n",
    "Now let's load the trained models from saved checkpoints using the FLRTrainer's built-in methods. The trainer provides several loading options:\n",
    "\n",
    "- **`load_vae_from_checkpoint()`**: Load only the VAE model\n",
    "- **`load_flrnet_from_checkpoint()`**: Load only the FLRNet model  \n",
    "- **`load_models_from_checkpoint()`**: Load both models at once\n",
    "\n",
    "### Features:\n",
    "- **Automatic checkpoint detection**: Finds the best available checkpoint (best ‚Üí last ‚Üí final_weights)\n",
    "- **Robust path handling**: Uses correct TensorFlow checkpoint format (no file extensions)\n",
    "- **Error handling**: Graceful fallback and clear error messages\n",
    "- **Architecture consistency**: Ensures loaded model matches trainer configuration\n",
    "- **Smart dependencies**: FLRNet loading automatically handles VAE dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bee5726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading trained VAE model from checkpoint using FLRTrainer...\n",
      "============================================================\n",
      "üìÇ Using checkpoint directory: checkpoints\\fourierTrue_percepTrue_random_8\n",
      "üìã Available checkpoint files:\n",
      "   - checkpoint\n",
      "   - checkpoint_fourierTrue_percepTrue_random_8_vae_best.data-00000-of-00001\n",
      "   - checkpoint_fourierTrue_percepTrue_random_8_vae_best.index\n",
      "üîç Loading VAE model from checkpoint directory: checkpoints\\fourierTrue_percepTrue_random_8\n",
      "‚úÖ Found vae_best checkpoint: checkpoints\\fourierTrue_percepTrue_random_8\\checkpoint_fourierTrue_percepTrue_random_8_vae_best\n",
      "üåä VAE model built for Fourier features\n",
      "üìã VAE Model Architecture:\n",
      "   - Input shape: (128, 256, 1)\n",
      "   - Latent dims: 8\n",
      "   - Base features: 64\n",
      "   - Use Fourier: True\n",
      "   - Perceptual loss: True\n",
      "üåä VAE model built for Fourier features\n",
      "üìã VAE Model Architecture:\n",
      "   - Input shape: (128, 256, 1)\n",
      "   - Latent dims: 8\n",
      "   - Base features: 64\n",
      "   - Use Fourier: True\n",
      "   - Perceptual loss: True\n",
      "‚úÖ Successfully loaded VAE model from vae_best checkpoint!\n",
      "\n",
      "üéâ VAE model loaded successfully!\n",
      "üìà Model is ready for inference and visualization\n",
      "‚úÖ Fourier VAE verification successful!\n",
      "   - Field input shape: (1, 128, 256, 1)\n",
      "   - Coordinate input shape: (1, 128, 256, 2)\n",
      "   - Output shape: (1, 128, 256, 1)\n",
      "‚úÖ Successfully loaded VAE model from vae_best checkpoint!\n",
      "\n",
      "üéâ VAE model loaded successfully!\n",
      "üìà Model is ready for inference and visualization\n",
      "‚úÖ Fourier VAE verification successful!\n",
      "   - Field input shape: (1, 128, 256, 1)\n",
      "   - Coordinate input shape: (1, 128, 256, 2)\n",
      "   - Output shape: (1, 128, 256, 1)\n",
      "   - Output range: [-0.0591, 1.8878]\n",
      "   - Output range: [-0.0591, 1.8878]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the VAE model using the trainer's built-in method\n",
    "print(\"üöÄ Loading trained VAE model from checkpoint using FLRTrainer...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fix checkpoint path - use the directory, not the specific file\n",
    "checkpoint_directory = Path(config['checkpoint_dir'])\n",
    "print(f\"üìÇ Using checkpoint directory: {checkpoint_directory}\")\n",
    "\n",
    "# List available checkpoints for debugging\n",
    "if checkpoint_directory.exists():\n",
    "    print(f\"üìã Available checkpoint files:\")\n",
    "    for file in sorted(checkpoint_directory.iterdir()):\n",
    "        print(f\"   - {file.name}\")\n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint directory not found: {checkpoint_directory}\")\n",
    "\n",
    "# Use the trainer's built-in checkpoint loading method\n",
    "vae_model = trainer.load_vae_from_checkpoint(\n",
    "    checkpoint_dir=checkpoint_directory,\n",
    "    latent_dims=config['latent_dims'],\n",
    "    n_base_features=config['n_base_features'],\n",
    "    use_perceptual_loss=config['use_perceptual_loss'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if vae_model is not None:\n",
    "    print(\"\\nüéâ VAE model loaded successfully!\")\n",
    "    print(\"üìà Model is ready for inference and visualization\")\n",
    "    \n",
    "    # Verify the model works with a test prediction\n",
    "    try:\n",
    "        # Get a small batch for testing\n",
    "        test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "        test_inputs, test_targets = test_batch\n",
    "        \n",
    "        # Handle different input formats (Fourier vs Standard VAE)\n",
    "        if isinstance(test_inputs, (list, tuple)) and len(test_inputs) == 2:\n",
    "            # Fourier VAE expects [field, coordinates]\n",
    "            test_field, test_coord = test_inputs\n",
    "            test_prediction = vae_model([test_field[:1], test_coord[:1]])\n",
    "            print(f\"‚úÖ Fourier VAE verification successful!\")\n",
    "            print(f\"   - Field input shape: {test_field[:1].shape}\")\n",
    "            print(f\"   - Coordinate input shape: {test_coord[:1].shape}\")\n",
    "        else:\n",
    "            # Standard VAE expects just field data\n",
    "            test_prediction = vae_model(test_inputs[:1])\n",
    "            print(f\"‚úÖ Standard VAE verification successful!\")\n",
    "            print(f\"   - Input shape: {test_inputs[:1].shape}\")\n",
    "        \n",
    "        print(f\"   - Output shape: {test_prediction.shape}\")\n",
    "        print(f\"   - Output range: [{test_prediction.numpy().min():.4f}, {test_prediction.numpy().max():.4f}]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Model verification failed: {str(e)}\")\n",
    "        print(\"Model loaded but may have compatibility issues\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to load VAE model\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Check if training completed successfully\")\n",
    "    print(\"   2. Verify checkpoint files exist in the directory\")\n",
    "    print(\"   3. Ensure model configuration matches training setup\")\n",
    "    print(\"   4. Try running the training cell again if checkpoints are missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "687601d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Starting FLRNet Training using FLRTrainer...\n",
      "============================================================\n",
      "üöÄ Training FLRNet Model...\n",
      "üõ°Ô∏è Checkpoint saving will be disabled for epochs 1-30, enabled from epoch 31\n",
      "üõ°Ô∏è Checkpoint saving will be disabled for epochs 1-30, enabled from epoch 31\n",
      "üõ°Ô∏è Checkpoint saving will be disabled for epochs 1-30, enabled from epoch 31\n",
      "üõ°Ô∏è Checkpoint saving will be disabled for epochs 1-30, enabled from epoch 31\n",
      "Epoch 1/150\n",
      "Epoch 1/150\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.z_mean_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.z_mean_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.z_log_var_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.z_log_var_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.conv_in.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.conv_in.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.conv_out.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.conv_out.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block1.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block1.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block1.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block1.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block2.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block2.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block2.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.z_mean_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.z_mean_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.z_log_var_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.z_log_var_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.conv_in.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.conv_in.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.conv_out.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.conv_out.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block1.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block1.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block1.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block1.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block2.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block2.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block2.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block2.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block3.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block3.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block3.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block3.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block4.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block4.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block4.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block4.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block5.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block5.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block5.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block5.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block1.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block1.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block2.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block3.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block3.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block3.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block3.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block4.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block4.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block4.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block4.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block5.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block5.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block5.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block5.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block1.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block1.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block1.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block1.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block2.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block2.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block2.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block2.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block3.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block3.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block3.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block3.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block4.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block4.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block4.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block4.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block5.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block5.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block5.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block5.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.z_mean_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.z_mean_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block1.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block1.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block2.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block2.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block2.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block2.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block3.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block3.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block3.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block3.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block4.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block4.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block4.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block4.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block5.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block5.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block5.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block5.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.z_mean_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.z_mean_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.z_log_var_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.z_log_var_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.conv_in.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.conv_in.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.conv_out.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.conv_out.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block1.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block1.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block1.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block1.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block2.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block2.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block2.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block2.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block3.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block3.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block3.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block3.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block4.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block4.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block4.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.z_log_var_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.z_log_var_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.conv_in.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.conv_in.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.conv_out.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.conv_out.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block1.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block1.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block1.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block1.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block2.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block2.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block2.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block2.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block3.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block3.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block3.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block3.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block4.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block4.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block4.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block4.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block5.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block5.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block5.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block5.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block1.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block1.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block1.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block1.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block2.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block2.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block2.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block2.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block3.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block3.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block3.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block3.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block4.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block5.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block5.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block5.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block5.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block1.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block1.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block1.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block1.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block2.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block2.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block2.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block2.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block3.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block3.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block3.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block3.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block4.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block4.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block4.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block4.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block5.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block5.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block5.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block5.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block4.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block4.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block4.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block4.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block5.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block5.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block5.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block5.conv1x1_layers.0.bias\n",
      "110/110 [==============================] - ETA: 0s - loss: 389716.7599 - reconstruction_loss: 51465.5039 - kl_loss: 261885.3125 - perceptual_loss: 27.0893Epoch 1: val_reconstruction_loss improved from inf to 15008.85742 (saving disabled for epochs 1-30)\n",
      "Epoch 1: val_reconstruction_loss improved from inf to 15008.85742 (saving disabled for epochs 1-30)\n",
      "Epoch 1: val_reconstruction_loss improved from inf to 15008.85742 (saving disabled for epochs 1-30)\n",
      "Epoch 1: val_reconstruction_loss improved from inf to 15008.85742 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 28s 191ms/step - loss: 389029.0236 - reconstruction_loss: 51465.5039 - kl_loss: 261885.3125 - perceptual_loss: 27.0893 - val_loss: 173379.5156 - val_reconstruction_loss: 15008.8574 - val_kl_loss: 79181.2422 - val_perceptual_loss: 8.1727 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 28s 191ms/step - loss: 389029.0236 - reconstruction_loss: 51465.5039 - kl_loss: 261885.3125 - perceptual_loss: 27.0893 - val_loss: 173379.5156 - val_reconstruction_loss: 15008.8574 - val_kl_loss: 79181.2422 - val_perceptual_loss: 8.1727 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "Epoch 2/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 226882.1401 - reconstruction_loss: 41916.0117 - kl_loss: 198905.6875 - perceptual_loss: 22.0080Epoch 2: val_reconstruction_loss improved from 15008.85742 to 14591.94922 (saving disabled for epochs 1-30)\n",
      "Epoch 2: val_reconstruction_loss improved from 15008.85742 to 14591.94922 (saving disabled for epochs 1-30)\n",
      "Epoch 2: val_reconstruction_loss improved from 15008.85742 to 14591.94922 (saving disabled for epochs 1-30)\n",
      "Epoch 2: val_reconstruction_loss improved from 15008.85742 to 14591.94922 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 20s 175ms/step - loss: 227007.9206 - reconstruction_loss: 41916.0117 - kl_loss: 198905.6875 - perceptual_loss: 22.0080 - val_loss: 148520.8750 - val_reconstruction_loss: 14591.9492 - val_kl_loss: 66960.2578 - val_perceptual_loss: 8.4095 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 20s 175ms/step - loss: 227007.9206 - reconstruction_loss: 41916.0117 - kl_loss: 198905.6875 - perceptual_loss: 22.0080 - val_loss: 148520.8750 - val_reconstruction_loss: 14591.9492 - val_kl_loss: 66960.2578 - val_perceptual_loss: 8.4095 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "Epoch 3/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 233186.1293 - reconstruction_loss: 40980.0391 - kl_loss: 191678.5781 - perceptual_loss: 21.1684Epoch 3: val_reconstruction_loss improved from 14591.94922 to 14589.81836 (saving disabled for epochs 1-30)\n",
      "Epoch 3: val_reconstruction_loss improved from 14591.94922 to 14589.81836 (saving disabled for epochs 1-30)\n",
      "Epoch 3: val_reconstruction_loss improved from 14591.94922 to 14589.81836 (saving disabled for epochs 1-30)\n",
      "Epoch 3: val_reconstruction_loss improved from 14591.94922 to 14589.81836 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 19s 170ms/step - loss: 233181.5681 - reconstruction_loss: 40980.0391 - kl_loss: 191678.5781 - perceptual_loss: 21.1684 - val_loss: 139787.8906 - val_reconstruction_loss: 14589.8184 - val_kl_loss: 62595.0195 - val_perceptual_loss: 8.0407 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 19s 170ms/step - loss: 233181.5681 - reconstruction_loss: 40980.0391 - kl_loss: 191678.5781 - perceptual_loss: 21.1684 - val_loss: 139787.8906 - val_reconstruction_loss: 14589.8184 - val_kl_loss: 62595.0195 - val_perceptual_loss: 8.0407 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "Epoch 4/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 216980.3844 - reconstruction_loss: 39189.2812 - kl_loss: 176717.9844 - perceptual_loss: 19.6734Epoch 4: val_reconstruction_loss did not improve from 14589.81836 (saving disabled)\n",
      "Epoch 4: val_reconstruction_loss did not improve from 14589.81836 (saving disabled)\n",
      "Epoch 4: val_reconstruction_loss did not improve from 14589.81836 (saving disabled)\n",
      "Epoch 4: val_reconstruction_loss did not improve from 14589.81836 (saving disabled)\n",
      "110/110 [==============================] - 19s 172ms/step - loss: 216970.8941 - reconstruction_loss: 39189.2812 - kl_loss: 176717.9844 - perceptual_loss: 19.6734 - val_loss: 144762.2500 - val_reconstruction_loss: 15021.4688 - val_kl_loss: 64867.0820 - val_perceptual_loss: 6.6275 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 19s 172ms/step - loss: 216970.8941 - reconstruction_loss: 39189.2812 - kl_loss: 176717.9844 - perceptual_loss: 19.6734 - val_loss: 144762.2500 - val_reconstruction_loss: 15021.4688 - val_kl_loss: 64867.0820 - val_perceptual_loss: 6.6275 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "Epoch 5/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 200820.2455 - reconstruction_loss: 36825.2148 - kl_loss: 160591.0000 - perceptual_loss: 16.8281Epoch 5: val_reconstruction_loss did not improve from 14589.81836 (saving disabled)\n",
      "Epoch 5: val_reconstruction_loss did not improve from 14589.81836 (saving disabled)\n",
      "Epoch 5: val_reconstruction_loss did not improve from 14589.81836 (saving disabled)\n",
      "Epoch 5: val_reconstruction_loss did not improve from 14589.81836 (saving disabled)\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 200789.7306 - reconstruction_loss: 36825.2148 - kl_loss: 160591.0000 - perceptual_loss: 16.8281 - val_loss: 127027.5000 - val_reconstruction_loss: 14789.8926 - val_kl_loss: 56115.2773 - val_perceptual_loss: 7.0583 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 200789.7306 - reconstruction_loss: 36825.2148 - kl_loss: 160591.0000 - perceptual_loss: 16.8281 - val_loss: 127027.5000 - val_reconstruction_loss: 14789.8926 - val_kl_loss: 56115.2773 - val_perceptual_loss: 7.0583 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "Epoch 6/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 177265.1594 - reconstruction_loss: 34797.5469 - kl_loss: 147923.4375 - perceptual_loss: 15.3812Epoch 6: val_reconstruction_loss improved from 14589.81836 to 13865.91309 (saving disabled for epochs 1-30)\n",
      "Epoch 6: val_reconstruction_loss improved from 14589.81836 to 13865.91309 (saving disabled for epochs 1-30)\n",
      "Epoch 6: val_reconstruction_loss improved from 14589.81836 to 13865.91309 (saving disabled for epochs 1-30)\n",
      "Epoch 6: val_reconstruction_loss improved from 14589.81836 to 13865.91309 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 21s 190ms/step - loss: 177314.4501 - reconstruction_loss: 34797.5469 - kl_loss: 147923.4375 - perceptual_loss: 15.3812 - val_loss: 132563.0781 - val_reconstruction_loss: 13865.9131 - val_kl_loss: 59345.4727 - val_perceptual_loss: 6.2207 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 21s 190ms/step - loss: 177314.4501 - reconstruction_loss: 34797.5469 - kl_loss: 147923.4375 - perceptual_loss: 15.3812 - val_loss: 132563.0781 - val_reconstruction_loss: 13865.9131 - val_kl_loss: 59345.4727 - val_perceptual_loss: 6.2207 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "Epoch 7/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 172198.0241 - reconstruction_loss: 33335.2578 - kl_loss: 140049.2031 - perceptual_loss: 14.9701Epoch 7: val_reconstruction_loss did not improve from 13865.91309 (saving disabled)\n",
      "Epoch 7: val_reconstruction_loss did not improve from 13865.91309 (saving disabled)\n",
      "Epoch 7: val_reconstruction_loss did not improve from 13865.91309 (saving disabled)\n",
      "Epoch 7: val_reconstruction_loss did not improve from 13865.91309 (saving disabled)\n",
      "110/110 [==============================] - 20s 182ms/step - loss: 172208.8481 - reconstruction_loss: 33335.2578 - kl_loss: 140049.2031 - perceptual_loss: 14.9701 - val_loss: 130812.0000 - val_reconstruction_loss: 14653.1689 - val_kl_loss: 58076.1367 - val_perceptual_loss: 6.5538 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 20s 182ms/step - loss: 172208.8481 - reconstruction_loss: 33335.2578 - kl_loss: 140049.2031 - perceptual_loss: 14.9701 - val_loss: 130812.0000 - val_reconstruction_loss: 14653.1689 - val_kl_loss: 58076.1367 - val_perceptual_loss: 6.5538 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "Epoch 8/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 162189.5909 - reconstruction_loss: 32713.7676 - kl_loss: 134886.7656 - perceptual_loss: 14.3855Epoch 8: val_reconstruction_loss did not improve from 13865.91309 (saving disabled)\n",
      "Epoch 8: val_reconstruction_loss did not improve from 13865.91309 (saving disabled)\n",
      "Epoch 8: val_reconstruction_loss did not improve from 13865.91309 (saving disabled)\n",
      "Epoch 8: val_reconstruction_loss did not improve from 13865.91309 (saving disabled)\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 162238.4680 - reconstruction_loss: 32713.7676 - kl_loss: 134886.7656 - perceptual_loss: 14.3855 - val_loss: 114818.2812 - val_reconstruction_loss: 14108.2168 - val_kl_loss: 50351.8125 - val_perceptual_loss: 6.4364 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 162238.4680 - reconstruction_loss: 32713.7676 - kl_loss: 134886.7656 - perceptual_loss: 14.3855 - val_loss: 114818.2812 - val_reconstruction_loss: 14108.2168 - val_kl_loss: 50351.8125 - val_perceptual_loss: 6.4364 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "Epoch 9/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 165409.5720 - reconstruction_loss: 31515.0176 - kl_loss: 128431.6172 - perceptual_loss: 13.8722Epoch 9: val_reconstruction_loss improved from 13865.91309 to 13383.15918 (saving disabled for epochs 1-30)\n",
      "Epoch 9: val_reconstruction_loss improved from 13865.91309 to 13383.15918 (saving disabled for epochs 1-30)\n",
      "Epoch 9: val_reconstruction_loss improved from 13865.91309 to 13383.15918 (saving disabled for epochs 1-30)\n",
      "Epoch 9: val_reconstruction_loss improved from 13865.91309 to 13383.15918 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 21s 185ms/step - loss: 165360.4809 - reconstruction_loss: 31515.0176 - kl_loss: 128431.6172 - perceptual_loss: 13.8722 - val_loss: 114473.4453 - val_reconstruction_loss: 13383.1592 - val_kl_loss: 50542.3945 - val_perceptual_loss: 5.5017 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 21s 185ms/step - loss: 165360.4809 - reconstruction_loss: 31515.0176 - kl_loss: 128431.6172 - perceptual_loss: 13.8722 - val_loss: 114473.4453 - val_reconstruction_loss: 13383.1592 - val_kl_loss: 50542.3945 - val_perceptual_loss: 5.5017 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "Epoch 10/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 154230.0384 - reconstruction_loss: 30925.0527 - kl_loss: 124730.0156 - perceptual_loss: 13.8250Epoch 10: val_reconstruction_loss improved from 13383.15918 to 11720.08984 (saving disabled for epochs 1-30)\n",
      "Epoch 10: val_reconstruction_loss improved from 13383.15918 to 11720.08984 (saving disabled for epochs 1-30)\n",
      "Epoch 10: val_reconstruction_loss improved from 13383.15918 to 11720.08984 (saving disabled for epochs 1-30)\n",
      "Epoch 10: val_reconstruction_loss improved from 13383.15918 to 11720.08984 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 154243.0007 - reconstruction_loss: 30925.0527 - kl_loss: 124730.0156 - perceptual_loss: 13.8250 - val_loss: 95249.3750 - val_reconstruction_loss: 11720.0898 - val_kl_loss: 41761.4180 - val_perceptual_loss: 6.4453 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 154243.0007 - reconstruction_loss: 30925.0527 - kl_loss: 124730.0156 - perceptual_loss: 13.8250 - val_loss: 95249.3750 - val_reconstruction_loss: 11720.0898 - val_kl_loss: 41761.4180 - val_perceptual_loss: 6.4453 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "Epoch 11/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 146962.0592 - reconstruction_loss: 29879.0566 - kl_loss: 117615.7891 - perceptual_loss: 13.2306Epoch 11: val_reconstruction_loss improved from 11720.08984 to 11229.65430 (saving disabled for epochs 1-30)\n",
      "Epoch 11: val_reconstruction_loss improved from 11720.08984 to 11229.65430 (saving disabled for epochs 1-30)\n",
      "Epoch 11: val_reconstruction_loss improved from 11720.08984 to 11229.65430 (saving disabled for epochs 1-30)\n",
      "Epoch 11: val_reconstruction_loss improved from 11720.08984 to 11229.65430 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 20s 182ms/step - loss: 146966.9784 - reconstruction_loss: 29879.0566 - kl_loss: 117615.7891 - perceptual_loss: 13.2306 - val_loss: 83826.5000 - val_reconstruction_loss: 11229.6543 - val_kl_loss: 36295.4297 - val_perceptual_loss: 5.9849 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 20s 182ms/step - loss: 146966.9784 - reconstruction_loss: 29879.0566 - kl_loss: 117615.7891 - perceptual_loss: 13.2306 - val_loss: 83826.5000 - val_reconstruction_loss: 11229.6543 - val_kl_loss: 36295.4297 - val_perceptual_loss: 5.9849 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "Epoch 12/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 153352.1227 - reconstruction_loss: 30086.0000 - kl_loss: 118443.6641 - perceptual_loss: 13.2544Epoch 12: val_reconstruction_loss did not improve from 11229.65430 (saving disabled)\n",
      "Epoch 12: val_reconstruction_loss did not improve from 11229.65430 (saving disabled)\n",
      "Epoch 12: val_reconstruction_loss did not improve from 11229.65430 (saving disabled)\n",
      "Epoch 12: val_reconstruction_loss did not improve from 11229.65430 (saving disabled)\n",
      "110/110 [==============================] - 20s 182ms/step - loss: 153308.7966 - reconstruction_loss: 30086.0000 - kl_loss: 118443.6641 - perceptual_loss: 13.2544 - val_loss: 91012.1094 - val_reconstruction_loss: 11778.3945 - val_kl_loss: 39614.2344 - val_perceptual_loss: 5.2437 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 20s 182ms/step - loss: 153308.7966 - reconstruction_loss: 30086.0000 - kl_loss: 118443.6641 - perceptual_loss: 13.2544 - val_loss: 91012.1094 - val_reconstruction_loss: 11778.3945 - val_kl_loss: 39614.2344 - val_perceptual_loss: 5.2437 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "Epoch 13/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 136198.6072 - reconstruction_loss: 28956.4707 - kl_loss: 113830.4219 - perceptual_loss: 12.8524Epoch 13: val_reconstruction_loss did not improve from 11229.65430 (saving disabled)\n",
      "Epoch 13: val_reconstruction_loss did not improve from 11229.65430 (saving disabled)\n",
      "Epoch 13: val_reconstruction_loss did not improve from 11229.65430 (saving disabled)\n",
      "Epoch 13: val_reconstruction_loss did not improve from 11229.65430 (saving disabled)\n",
      "110/110 [==============================] - 21s 184ms/step - loss: 136258.0769 - reconstruction_loss: 28956.4707 - kl_loss: 113830.4219 - perceptual_loss: 12.8524 - val_loss: 75810.9297 - val_reconstruction_loss: 11253.7500 - val_kl_loss: 32275.8652 - val_perceptual_loss: 5.4473 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 21s 184ms/step - loss: 136258.0769 - reconstruction_loss: 28956.4707 - kl_loss: 113830.4219 - perceptual_loss: 12.8524 - val_loss: 75810.9297 - val_reconstruction_loss: 11253.7500 - val_kl_loss: 32275.8652 - val_perceptual_loss: 5.4473 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "Epoch 14/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 139805.0663 - reconstruction_loss: 28774.6055 - kl_loss: 112970.1484 - perceptual_loss: 12.8748Epoch 14: val_reconstruction_loss did not improve from 11229.65430 (saving disabled)\n",
      "Epoch 14: val_reconstruction_loss did not improve from 11229.65430 (saving disabled)\n",
      "Epoch 14: val_reconstruction_loss did not improve from 11229.65430 (saving disabled)\n",
      "Epoch 14: val_reconstruction_loss did not improve from 11229.65430 (saving disabled)\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 139822.6569 - reconstruction_loss: 28774.6055 - kl_loss: 112970.1484 - perceptual_loss: 12.8748 - val_loss: 81922.7734 - val_reconstruction_loss: 11273.1406 - val_kl_loss: 35322.0586 - val_perceptual_loss: 5.5193 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 139822.6569 - reconstruction_loss: 28774.6055 - kl_loss: 112970.1484 - perceptual_loss: 12.8748 - val_loss: 81922.7734 - val_reconstruction_loss: 11273.1406 - val_kl_loss: 35322.0586 - val_perceptual_loss: 5.5193 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "Epoch 15/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 147431.8700 - reconstruction_loss: 28953.4121 - kl_loss: 111406.3672 - perceptual_loss: 12.6906Epoch 15: val_reconstruction_loss improved from 11229.65430 to 11169.36523 (saving disabled for epochs 1-30)\n",
      "Epoch 15: val_reconstruction_loss improved from 11229.65430 to 11169.36523 (saving disabled for epochs 1-30)\n",
      "Epoch 15: val_reconstruction_loss improved from 11229.65430 to 11169.36523 (saving disabled for epochs 1-30)\n",
      "Epoch 15: val_reconstruction_loss improved from 11229.65430 to 11169.36523 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 21s 186ms/step - loss: 147368.2717 - reconstruction_loss: 28953.4121 - kl_loss: 111406.3672 - perceptual_loss: 12.6906 - val_loss: 76854.0156 - val_reconstruction_loss: 11169.3652 - val_kl_loss: 32839.6953 - val_perceptual_loss: 5.2586 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 21s 186ms/step - loss: 147368.2717 - reconstruction_loss: 28953.4121 - kl_loss: 111406.3672 - perceptual_loss: 12.6906 - val_loss: 76854.0156 - val_reconstruction_loss: 11169.3652 - val_kl_loss: 32839.6953 - val_perceptual_loss: 5.2586 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "Epoch 16/150\n",
      " 21/110 [====>.........................] - ETA: 10s - loss: 122965.0923 - reconstruction_loss: 27563.6191 - kl_loss: 103778.2891 - perceptual_loss: 13.2540"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Train FLRNet using the trainer\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     flr_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_flr_net\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflrnet_train_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflrnet_test_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_sensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_sensors\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflr_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflr_learning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_vae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the VAE model we just trained\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatent_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatent_dims\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_base_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_base_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_perceptual_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_perceptual_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduce_lr_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreduce_lr_patience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ FLRNet training completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Research\\Physics-informed-machine-learning\\flow_field_recon_parc\\models_improved.py:1292\u001b[0m, in \u001b[0;36mFLRTrainer.train_flr_net\u001b[1;34m(self, train_dataset, val_dataset, n_sensors, epochs, learning_rate, pretrained_vae, latent_dims, n_base_features, use_perceptual_loss, freeze_autoencoder, patience, reduce_lr_patience, **kwargs)\u001b[0m\n\u001b[0;32m   1289\u001b[0m callback_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflrnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39mpatience, reduce_lr_patience\u001b[38;5;241m=\u001b[39mreduce_lr_patience)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m-> 1292\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflr_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;66;03m# Save final model using weights if save_last_model is enabled\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_last_model:\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train FLRNet model using FLRTrainer\n",
    "if train_flrnet_model:\n",
    "    print(\"\\nüîÑ Starting FLRNet Training using FLRTrainer...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Train FLRNet using the trainer\n",
    "    flr_model = trainer.train_flr_net(\n",
    "        train_dataset=flrnet_train_dataset,\n",
    "        val_dataset=flrnet_test_dataset,\n",
    "        n_sensors=config['n_sensors'],\n",
    "        epochs=config['flr_epochs'],\n",
    "        learning_rate=config['flr_learning_rate'],\n",
    "        pretrained_vae=vae_model,  # Use the VAE model we just trained\n",
    "        latent_dims=config['latent_dims'],\n",
    "        n_base_features=config['n_base_features'],\n",
    "        use_perceptual_loss=config['use_perceptual_loss'],\n",
    "        patience=config['patience'],\n",
    "        reduce_lr_patience=config['reduce_lr_patience']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ FLRNet training completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping FLRNet training. Loading existing model...\")\n",
    "    # The trainer will handle loading existing FLRNet weights automatically\n",
    "    flr_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e2163b",
   "metadata": {},
   "source": [
    "## FLRNet Validation and Visualization\n",
    "\n",
    "After training the FLRNet model, we can evaluate its performance by:\n",
    "1. Testing sensor-to-field reconstruction accuracy\n",
    "2. Comparing predicted vs ground truth flow fields\n",
    "3. Analyzing reconstruction quality metrics\n",
    "4. Visualizing sensor positions overlaid on predictions\n",
    "\n",
    "This section validates the main FLRNet model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load FLRNet Model\n",
    "print(\"üöÄ Loading trained FLRNet model from checkpoint using FLRTrainer...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the same checkpoint directory\n",
    "checkpoint_directory = Path(config['checkpoint_dir'])\n",
    "print(f\"üìÇ Using checkpoint directory: {checkpoint_directory}\")\n",
    "\n",
    "# List available checkpoints for debugging\n",
    "if checkpoint_directory.exists():\n",
    "    print(f\"üìã Available FLRNet checkpoint files:\")\n",
    "    flrnet_files = [f for f in sorted(checkpoint_directory.iterdir()) if 'flrnet' in f.name.lower()]\n",
    "    for file in flrnet_files:\n",
    "        print(f\"   - {file.name}\")\n",
    "    \n",
    "    if not flrnet_files:\n",
    "        print(\"   ‚ö†Ô∏è  No FLRNet checkpoint files found\")\n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint directory not found: {checkpoint_directory}\")\n",
    "\n",
    "# Use the trainer's built-in checkpoint loading method for FLRNet\n",
    "flr_model = trainer.load_flrnet_from_checkpoint(\n",
    "    checkpoint_dir=checkpoint_directory,\n",
    "    n_sensors=config['n_sensors'],\n",
    "    latent_dims=config['latent_dims'],\n",
    "    n_base_features=config['n_base_features'],\n",
    "    use_perceptual_loss=config['use_perceptual_loss'],\n",
    "    pretrained_vae=vae_model,  # Use the VAE model we just loaded\n",
    "    freeze_autoencoder=True,   # Keep autoencoder frozen for inference\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if flr_model is not None:\n",
    "    print(\"\\nüéâ FLRNet model loaded successfully!\")\n",
    "    print(\"üìà Model is ready for sensor-to-field reconstruction\")\n",
    "    \n",
    "    # Verify the model works with a test prediction\n",
    "    try:\n",
    "        # Get a small batch for testing\n",
    "        test_batch = next(iter(flrnet_test_dataset.take(1)))\n",
    "        \n",
    "        # Handle different input formats based on Fourier configuration\n",
    "        if config['use_fourier']:\n",
    "            # Fourier FLRNet expects (sensor_data, field_data, coordinates)\n",
    "            sensor_data, field_data, coord_data = test_batch\n",
    "            print(f\"‚úÖ Fourier FLRNet input format detected:\")\n",
    "            print(f\"   - Sensor data shape: {sensor_data[:1].shape}\")\n",
    "            print(f\"   - Field data shape: {field_data[:1].shape}\")\n",
    "            print(f\"   - Coordinate data shape: {coord_data[:1].shape}\")\n",
    "            \n",
    "            # Test prediction with all inputs (training format)\n",
    "            test_prediction = flr_model([sensor_data[:1], field_data[:1], coord_data[:1]], training=False)\n",
    "            print(f\"‚úÖ Fourier FLRNet verification successful!\")\n",
    "            \n",
    "            # Test prediction with just sensor data (inference format)\n",
    "            sensor_only_prediction = flr_model(sensor_data[:1], training=False)\n",
    "            print(f\"‚úÖ Sensor-only prediction also works!\")\n",
    "            \n",
    "        else:\n",
    "            # Standard FLRNet expects (sensor_data, field_data)\n",
    "            sensor_data, field_data = test_batch\n",
    "            print(f\"‚úÖ Standard FLRNet input format detected:\")\n",
    "            print(f\"   - Sensor data shape: {sensor_data[:1].shape}\")\n",
    "            print(f\"   - Field data shape: {field_data[:1].shape}\")\n",
    "            \n",
    "            # Test prediction\n",
    "            test_prediction = flr_model([sensor_data[:1], field_data[:1]], training=False)\n",
    "            print(f\"‚úÖ Standard FLRNet verification successful!\")\n",
    "            \n",
    "            # Test prediction with just sensor data (inference format)\n",
    "            sensor_only_prediction = flr_model(sensor_data[:1], training=False)\n",
    "            print(f\"‚úÖ Sensor-only prediction also works!\")\n",
    "        \n",
    "        print(f\"   - Output shape: {test_prediction.shape}\")\n",
    "        print(f\"   - Output range: [{test_prediction.numpy().min():.4f}, {test_prediction.numpy().max():.4f}]\")\n",
    "        \n",
    "        # Check if model has the new perceptual loss features\n",
    "        if hasattr(flr_model, 'use_perceptual_loss'):\n",
    "            print(f\"   - Perceptual loss enabled: {flr_model.use_perceptual_loss}\")\n",
    "            print(f\"   - Available metrics: {[metric.name for metric in flr_model.metrics]}\")\n",
    "        else:\n",
    "            print(\"   - Legacy model (no perceptual loss configuration)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Model verification failed: {str(e)}\")\n",
    "        print(\"Model loaded but may have compatibility issues\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to load FLRNet model\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Check if FLRNet training completed successfully\")\n",
    "    print(\"   2. Verify FLRNet checkpoint files exist in the directory\")\n",
    "    print(\"   3. Ensure model configuration matches training setup\")\n",
    "    print(\"   4. Make sure VAE model is loaded first (FLRNet depends on it)\")\n",
    "    print(\"   5. Try running the FLRNet training cell if checkpoints are missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c64bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è  FLRNet training continuation is disabled\n",
      "   Set continue_flrnet_training = True to enable\n",
      "   Current perceptual loss setting: ‚úÖ ENABLED\n",
      "   (can be changed via use_perceptual_loss_continuation variable)\n"
     ]
    }
   ],
   "source": [
    "# Continue FLRNet Training from Checkpoint\n",
    "# Set this to True if you want to continue training from the loaded checkpoint\n",
    "continue_flrnet_training = False  # Change to True to enable\n",
    "\n",
    "# Toggle for perceptual loss during continuation (can be different from original training)\n",
    "use_perceptual_loss_continuation = True  # Change to False to disable perceptual loss\n",
    "\n",
    "if continue_flrnet_training and flr_model is not None:\n",
    "    print(\"üîÑ Continuing FLRNet training from loaded checkpoint...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration for continued training\n",
    "    additional_epochs = 50\n",
    "    learning_rate = 1e-5  # Lower learning rate for fine-tuning\n",
    "    \n",
    "    print(f\"üìã Continuation Configuration:\")\n",
    "    print(f\"   - Additional epochs: {additional_epochs}\")\n",
    "    print(f\"   - Learning rate: {learning_rate} (reduced for fine-tuning)\")\n",
    "    print(f\"   - Perceptual loss: {'‚úÖ ENABLED' if use_perceptual_loss_continuation else '‚ùå DISABLED'}\")\n",
    "    print(f\"   - Model weights: ‚úÖ PRESERVED from checkpoint\")\n",
    "    print(f\"   - VAE dependency: {'‚úÖ LOADED' if vae_model is not None else '‚ùå MISSING'}\")\n",
    "    \n",
    "    # Ensure VAE model is available (required for FLRNet)\n",
    "    if vae_model is None:\n",
    "        print(\"\\nüîÑ Loading VAE model first (required for FLRNet)...\")\n",
    "        vae_model = trainer.load_vae_from_checkpoint(\n",
    "            checkpoint_dir=checkpoint_directory,\n",
    "            latent_dims=config['latent_dims'],\n",
    "            n_base_features=config['n_base_features'],\n",
    "            use_perceptual_loss=config['use_perceptual_loss'],\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    if vae_model is not None:\n",
    "        print(\"\\nüöÄ Method 1: Using FLRTrainer (Recommended)\")\n",
    "        \n",
    "        # Set the loaded models in the trainer\n",
    "        trainer.flr_model = flr_model\n",
    "        trainer.vae_model = vae_model\n",
    "        \n",
    "        # Continue training with the trainer's enhanced method\n",
    "        continued_flrnet = trainer.continue_flrnet_training(\n",
    "            train_dataset=flrnet_train_dataset,\n",
    "            val_dataset=flrnet_test_dataset,\n",
    "            epochs=additional_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            patience=config['patience'],\n",
    "            reduce_lr_patience=config['reduce_lr_patience'],\n",
    "            use_perceptual_loss=use_perceptual_loss_continuation  # New option for perceptual loss\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ FLRNet training continuation completed!\")\n",
    "        \n",
    "        # Update the model reference\n",
    "        if continued_flrnet is not None:\n",
    "            flr_model = continued_flrnet\n",
    "            print(\"üîÑ Model reference updated to continued version\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Could not load VAE model - required for FLRNet training\")\n",
    "        print(\"   FLRNet training requires a pre-trained VAE model\")\n",
    "        \n",
    "elif continue_flrnet_training and flr_model is None:\n",
    "    print(\"‚ùå Cannot continue FLRNet training: No model loaded from checkpoint\")\n",
    "    print(\"   First load a model using the FLRNet checkpoint loading cell above\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  FLRNet training continuation is disabled\")\n",
    "    print(\"   Set continue_flrnet_training = True to enable\")\n",
    "    if not continue_flrnet_training:\n",
    "        print(f\"   Current perceptual loss setting: {'‚úÖ ENABLED' if use_perceptual_loss_continuation else '‚ùå DISABLED'}\")\n",
    "        print(\"   (can be changed via use_perceptual_loss_continuation variable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced FLRNet Test Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if trainer.flr_model is not None or 'test_flr_model' in locals():\n",
    "    print(\"üé® === FLRNet Test Visualization ===\")\n",
    "    \n",
    "    # Use the test model if available, otherwise use trainer model\n",
    "    flr_model_to_use = test_flr_model if 'test_flr_model' in locals() else trainer.flr_model\n",
    "    print(f\"Model type: {'Fourier-aware' if config['use_fourier'] else 'Standard'} FLRNet\")\n",
    "    \n",
    "    # Get test data batch directly from the properly formatted dataset\n",
    "    test_batch = next(iter(flrnet_test_dataset.take(1)))\n",
    "    print(\"Using FLRNet test dataset\")\n",
    "    \n",
    "    # Handle different input formats based on Fourier configuration\n",
    "    if config['use_fourier']:\n",
    "        # Fourier FLRNet expects (sensor_data, field_data, coordinates)\n",
    "        sensor_data, field_data, coord_data = test_batch\n",
    "        print(f\"‚úÖ Fourier FLRNet inputs detected:\")\n",
    "        print(f\"   Sensor data shape: {sensor_data.shape}\")\n",
    "        print(f\"   Field data shape: {field_data.shape}\")\n",
    "        print(f\"   Coordinate data shape: {coord_data.shape}\")\n",
    "        \n",
    "        # Generate FLRNet predictions\n",
    "        print(\"\\nüîÆ Generating FLRNet reconstructions...\")\n",
    "        try:\n",
    "            # Limit to manageable batch size for visualization\n",
    "            max_samples = min(4, sensor_data.shape[0])\n",
    "            \n",
    "            limited_sensor = sensor_data[:max_samples]\n",
    "            limited_field = field_data[:max_samples]\n",
    "            limited_coord = coord_data[:max_samples]\n",
    "            \n",
    "            predictions = flr_model_to_use([limited_sensor, limited_field, limited_coord], training=False)\n",
    "            print(f\"‚úÖ Reconstruction successful! Shape: {predictions.shape}\")\n",
    "            \n",
    "            # Use limited_field as targets for comparison\n",
    "            targets = limited_field\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå FLRNet prediction failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    else:\n",
    "        # Standard FLRNet expects (sensor_data, field_data)\n",
    "        sensor_data, field_data = test_batch\n",
    "        print(f\"‚úÖ Standard FLRNet inputs:\")\n",
    "        print(f\"   Sensor data shape: {sensor_data.shape}\")\n",
    "        print(f\"   Field data shape: {field_data.shape}\")\n",
    "        \n",
    "        # Generate FLRNet predictions\n",
    "        print(\"\\nüîÆ Generating FLRNet reconstructions...\")\n",
    "        try:\n",
    "            # Limit to manageable batch size for visualization\n",
    "            max_samples = min(4, sensor_data.shape[0])\n",
    "            \n",
    "            limited_sensor = sensor_data[:max_samples]\n",
    "            limited_field = field_data[:max_samples]\n",
    "            \n",
    "            predictions = flr_model_to_use([limited_sensor, limited_field], training=False)\n",
    "            print(f\"‚úÖ Reconstruction successful! Shape: {predictions.shape}\")\n",
    "            \n",
    "            # Use limited_field as targets for comparison\n",
    "            targets = limited_field\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå FLRNet prediction failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Calculate reconstruction metrics\n",
    "    mse = np.mean((targets.numpy() - predictions) ** 2)\n",
    "    mae = np.mean(np.abs(targets.numpy() - predictions))\n",
    "    max_error = np.max(np.abs(targets.numpy() - predictions))\n",
    "    \n",
    "    print(f\"\\nüìä FLRNet Reconstruction Metrics:\")\n",
    "    print(f\"   MSE: {mse:.6f} ({'Excellent' if mse < 0.01 else 'Good' if mse < 0.05 else 'Needs improvement'})\")\n",
    "    print(f\"   MAE: {mae:.6f}\")\n",
    "    print(f\"   Max Error: {max_error:.6f}\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    n_samples = max_samples\n",
    "    fig, axes = plt.subplots(4, n_samples, figsize=(5*n_samples, 16))\n",
    "    \n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Sensor readings visualization (create a sensor field overlay)\n",
    "        ax1 = axes[0, i]\n",
    "        # Create a field to visualize sensor positions and values\n",
    "        sensor_field = np.zeros((config['input_shape'][0], config['input_shape'][1]))\n",
    "        \n",
    "        # If we have sensor positions from dataset, use them\n",
    "        if 'sensor_positions' in locals():\n",
    "            for j, (x_pos, y_pos) in enumerate(sensor_positions):\n",
    "                if j < len(limited_sensor[i]):\n",
    "                    # Convert normalized positions to pixel coordinates\n",
    "                    x_idx = int(x_pos * (config['input_shape'][1] - 1))\n",
    "                    y_idx = int(y_pos * (config['input_shape'][0] - 1))\n",
    "                    if 0 <= x_idx < config['input_shape'][1] and 0 <= y_idx < config['input_shape'][0]:\n",
    "                        sensor_field[y_idx, x_idx] = limited_sensor[i, j]\n",
    "        \n",
    "        im1 = ax1.imshow(sensor_field, cmap='RdBu_r', origin='lower')\n",
    "        ax1.set_title(f'Sensor Readings {i+1}', fontweight='bold')\n",
    "        ax1.set_xlabel('X Position')\n",
    "        ax1.set_ylabel('Y Position')\n",
    "        plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
    "        \n",
    "        # Add sensor positions as scatter points if available\n",
    "        if 'sensor_positions' in locals():\n",
    "            sensor_x = sensor_positions[:, 0] * (config['input_shape'][1] - 1)\n",
    "            sensor_y = sensor_positions[:, 1] * (config['input_shape'][0] - 1)\n",
    "            ax1.scatter(sensor_x, sensor_y, c='black', s=50, marker='o', edgecolors='white', linewidth=1)\n",
    "        \n",
    "        # Ground truth field\n",
    "        im2 = axes[1, i].imshow(targets[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "        axes[1, i].set_title(f'Ground Truth Field {i+1}', fontweight='bold')\n",
    "        axes[1, i].set_xlabel('X Position')\n",
    "        axes[1, i].set_ylabel('Y Position')\n",
    "        plt.colorbar(im2, ax=axes[1, i], shrink=0.8)\n",
    "        \n",
    "        # FLRNet prediction\n",
    "        im3 = axes[2, i].imshow(predictions[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "        axes[2, i].set_title(f'FLRNet Prediction {i+1}', fontweight='bold')\n",
    "        axes[2, i].set_xlabel('X Position')\n",
    "        axes[2, i].set_ylabel('Y Position')\n",
    "        plt.colorbar(im3, ax=axes[2, i], shrink=0.8)\n",
    "        \n",
    "        # Error map\n",
    "        error = np.abs(targets[i, :, :, 0] - predictions[i, :, :, 0])\n",
    "        im4 = axes[3, i].imshow(error, cmap='hot', origin='lower')\n",
    "        axes[3, i].set_title(f'Reconstruction Error {i+1}', fontweight='bold')\n",
    "        axes[3, i].set_xlabel('X Position')\n",
    "        axes[3, i].set_ylabel('Y Position')\n",
    "        plt.colorbar(im4, ax=axes[3, i], shrink=0.8)\n",
    "        \n",
    "        # Add error statistics as text\n",
    "        sample_mse = np.mean(error**2)\n",
    "        sample_max = np.max(error)\n",
    "        axes[3, i].text(0.02, 0.98, f'MSE: {sample_mse:.4f}\\nMax: {sample_max:.4f}', \n",
    "                       transform=axes[3, i].transAxes, verticalalignment='top',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                       fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'FLRNet Reconstruction Results - {config_name}', y=1.02, fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(f\"\\nüìà Statistical Comparison:\")\n",
    "    orig_stats = {\n",
    "        'mean': np.mean(targets.numpy()),\n",
    "        'std': np.std(targets.numpy()),\n",
    "        'min': np.min(targets.numpy()),\n",
    "        'max': np.max(targets.numpy())\n",
    "    }\n",
    "    \n",
    "    pred_stats = {\n",
    "        'mean': np.mean(predictions.numpy()),\n",
    "        'std': np.std(predictions.numpy()),\n",
    "        'min': np.min(predictions.numpy()),\n",
    "        'max': np.max(predictions.numpy())\n",
    "    }\n",
    "    \n",
    "    print(f\"   Original  - Mean: {orig_stats['mean']:.4f}, Std: {orig_stats['std']:.4f}, Range: [{orig_stats['min']:.4f}, {orig_stats['max']:.4f}]\")\n",
    "    print(f\"   Predicted - Mean: {pred_stats['mean']:.4f}, Std: {pred_stats['std']:.4f}, Range: [{pred_stats['min']:.4f}, {pred_stats['max']:.4f}]\")\n",
    "    \n",
    "    # Distribution comparison\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Value distributions\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(targets.numpy().flatten(), bins=50, alpha=0.7, label='Ground Truth', density=True, color='blue')\n",
    "    plt.hist(predictions.numpy().flatten(), bins=50, alpha=0.7, label='FLRNet Prediction', density=True, color='red')\n",
    "    plt.xlabel('Field Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Value Distribution Comparison', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error distribution\n",
    "    plt.subplot(1, 3, 2)\n",
    "    errors = np.abs(targets.numpy() - predictions).flatten()\n",
    "    plt.hist(errors, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "    plt.xlabel('Absolute Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Reconstruction Error Distribution', fontweight='bold')\n",
    "    plt.axvline(np.mean(errors), color='darkred', linestyle='--', linewidth=2, label=f'Mean: {np.mean(errors):.4f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Relative error distribution\n",
    "    plt.subplot(1, 3, 3)\n",
    "    relative_errors = errors / (np.abs(targets.numpy().flatten()) + 1e-8)\n",
    "    plt.hist(relative_errors, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "    plt.xlabel('Relative Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Relative Error Distribution', fontweight='bold')\n",
    "    plt.axvline(np.mean(relative_errors), color='darkorange', linestyle='--', linewidth=2, label=f'Mean: {np.mean(relative_errors):.4f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('FLRNet Performance Analysis', y=1.02, fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    # Sensor analysis\n",
    "    print(f\"\\nüì° Sensor Analysis:\")\n",
    "    print(f\"   Number of sensors: {limited_sensor.shape[1]}\")\n",
    "    print(f\"   Sensor value range: [{limited_sensor.numpy().min():.4f}, {limited_sensor.numpy().max():.4f}]\")\n",
    "    print(f\"   Sensor value std: {limited_sensor.numpy().std():.4f}\")\n",
    "    \n",
    "    # Show sensor readings distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.boxplot([limited_sensor[i, :].numpy() for i in range(n_samples)], \n",
    "                labels=[f'Sample {i+1}' for i in range(n_samples)])\n",
    "    plt.ylabel('Sensor Reading Value')\n",
    "    plt.title('Sensor Readings Distribution per Sample', fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(n_samples):\n",
    "        plt.plot(limited_sensor[i, :].numpy(), 'o-', label=f'Sample {i+1}', alpha=0.7)\n",
    "    plt.xlabel('Sensor Index')\n",
    "    plt.ylabel('Sensor Reading Value')\n",
    "    plt.title('Sensor Readings by Position', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sensor Input Analysis', y=1.02, fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    # Reconstruction quality analysis per sample\n",
    "    print(f\"\\nüîó Reconstruction Quality Analysis:\")\n",
    "    sample_mses = []\n",
    "    sensor_means = []\n",
    "    sensor_stds = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        sample_error = np.abs(targets[i, :, :, 0] - predictions[i, :, :, 0])\n",
    "        sample_mse = np.mean(sample_error**2)\n",
    "        sample_mses.append(sample_mse)\n",
    "        sensor_means.append(np.mean(limited_sensor[i, :]))\n",
    "        sensor_stds.append(np.std(limited_sensor[i, :]))\n",
    "    \n",
    "    print(f\"   Sample MSEs: {[f'{mse:.6f}' for mse in sample_mses]}\")\n",
    "    print(f\"   Best reconstruction: Sample {np.argmin(sample_mses) + 1} (MSE: {min(sample_mses):.6f})\")\n",
    "    print(f\"   Worst reconstruction: Sample {np.argmax(sample_mses) + 1} (MSE: {max(sample_mses):.6f})\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    correlation_field = np.corrcoef(targets.numpy().flatten(), predictions.numpy().flatten())[0, 1]\n",
    "    print(f\"   Field correlation coefficient: {correlation_field:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéâ FLRNet visualization completed successfully!\")\n",
    "    print(f\"   üìä Reconstruction Quality: {'üèÜ Excellent' if mse < 0.01 else '‚úÖ Good' if mse < 0.05 else '‚ö†Ô∏è Needs improvement'}\")\n",
    "    print(f\"   üåä Fourier Features: {'‚úÖ Working correctly' if config['use_fourier'] else '‚ûñ Not used'}\")\n",
    "    print(f\"   üì° Sensor Count: {limited_sensor.shape[1]} sensors\")\n",
    "    print(f\"   üîó Field Correlation: {correlation_field:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No FLRNet model available for visualization\")\n",
    "    print(\"Please run the FLRNet model creation cell above first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
