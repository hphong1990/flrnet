{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957488d1",
   "metadata": {},
   "source": [
    "# Flow Field Reconstruction with 8 Edge Sensors\n",
    "\n",
    "This notebook implements a physics-informed machine learning approach to reconstruct flow fields using data from 8 edge sensors. The implementation uses:\n",
    "- Variational Autoencoder (VAE) for dimensionality reduction and flow field reconstruction\n",
    "- Fourier feature embeddings for coordinate information\n",
    "- FLRNet architecture to predict flow fields from sparse sensor measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ded4a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll import the necessary libraries for data manipulation, visualization, and deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6e48aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n",
      "Found 1 GPU(s): [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Suppress TensorFlow warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Force reload of modules\n",
    "if 'models_improved' in sys.modules:\n",
    "    importlib.reload(sys.modules['models_improved'])\n",
    "if 'config_manager' in sys.modules:\n",
    "    importlib.reload(sys.modules['config_manager'])\n",
    "if 'data.flow_field_dataset' in sys.modules:\n",
    "    importlib.reload(sys.modules['data.flow_field_dataset'])\n",
    "\n",
    "# Now import everything\n",
    "import models_improved\n",
    "import config_manager\n",
    "from data.flow_field_dataset import FlowFieldDatasetCreator\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Configure GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"Found {len(gpus)} GPU(s): {gpus}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"Using GPU: {gpu}\")\n",
    "        # Enable memory growth to avoid allocation issues\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPUs found, using CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22807b8f",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup\n",
    "\n",
    "Let's set up the configuration for our model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "994c56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Configuration Summary\n",
      "==================================================\n",
      "üèóÔ∏è  Model Architecture:\n",
      "   - Fourier Enhancement: True\n",
      "   - Perceptual Loss: True\n",
      "   - Input Shape: [128, 256, 1]\n",
      "   - Latent Dimensions: 8\n",
      "   - Base Features: 64\n",
      "\n",
      "üì° Sensor Configuration:\n",
      "   - Layout: random\n",
      "   - Number of Sensors: 32\n",
      "   - Dataset: data/datasets\\dataset_random_32.npz\n",
      "\n",
      "üöÄ Training Parameters:\n",
      "   - VAE Epochs: 250\n",
      "   - FLRNet Epochs: 150\n",
      "   - VAE Learning Rate: 0.0001\n",
      "   - FLRNet Learning Rate: 0.0001\n",
      "   - Batch Size: 8\n",
      "   - Test Split: 0.2\n",
      "\n",
      "üíæ Output Configuration:\n",
      "   - Model Name: fourierTrue_percepTrue_random_32\n",
      "   - Checkpoints: ./checkpoints\\fourierTrue_percepTrue_random_32\n",
      "   - Logs: ./logs\\fourierTrue_percepTrue_random_32\n",
      "   - Save Best Model: True\n",
      "   - Save Last Model: True\n",
      "\n",
      "\n",
      "üîß Final Configuration (lowercase keys):\n",
      "   Model name: fourierTrue_percepTrue_random_32\n",
      "   Use Fourier: True\n",
      "   Use perceptual loss: True\n",
      "   Input shape: (128, 256, 1)\n",
      "   Number of sensors: 32\n",
      "   Latent dims: 8\n",
      "   Base features: 64\n",
      "   Batch size: 8\n",
      "   VAE epochs: 250\n",
      "   FLRNet epochs: 150\n",
      "   VAE learning rate: 0.0001\n",
      "   FLRNet learning rate: 0.0001\n",
      "   Dataset path: data/datasets\\dataset_random_32.npz\n",
      "   Checkpoint dir: ./checkpoints\\fourierTrue_percepTrue_random_32\n",
      "   Logs dir: ./logs\\fourierTrue_percepTrue_random_32\n",
      "‚úÖ All required configuration keys are present\n"
     ]
    }
   ],
   "source": [
    "# Load configuration using ConfigManager\n",
    "config_name = \"random_32_fourier\"\n",
    "config_mgr = config_manager.ConfigManager()\n",
    "hierarchical_config = config_mgr.load_config(config_name)\n",
    "\n",
    "# Print configuration summary\n",
    "print(config_mgr.create_config_summary(hierarchical_config))\n",
    "\n",
    "# Flatten the config for training (with lowercase keys for Python style)\n",
    "flattened_config = config_manager.flatten_config_for_training(hierarchical_config)\n",
    "\n",
    "# Convert to lowercase keys for consistent Python style\n",
    "config = {}\n",
    "for key, value in flattened_config.items():\n",
    "    config[key.lower()] = value\n",
    "\n",
    "print(\"\\nüîß Final Configuration (lowercase keys):\")\n",
    "print(f\"   Model name: {config['model_name']}\")\n",
    "print(f\"   Use Fourier: {config['use_fourier']}\")\n",
    "print(f\"   Use perceptual loss: {config['use_perceptual_loss']}\")\n",
    "print(f\"   Input shape: {config['input_shape']}\")\n",
    "print(f\"   Number of sensors: {config['n_sensors']}\")\n",
    "print(f\"   Latent dims: {config['latent_dims']}\")\n",
    "print(f\"   Base features: {config['n_base_features']}\")\n",
    "print(f\"   Batch size: {config['batch_size']}\")\n",
    "print(f\"   VAE epochs: {config['vae_epochs']}\")\n",
    "print(f\"   FLRNet epochs: {config['flr_epochs']}\")\n",
    "print(f\"   VAE learning rate: {config['vae_learning_rate']}\")\n",
    "print(f\"   FLRNet learning rate: {config['flr_learning_rate']}\")\n",
    "print(f\"   Dataset path: {config['dataset_path']}\")\n",
    "print(f\"   Checkpoint dir: {config['checkpoint_dir']}\")\n",
    "print(f\"   Logs dir: {config['logs_dir']}\")\n",
    "\n",
    "# Verify essential config keys exist\n",
    "required_keys = ['model_name', 'use_fourier', 'use_perceptual_loss', 'input_shape', \n",
    "                 'n_sensors', 'latent_dims', 'n_base_features', 'batch_size', \n",
    "                 'vae_epochs', 'flr_epochs', 'vae_learning_rate', 'flr_learning_rate',\n",
    "                 'dataset_path', 'checkpoint_dir', 'logs_dir']\n",
    "missing_keys = [key for key in required_keys if key not in config]\n",
    "if missing_keys:\n",
    "    print(f\"‚ö†Ô∏è  Missing required config keys: {missing_keys}\")\n",
    "else:\n",
    "    print(\"‚úÖ All required configuration keys are present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c2fdc",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset\n",
    "\n",
    "We'll load the flow field dataset and the sensor layout for 8 edge sensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f51841e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from: data/datasets\\dataset_random_32.npz\n",
      "üìä Dataset parameters:\n",
      "   Layout type: random\n",
      "   Number of sensors: 32\n",
      "üìÅ Loading dataset file: data/datasets\\dataset_random_32.npz\n",
      "üìã Available keys in dataset: ['sensor_data', 'field_data', 'sensor_positions', 'reynolds_numbers', 'layout_type', 'n_sensors']\n",
      "üìä Dataset loaded successfully:\n",
      "   sensor_data: (28, 32, 39) (float64)\n",
      "   field_data: (28, 128, 256, 39) (float64)\n",
      "   sensor_positions: (32, 2) (float64)\n",
      "   reynolds_numbers: (28,) (int32)\n",
      "   layout_type: () (<U6)\n",
      "   n_sensors: () (int32)\n",
      "üìç Sensor positions shape: (32, 2)\n",
      "Dataset reshaped:\n",
      "  Original sensor data: (28, 32, 39)\n",
      "  Reshaped sensor data: (1092, 32)\n",
      "  Original field data: (28, 128, 256, 39)\n",
      "  Reshaped field data: (1092, 128, 256, 1)\n",
      "  Total samples: 1092\n",
      "TensorFlow datasets created:\n",
      "  Train samples: 873\n",
      "  Test samples: 219\n",
      "\n",
      "üìä TensorFlow datasets created:\n",
      "   Train dataset: <ShuffleDataset element_spec={'sensor_data': TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), 'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None)}>\n",
      "   Test dataset: <BatchDataset element_spec={'sensor_data': TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), 'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None)}>\n",
      "\n",
      "üìä Coordinate-aware datasets created:\n",
      "   Train dataset with coordinates: <MapDataset element_spec={'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), 'sensor_data': TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), 'coordinates': TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)}>\n",
      "   Test dataset with coordinates: <MapDataset element_spec={'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), 'sensor_data': TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), 'coordinates': TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)}>\n",
      "\n",
      "üåä Creating Fourier-aware VAE datasets...\n",
      "‚úÖ Fourier-aware VAE datasets created\n",
      "\n",
      "üåä Creating Fourier-aware FLRNet datasets...\n",
      "‚úÖ Fourier-aware FLRNet datasets created\n",
      "\n",
      "üìä Specialized datasets:\n",
      "   VAE train dataset: <MapDataset element_spec=((TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   VAE test dataset: <MapDataset element_spec=((TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   FLRNet train dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None))>\n",
      "   FLRNet test dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None))>\n",
      "\n",
      "üìä Data shape verification:\n",
      "   Sensor data shape: (8, 32)\n",
      "   Field data shape: (8, 128, 256, 1)\n",
      "   Coordinates shape: (8, 128, 256, 2)\n",
      "\n",
      "üìä VAE dataset structure verification:\n",
      "   VAE field input shape: (8, 128, 256, 1)\n",
      "   VAE coordinate input shape: (8, 128, 256, 2)\n",
      "   VAE target shape: (8, 128, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare dataset created from data_creation_and_viz.ipynb\n",
    "print(f\"üìÇ Loading dataset from: {config['dataset_path']}\")\n",
    "\n",
    "# Parse the dataset filename to get layout and n_sensors\n",
    "dataset_filename = Path(config['dataset_path']).name\n",
    "# Expected format: dataset_edge_8.npz\n",
    "parts = dataset_filename.split('_')\n",
    "layout_type = parts[1]  # 'edge'\n",
    "n_sensors = int(parts[2].split('.')[0])  # 8\n",
    "\n",
    "print(f\"üìä Dataset parameters:\")\n",
    "print(f\"   Layout type: {layout_type}\")\n",
    "print(f\"   Number of sensors: {n_sensors}\")\n",
    "\n",
    "# Load the dataset directly from the NPZ file\n",
    "print(f\"üìÅ Loading dataset file: {config['dataset_path']}\")\n",
    "data = np.load(config['dataset_path'])\n",
    "\n",
    "# Check what keys are available in the dataset\n",
    "print(f\"üìã Available keys in dataset: {list(data.keys())}\")\n",
    "\n",
    "# Create dataset dictionary\n",
    "dataset = {key: data[key] for key in data.keys()}\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"üìä Dataset loaded successfully:\")\n",
    "for key, value in dataset.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"   {key}: {value.shape} ({value.dtype})\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "# Extract sensor positions\n",
    "sensor_positions = dataset['sensor_positions']\n",
    "print(f\"üìç Sensor positions shape: {sensor_positions.shape}\")\n",
    "\n",
    "# Create dataset creator instance for TensorFlow dataset creation\n",
    "creator = FlowFieldDatasetCreator(\n",
    "    output_path=\"./data/\",\n",
    "    domain_shape=config['input_shape'][:2],  # (height, width)\n",
    "    use_synthetic_data=False  # Don't create synthetic data, just use for TF dataset creation\n",
    ")\n",
    "\n",
    "# Create TensorFlow datasets using the creator's method\n",
    "train_dataset, test_dataset = creator.create_tensorflow_dataset(\n",
    "    dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    test_split=config['test_split']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä TensorFlow datasets created:\")\n",
    "print(f\"   Train dataset: {train_dataset}\")\n",
    "print(f\"   Test dataset: {test_dataset}\")\n",
    "\n",
    "# Function to add coordinate grids to field data for Fourier-aware VAE training\n",
    "def add_coordinate_grid(batch):\n",
    "    \"\"\"Add coordinate grid to field data for Fourier VAE training\"\"\"\n",
    "    field_data = batch['field_data']\n",
    "    \n",
    "    # Get dimensions\n",
    "    batch_size = tf.shape(field_data)[0]\n",
    "    height = tf.shape(field_data)[1]\n",
    "    width = tf.shape(field_data)[2]\n",
    "    \n",
    "    # Create normalized coordinate grids [0, 1] - match field dimensions\n",
    "    x_coords = tf.linspace(0.0, 1.0, width)   # Width corresponds to x\n",
    "    y_coords = tf.linspace(0.0, 1.0, height)  # Height corresponds to y\n",
    "    \n",
    "    # Create meshgrid to match image indexing: [height, width, 2]\n",
    "    y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    # Stack to create coordinate grid (height, width, 2)\n",
    "    coord_grid = tf.stack([x_grid, y_grid], axis=-1)\n",
    "    \n",
    "    # Expand to batch size (batch_size, height, width, 2)\n",
    "    coord_batch = tf.tile(tf.expand_dims(coord_grid, 0), [batch_size, 1, 1, 1])\n",
    "    \n",
    "    # Update batch to include coordinates\n",
    "    return {\n",
    "        'field_data': field_data,\n",
    "        'sensor_data': batch['sensor_data'],\n",
    "        'coordinates': coord_batch\n",
    "    }\n",
    "\n",
    "# Add coordinate grids to datasets\n",
    "coord_train_dataset = train_dataset.map(add_coordinate_grid)\n",
    "coord_test_dataset = test_dataset.map(add_coordinate_grid)\n",
    "\n",
    "print(f\"\\nüìä Coordinate-aware datasets created:\")\n",
    "print(f\"   Train dataset with coordinates: {coord_train_dataset}\")\n",
    "print(f\"   Test dataset with coordinates: {coord_test_dataset}\")\n",
    "\n",
    "# Create specialized datasets for VAE and FLRNet training\n",
    "# VAE trains on field reconstruction WITH coordinates for Fourier features\n",
    "if config['use_fourier']:\n",
    "    print(\"\\nüåä Creating Fourier-aware VAE datasets...\")\n",
    "    # For Fourier VAE: input = (field, coordinates), output = field\n",
    "    vae_train_dataset = coord_train_dataset.map(\n",
    "        lambda batch: ((batch['field_data'], batch['coordinates']), batch['field_data'])\n",
    "    )\n",
    "    vae_test_dataset = coord_test_dataset.map(\n",
    "        lambda batch: ((batch['field_data'], batch['coordinates']), batch['field_data'])\n",
    "    )\n",
    "    print(\"‚úÖ Fourier-aware VAE datasets created\")\n",
    "else:\n",
    "    print(\"\\nüîÑ Creating standard VAE datasets...\")\n",
    "    # Standard VAE: input = field, output = field\n",
    "    vae_train_dataset = coord_train_dataset.map(\n",
    "        lambda batch: (batch['field_data'], batch['field_data'])\n",
    "    )\n",
    "    vae_test_dataset = coord_test_dataset.map(\n",
    "        lambda batch: (batch['field_data'], batch['field_data'])\n",
    "    )\n",
    "    print(\"‚úÖ Standard VAE datasets created\")\n",
    "\n",
    "# FLRNet trains on sensor-to-field reconstruction \n",
    "if config['use_fourier']:\n",
    "    print(\"\\nüåä Creating Fourier-aware FLRNet datasets...\")\n",
    "    # For Fourier FLRNet: input = (sensor, field, coordinates), output = field\n",
    "    flrnet_train_dataset = coord_train_dataset.map(\n",
    "        lambda batch: (batch['sensor_data'], batch['field_data'], batch['coordinates'])\n",
    "    )\n",
    "    flrnet_test_dataset = coord_test_dataset.map(\n",
    "        lambda batch: (batch['sensor_data'], batch['field_data'], batch['coordinates'])\n",
    "    )\n",
    "    print(\"‚úÖ Fourier-aware FLRNet datasets created\")\n",
    "else:\n",
    "    print(\"\\nüîÑ Creating standard FLRNet datasets...\")\n",
    "    # Standard FLRNet: input = (sensor, field), output = field\n",
    "    flrnet_train_dataset = coord_train_dataset.map(\n",
    "        lambda batch: (batch['sensor_data'], batch['field_data'])\n",
    "    )\n",
    "    flrnet_test_dataset = coord_test_dataset.map(\n",
    "        lambda batch: (batch['sensor_data'], batch['field_data'])\n",
    "    )\n",
    "    print(\"‚úÖ Standard FLRNet datasets created\")\n",
    "\n",
    "print(f\"\\nüìä Specialized datasets:\")\n",
    "print(f\"   VAE train dataset: {vae_train_dataset}\")\n",
    "print(f\"   VAE test dataset: {vae_test_dataset}\")\n",
    "print(f\"   FLRNet train dataset: {flrnet_train_dataset}\")\n",
    "print(f\"   FLRNet test dataset: {flrnet_test_dataset}\")\n",
    "\n",
    "# Get a sample to verify data shapes\n",
    "print(f\"\\nüìä Data shape verification:\")\n",
    "for batch in coord_train_dataset.take(1):\n",
    "    print(f\"   Sensor data shape: {batch['sensor_data'].shape}\")\n",
    "    print(f\"   Field data shape: {batch['field_data'].shape}\")\n",
    "    print(f\"   Coordinates shape: {batch['coordinates'].shape}\")\n",
    "    break\n",
    "\n",
    "# Verify VAE dataset structure\n",
    "print(f\"\\nüìä VAE dataset structure verification:\")\n",
    "for vae_batch in vae_train_dataset.take(1):\n",
    "    if config['use_fourier']:\n",
    "        inputs, targets = vae_batch\n",
    "        if isinstance(inputs, tuple):\n",
    "            field_input, coord_input = inputs\n",
    "            print(f\"   VAE field input shape: {field_input.shape}\")\n",
    "            print(f\"   VAE coordinate input shape: {coord_input.shape}\")\n",
    "            print(f\"   VAE target shape: {targets.shape}\")\n",
    "        else:\n",
    "            print(f\"   Unexpected VAE input format: {type(inputs)}\")\n",
    "    else:\n",
    "        inputs, targets = vae_batch\n",
    "        print(f\"   VAE input shape: {inputs.shape}\")\n",
    "        print(f\"   VAE target shape: {targets.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61349511",
   "metadata": {},
   "source": [
    "## 5. Train VAE and FLRNet Models\n",
    "\n",
    "Now we'll train our models using the FLRTrainer:\n",
    "1. First, the Variational Autoencoder (VAE) to learn a compressed representation of flow fields\n",
    "2. Then, the FLRNet to reconstruct flow fields from sparse sensor measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "566a6830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Clearing TensorFlow session...\n",
      "üöÄ Initializing FLRTrainer...\n",
      "‚úÖ FLRTrainer initialized:\n",
      "   Input shape: (128, 256, 1)\n",
      "   Use Fourier: True\n",
      "   Model name: fourierTrue_percepTrue_random_32\n",
      "   Gradient clipping: 2.0\n",
      "   Checkpoints: ./checkpoints\\fourierTrue_percepTrue_random_32\n",
      "   Logs: ./logs\\fourierTrue_percepTrue_random_32\n",
      "   Save best model: True\n",
      "   Save last model: True\n",
      "\n",
      "üîß Training configuration:\n",
      "   Train VAE: True\n",
      "   Train FLRNet: True\n",
      "\n",
      "üîç Verifying dataset shapes...\n",
      "   Field data shape: (8, 128, 256, 1)\n",
      "   Coordinates shape: (8, 128, 256, 2)\n",
      "   VAE field input shape: (8, 128, 256, 1)\n",
      "   VAE coordinate input shape: (8, 128, 256, 2)\n",
      "   VAE target shape: (8, 128, 256, 1)\n",
      "   ‚úÖ Coordinate shapes now match field shapes!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# Clear TensorFlow session to fix any shape mismatches\n",
    "print(\"üîÑ Clearing TensorFlow session...\")\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Initialize FLRTrainer from models_improved.py\n",
    "print(\"üöÄ Initializing FLRTrainer...\")\n",
    "\n",
    "# Create the FLRTrainer instance\n",
    "trainer = models_improved.FLRTrainer(\n",
    "    input_shape=config['input_shape'],\n",
    "    use_fourier=config['use_fourier'],\n",
    "    checkpoint_dir=config['checkpoint_dir'],\n",
    "    logs_dir=config['logs_dir'],\n",
    "    model_name=config['model_name'],\n",
    "    save_best_model=config['save_best_model'],\n",
    "    save_last_model=config['save_last_model'],\n",
    "    gradient_clip_norm=config['gradient_clip_norm']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ FLRTrainer initialized:\")\n",
    "print(f\"   Input shape: {config['input_shape']}\")\n",
    "print(f\"   Use Fourier: {config['use_fourier']}\")\n",
    "print(f\"   Model name: {config['model_name']}\")\n",
    "print(f\"   Gradient clipping: {config['gradient_clip_norm']}\")\n",
    "print(f\"   Checkpoints: {config['checkpoint_dir']}\")\n",
    "print(f\"   Logs: {config['logs_dir']}\")\n",
    "print(f\"   Save best model: {config['save_best_model']}\")\n",
    "print(f\"   Save last model: {config['save_last_model']}\")\n",
    "\n",
    "# Set training flags\n",
    "train_vae_model = True  # Set to False if you want to skip VAE training\n",
    "train_flrnet_model = True  # Set to True to test FLRNet with proper Fourier features\n",
    "\n",
    "print(f\"\\nüîß Training configuration:\")\n",
    "print(f\"   Train VAE: {train_vae_model}\")\n",
    "print(f\"   Train FLRNet: {train_flrnet_model}\")\n",
    "\n",
    "# Quick test to verify dataset shapes are correct\n",
    "print(f\"\\nüîç Verifying dataset shapes...\")\n",
    "for batch in coord_train_dataset.take(1):\n",
    "    print(f\"   Field data shape: {batch['field_data'].shape}\")\n",
    "    print(f\"   Coordinates shape: {batch['coordinates'].shape}\")\n",
    "    break\n",
    "\n",
    "# Test VAE dataset format\n",
    "for vae_batch in vae_train_dataset.take(1):\n",
    "    inputs, targets = vae_batch\n",
    "    if isinstance(inputs, tuple):\n",
    "        field_input, coord_input = inputs\n",
    "        print(f\"   VAE field input shape: {field_input.shape}\")\n",
    "        print(f\"   VAE coordinate input shape: {coord_input.shape}\")\n",
    "        print(f\"   VAE target shape: {targets.shape}\")\n",
    "        \n",
    "        # Check if shapes match now\n",
    "        if field_input.shape[1:3] == coord_input.shape[1:3]:\n",
    "            print(f\"   ‚úÖ Coordinate shapes now match field shapes!\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Shape mismatch: field {field_input.shape[1:3]} vs coord {coord_input.shape[1:3]}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50931c46",
   "metadata": {},
   "source": [
    "## Load Trained Models from Checkpoint\n",
    "\n",
    "Now let's load the trained models from saved checkpoints using the FLRTrainer's built-in methods. The trainer provides several loading options:\n",
    "\n",
    "- **`load_vae_from_checkpoint()`**: Load only the VAE model\n",
    "- **`load_flrnet_from_checkpoint()`**: Load only the FLRNet model  \n",
    "- **`load_models_from_checkpoint()`**: Load both models at once\n",
    "\n",
    "### Features:\n",
    "- **Automatic checkpoint detection**: Finds the best available checkpoint (best ‚Üí last ‚Üí final_weights)\n",
    "- **Robust path handling**: Uses correct TensorFlow checkpoint format (no file extensions)\n",
    "- **Error handling**: Graceful fallback and clear error messages\n",
    "- **Architecture consistency**: Ensures loaded model matches trainer configuration\n",
    "- **Smart dependencies**: FLRNet loading automatically handles VAE dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bee5726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading trained VAE model from checkpoint using FLRTrainer...\n",
      "============================================================\n",
      "üìÇ Using checkpoint directory: checkpoints\\fourierTrue_percepTrue_random_32\n",
      "üìã Available checkpoint files:\n",
      "   - checkpoint\n",
      "   - checkpoint_fourierTrue_percepTrue_random_32_vae_best.data-00000-of-00001\n",
      "   - checkpoint_fourierTrue_percepTrue_random_32_vae_best.index\n",
      "üîç Loading VAE model from checkpoint directory: checkpoints\\fourierTrue_percepTrue_random_32\n",
      "‚úÖ Found vae_best checkpoint: checkpoints\\fourierTrue_percepTrue_random_32\\checkpoint_fourierTrue_percepTrue_random_32_vae_best\n",
      "üåä VAE model built for Fourier features\n",
      "üìã VAE Model Architecture:\n",
      "   - Input shape: (128, 256, 1)\n",
      "   - Latent dims: 8\n",
      "   - Base features: 64\n",
      "   - Use Fourier: True\n",
      "   - Perceptual loss: True\n",
      "‚úÖ Successfully loaded VAE model from vae_best checkpoint!\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.z_mean_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.z_mean_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.z_log_var_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.z_log_var_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.conv_in.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.conv_in.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.conv_out.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.conv_out.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block1.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block1.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block1.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block1.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block2.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block2.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block2.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block2.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block3.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block3.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block3.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block3.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block4.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block4.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block4.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block4.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block5.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block5.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block5.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).encoder.conv_block5.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block1.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block1.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block1.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block1.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block2.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block2.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block2.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block2.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block3.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block3.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block3.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block3.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block4.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block4.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block4.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block4.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block5.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block5.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block5.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).decoder.up_block5.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.z_mean_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.z_mean_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.z_log_var_layer.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.z_log_var_layer.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.conv_in.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.conv_in.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.conv_out.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.conv_out.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block1.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block1.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block1.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block1.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block2.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block2.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block2.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block2.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block3.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block3.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block3.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block3.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block4.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block4.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block4.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block4.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block5.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block5.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block5.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).encoder.conv_block5.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block1.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block1.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block1.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block1.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block2.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block2.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block2.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block2.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block3.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block3.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block3.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block3.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block4.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block4.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block4.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block4.conv1x1_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block5.conv_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block5.conv_layers.0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block5.conv1x1_layers.0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).decoder.up_block5.conv1x1_layers.0.bias\n",
      "\n",
      "üéâ VAE model loaded successfully!\n",
      "üìà Model is ready for inference and visualization\n",
      "‚úÖ Fourier VAE verification successful!\n",
      "   - Field input shape: (1, 128, 256, 1)\n",
      "   - Coordinate input shape: (1, 128, 256, 2)\n",
      "   - Output shape: (1, 128, 256, 1)\n",
      "   - Output range: [-0.0591, 1.8878]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the VAE model using the trainer's built-in method\n",
    "print(\"üöÄ Loading trained VAE model from checkpoint using FLRTrainer...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fix checkpoint path - use the directory, not the specific file\n",
    "checkpoint_directory = Path(config['checkpoint_dir'])\n",
    "print(f\"üìÇ Using checkpoint directory: {checkpoint_directory}\")\n",
    "\n",
    "# List available checkpoints for debugging\n",
    "if checkpoint_directory.exists():\n",
    "    print(f\"üìã Available checkpoint files:\")\n",
    "    for file in sorted(checkpoint_directory.iterdir()):\n",
    "        print(f\"   - {file.name}\")\n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint directory not found: {checkpoint_directory}\")\n",
    "\n",
    "# Use the trainer's built-in checkpoint loading method\n",
    "vae_model = trainer.load_vae_from_checkpoint(\n",
    "    checkpoint_dir=checkpoint_directory,\n",
    "    latent_dims=config['latent_dims'],\n",
    "    n_base_features=config['n_base_features'],\n",
    "    use_perceptual_loss=config['use_perceptual_loss'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if vae_model is not None:\n",
    "    print(\"\\nüéâ VAE model loaded successfully!\")\n",
    "    print(\"üìà Model is ready for inference and visualization\")\n",
    "    \n",
    "    # Verify the model works with a test prediction\n",
    "    try:\n",
    "        # Get a small batch for testing\n",
    "        test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "        test_inputs, test_targets = test_batch\n",
    "        \n",
    "        # Handle different input formats (Fourier vs Standard VAE)\n",
    "        if isinstance(test_inputs, (list, tuple)) and len(test_inputs) == 2:\n",
    "            # Fourier VAE expects [field, coordinates]\n",
    "            test_field, test_coord = test_inputs\n",
    "            test_prediction = vae_model([test_field[:1], test_coord[:1]])\n",
    "            print(f\"‚úÖ Fourier VAE verification successful!\")\n",
    "            print(f\"   - Field input shape: {test_field[:1].shape}\")\n",
    "            print(f\"   - Coordinate input shape: {test_coord[:1].shape}\")\n",
    "        else:\n",
    "            # Standard VAE expects just field data\n",
    "            test_prediction = vae_model(test_inputs[:1])\n",
    "            print(f\"‚úÖ Standard VAE verification successful!\")\n",
    "            print(f\"   - Input shape: {test_inputs[:1].shape}\")\n",
    "        \n",
    "        print(f\"   - Output shape: {test_prediction.shape}\")\n",
    "        print(f\"   - Output range: [{test_prediction.numpy().min():.4f}, {test_prediction.numpy().max():.4f}]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Model verification failed: {str(e)}\")\n",
    "        print(\"Model loaded but may have compatibility issues\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to load VAE model\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Check if training completed successfully\")\n",
    "    print(\"   2. Verify checkpoint files exist in the directory\")\n",
    "    print(\"   3. Ensure model configuration matches training setup\")\n",
    "    print(\"   4. Try running the training cell again if checkpoints are missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687601d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Starting FLRNet Training using FLRTrainer...\n",
      "============================================================\n",
      "üöÄ Training FLRNet Model...\n",
      "üõ°Ô∏è Checkpoint saving will be disabled for epochs 1-30, enabled from epoch 31\n",
      "üõ°Ô∏è Checkpoint saving will be disabled for epochs 1-30, enabled from epoch 31\n",
      "Epoch 1/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 394097.4418 - reconstruction_loss: 53449.8086 - kl_loss: 267489.4688 - perceptual_loss: 28.0999Epoch 1: val_reconstruction_loss improved from inf to 16149.38379 (saving disabled for epochs 1-30)\n",
      "Epoch 1: val_reconstruction_loss improved from inf to 16149.38379 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 28s 195ms/step - loss: 393438.6132 - reconstruction_loss: 53449.8086 - kl_loss: 267489.4688 - perceptual_loss: 28.0999 - val_loss: 188372.7500 - val_reconstruction_loss: 16149.3838 - val_kl_loss: 86106.7500 - val_perceptual_loss: 9.8629 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 226863.1751 - reconstruction_loss: 40971.7500 - kl_loss: 196728.8594 - perceptual_loss: 21.7650Epoch 2: val_reconstruction_loss improved from 16149.38379 to 14111.30566 (saving disabled for epochs 1-30)\n",
      "Epoch 2: val_reconstruction_loss improved from 16149.38379 to 14111.30566 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 20s 179ms/step - loss: 226961.0063 - reconstruction_loss: 40971.7500 - kl_loss: 196728.8594 - perceptual_loss: 21.7650 - val_loss: 152278.7656 - val_reconstruction_loss: 14111.3057 - val_kl_loss: 69079.2188 - val_perceptual_loss: 9.0154 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 225047.6114 - reconstruction_loss: 38823.1758 - kl_loss: 184815.5938 - perceptual_loss: 20.8841Epoch 3: val_reconstruction_loss did not improve from 14111.30566 (saving disabled)\n",
      "Epoch 3: val_reconstruction_loss did not improve from 14111.30566 (saving disabled)\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 225035.1074 - reconstruction_loss: 38823.1758 - kl_loss: 184815.5938 - perceptual_loss: 20.8841 - val_loss: 148680.5938 - val_reconstruction_loss: 14433.2588 - val_kl_loss: 67118.3359 - val_perceptual_loss: 10.6592 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 208819.7396 - reconstruction_loss: 37087.5391 - kl_loss: 173349.4219 - perceptual_loss: 20.2073Epoch 4: val_reconstruction_loss improved from 14111.30566 to 13615.29492 (saving disabled for epochs 1-30)\n",
      "Epoch 4: val_reconstruction_loss improved from 14111.30566 to 13615.29492 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 208834.4906 - reconstruction_loss: 37087.5391 - kl_loss: 173349.4219 - perceptual_loss: 20.2073 - val_loss: 132640.0156 - val_reconstruction_loss: 13615.2949 - val_kl_loss: 59508.3047 - val_perceptual_loss: 8.1060 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 191283.3109 - reconstruction_loss: 33896.1328 - kl_loss: 151129.2656 - perceptual_loss: 16.8594Epoch 5: val_reconstruction_loss improved from 13615.29492 to 11124.45703 (saving disabled for epochs 1-30)\n",
      "Epoch 5: val_reconstruction_loss improved from 13615.29492 to 11124.45703 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 20s 177ms/step - loss: 191227.0856 - reconstruction_loss: 33896.1328 - kl_loss: 151129.2656 - perceptual_loss: 16.8594 - val_loss: 106955.5781 - val_reconstruction_loss: 11124.4570 - val_kl_loss: 47912.8008 - val_perceptual_loss: 5.5206 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 160730.0295 - reconstruction_loss: 31149.9082 - kl_loss: 129321.7891 - perceptual_loss: 14.7468Epoch 6: val_reconstruction_loss improved from 11124.45703 to 11067.12305 (saving disabled for epochs 1-30)\n",
      "Epoch 6: val_reconstruction_loss improved from 11124.45703 to 11067.12305 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 19s 175ms/step - loss: 160727.8353 - reconstruction_loss: 31149.9082 - kl_loss: 129321.7891 - perceptual_loss: 14.7468 - val_loss: 94401.1172 - val_reconstruction_loss: 11067.1230 - val_kl_loss: 41663.9844 - val_perceptual_loss: 6.0205 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 141066.0259 - reconstruction_loss: 28628.2871 - kl_loss: 110630.5625 - perceptual_loss: 13.0685Epoch 7: val_reconstruction_loss improved from 11067.12305 to 10837.98047 (saving disabled for epochs 1-30)\n",
      "Epoch 7: val_reconstruction_loss improved from 11067.12305 to 10837.98047 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 20s 179ms/step - loss: 141049.8629 - reconstruction_loss: 28628.2871 - kl_loss: 110630.5625 - perceptual_loss: 13.0685 - val_loss: 81820.5469 - val_reconstruction_loss: 10837.9805 - val_kl_loss: 35488.3203 - val_perceptual_loss: 5.9250 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 122808.2279 - reconstruction_loss: 26596.3750 - kl_loss: 99343.9375 - perceptual_loss: 11.9781Epoch 8: val_reconstruction_loss improved from 10837.98047 to 9041.28906 (saving disabled for epochs 1-30)\n",
      "Epoch 8: val_reconstruction_loss improved from 10837.98047 to 9041.28906 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 19s 175ms/step - loss: 122836.5526 - reconstruction_loss: 26596.3750 - kl_loss: 99343.9375 - perceptual_loss: 11.9781 - val_loss: 69393.1094 - val_reconstruction_loss: 9041.2891 - val_kl_loss: 30173.3965 - val_perceptual_loss: 5.0279 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 120688.0344 - reconstruction_loss: 25431.7070 - kl_loss: 92529.1328 - perceptual_loss: 11.5256Epoch 9: val_reconstruction_loss did not improve from 9041.28906 (saving disabled)\n",
      "Epoch 9: val_reconstruction_loss did not improve from 9041.28906 (saving disabled)\n",
      "110/110 [==============================] - 19s 170ms/step - loss: 120663.5690 - reconstruction_loss: 25431.7070 - kl_loss: 92529.1328 - perceptual_loss: 11.5256 - val_loss: 75340.3906 - val_reconstruction_loss: 9546.3486 - val_kl_loss: 32894.3906 - val_perceptual_loss: 5.2560 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 112166.6751 - reconstruction_loss: 24527.1992 - kl_loss: 87652.7734 - perceptual_loss: 11.2595Epoch 10: val_reconstruction_loss did not improve from 9041.28906 (saving disabled)\n",
      "Epoch 10: val_reconstruction_loss did not improve from 9041.28906 (saving disabled)\n",
      "110/110 [==============================] - 20s 176ms/step - loss: 112166.8963 - reconstruction_loss: 24527.1992 - kl_loss: 87652.7734 - perceptual_loss: 11.2595 - val_loss: 77378.7188 - val_reconstruction_loss: 9117.2354 - val_kl_loss: 34128.1094 - val_perceptual_loss: 5.2621 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 102448.8259 - reconstruction_loss: 23272.1348 - kl_loss: 80179.9219 - perceptual_loss: 10.4711Epoch 11: val_reconstruction_loss improved from 9041.28906 to 8749.77930 (saving disabled for epochs 1-30)\n",
      "Epoch 11: val_reconstruction_loss improved from 9041.28906 to 8749.77930 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 19s 175ms/step - loss: 102457.9582 - reconstruction_loss: 23272.1348 - kl_loss: 80179.9219 - perceptual_loss: 10.4711 - val_loss: 62031.4375 - val_reconstruction_loss: 8749.7793 - val_kl_loss: 26637.9277 - val_perceptual_loss: 5.7990 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 101631.2239 - reconstruction_loss: 22675.4961 - kl_loss: 77017.4297 - perceptual_loss: 10.4434Epoch 12: val_reconstruction_loss improved from 8749.77930 to 8411.87500 (saving disabled for epochs 1-30)\n",
      "Epoch 12: val_reconstruction_loss improved from 8749.77930 to 8411.87500 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 101613.8559 - reconstruction_loss: 22675.4961 - kl_loss: 77017.4297 - perceptual_loss: 10.4434 - val_loss: 59029.3320 - val_reconstruction_loss: 8411.8750 - val_kl_loss: 25306.0664 - val_perceptual_loss: 5.3254 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 91418.6703 - reconstruction_loss: 21815.0156 - kl_loss: 72015.1094 - perceptual_loss: 9.9872Epoch 13: val_reconstruction_loss improved from 8411.87500 to 8092.04590 (saving disabled for epochs 1-30)\n",
      "Epoch 13: val_reconstruction_loss improved from 8411.87500 to 8092.04590 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 20s 177ms/step - loss: 91440.4853 - reconstruction_loss: 21815.0156 - kl_loss: 72015.1094 - perceptual_loss: 9.9872 - val_loss: 59334.3789 - val_reconstruction_loss: 8092.0459 - val_kl_loss: 25618.6738 - val_perceptual_loss: 4.9845 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 92076.5613 - reconstruction_loss: 21425.7598 - kl_loss: 69587.1875 - perceptual_loss: 9.9409Epoch 14: val_reconstruction_loss improved from 8092.04590 to 7386.91650 (saving disabled for epochs 1-30)\n",
      "Epoch 14: val_reconstruction_loss improved from 8092.04590 to 7386.91650 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 92067.0688 - reconstruction_loss: 21425.7598 - kl_loss: 69587.1875 - perceptual_loss: 9.9409 - val_loss: 47549.1016 - val_reconstruction_loss: 7386.9165 - val_kl_loss: 20078.8203 - val_perceptual_loss: 4.5430 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 91948.7151 - reconstruction_loss: 20486.7500 - kl_loss: 65065.5352 - perceptual_loss: 9.7185Epoch 15: val_reconstruction_loss did not improve from 7386.91650 (saving disabled)\n",
      "Epoch 15: val_reconstruction_loss did not improve from 7386.91650 (saving disabled)\n",
      "110/110 [==============================] - 19s 171ms/step - loss: 91891.1771 - reconstruction_loss: 20486.7500 - kl_loss: 65065.5352 - perceptual_loss: 9.7185 - val_loss: 52344.7188 - val_reconstruction_loss: 7955.1768 - val_kl_loss: 22192.1445 - val_perceptual_loss: 5.2527 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 79237.6961 - reconstruction_loss: 19948.2305 - kl_loss: 61742.8555 - perceptual_loss: 9.4742Epoch 16: val_reconstruction_loss improved from 7386.91650 to 6749.73340 (saving disabled for epochs 1-30)\n",
      "Epoch 16: val_reconstruction_loss improved from 7386.91650 to 6749.73340 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 19s 172ms/step - loss: 79259.8840 - reconstruction_loss: 19948.2305 - kl_loss: 61742.8555 - perceptual_loss: 9.4742 - val_loss: 55208.3906 - val_reconstruction_loss: 6749.7334 - val_kl_loss: 24227.5156 - val_perceptual_loss: 3.6243 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 80443.3931 - reconstruction_loss: 19713.7910 - kl_loss: 60994.4766 - perceptual_loss: 9.4523Epoch 17: val_reconstruction_loss did not improve from 6749.73340 (saving disabled)\n",
      "Epoch 17: val_reconstruction_loss did not improve from 6749.73340 (saving disabled)\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 80445.8646 - reconstruction_loss: 19713.7910 - kl_loss: 60994.4766 - perceptual_loss: 9.4523 - val_loss: 40919.8984 - val_reconstruction_loss: 7010.3145 - val_kl_loss: 16952.4707 - val_perceptual_loss: 4.6431 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 83192.9270 - reconstruction_loss: 19692.0469 - kl_loss: 59985.6289 - perceptual_loss: 9.3361Epoch 18: val_reconstruction_loss improved from 6749.73340 to 5751.99561 (saving disabled for epochs 1-30)\n",
      "Epoch 18: val_reconstruction_loss improved from 6749.73340 to 5751.99561 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 20s 176ms/step - loss: 83161.3420 - reconstruction_loss: 19692.0469 - kl_loss: 59985.6289 - perceptual_loss: 9.3361 - val_loss: 37527.1953 - val_reconstruction_loss: 5751.9956 - val_kl_loss: 15885.9160 - val_perceptual_loss: 3.3667 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 76876.8509 - reconstruction_loss: 19264.1367 - kl_loss: 57905.6562 - perceptual_loss: 9.2258Epoch 19: val_reconstruction_loss did not improve from 5751.99561 (saving disabled)\n",
      "Epoch 19: val_reconstruction_loss did not improve from 5751.99561 (saving disabled)\n",
      "110/110 [==============================] - 20s 176ms/step - loss: 76879.5729 - reconstruction_loss: 19264.1367 - kl_loss: 57905.6562 - perceptual_loss: 9.2258 - val_loss: 42846.1016 - val_reconstruction_loss: 6215.3330 - val_kl_loss: 18313.4707 - val_perceptual_loss: 3.8273 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 74383.0526 - reconstruction_loss: 18721.2793 - kl_loss: 55707.9805 - perceptual_loss: 8.9353Epoch 20: val_reconstruction_loss improved from 5751.99561 to 5403.54297 (saving disabled for epochs 1-30)\n",
      "Epoch 20: val_reconstruction_loss improved from 5751.99561 to 5403.54297 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 20s 177ms/step - loss: 74383.5496 - reconstruction_loss: 18721.2793 - kl_loss: 55707.9805 - perceptual_loss: 8.9353 - val_loss: 36172.4141 - val_reconstruction_loss: 5403.5430 - val_kl_loss: 15382.7881 - val_perceptual_loss: 3.2951 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 74071.7904 - reconstruction_loss: 18349.6758 - kl_loss: 52832.3789 - perceptual_loss: 8.9170Epoch 21: val_reconstruction_loss did not improve from 5403.54297 (saving disabled)\n",
      "Epoch 21: val_reconstruction_loss did not improve from 5403.54297 (saving disabled)\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 74045.8368 - reconstruction_loss: 18349.6758 - kl_loss: 52832.3789 - perceptual_loss: 8.9170 - val_loss: 39383.1016 - val_reconstruction_loss: 6437.9980 - val_kl_loss: 16470.5859 - val_perceptual_loss: 3.9341 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 74060.4877 - reconstruction_loss: 18481.2461 - kl_loss: 52813.3438 - perceptual_loss: 8.8217Epoch 22: val_reconstruction_loss did not improve from 5403.54297 (saving disabled)\n",
      "Epoch 22: val_reconstruction_loss did not improve from 5403.54297 (saving disabled)\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 74035.6494 - reconstruction_loss: 18481.2461 - kl_loss: 52813.3438 - perceptual_loss: 8.8217 - val_loss: 37425.6328 - val_reconstruction_loss: 6111.7891 - val_kl_loss: 15655.1787 - val_perceptual_loss: 3.4842 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 72142.2763 - reconstruction_loss: 18274.8242 - kl_loss: 51849.9414 - perceptual_loss: 8.7512Epoch 23: val_reconstruction_loss did not improve from 5403.54297 (saving disabled)\n",
      "Epoch 23: val_reconstruction_loss did not improve from 5403.54297 (saving disabled)\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 72124.1794 - reconstruction_loss: 18274.8242 - kl_loss: 51849.9414 - perceptual_loss: 8.7512 - val_loss: 38135.4492 - val_reconstruction_loss: 6181.1997 - val_kl_loss: 15975.3535 - val_perceptual_loss: 3.5426 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 68091.7099 - reconstruction_loss: 17660.1250 - kl_loss: 50170.4805 - perceptual_loss: 8.5671Epoch 24: val_reconstruction_loss improved from 5403.54297 to 5357.44141 (saving disabled for epochs 1-30)\n",
      "Epoch 24: val_reconstruction_loss improved from 5403.54297 to 5357.44141 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 68089.4348 - reconstruction_loss: 17660.1250 - kl_loss: 50170.4805 - perceptual_loss: 8.5671 - val_loss: 29451.0859 - val_reconstruction_loss: 5357.4414 - val_kl_loss: 12045.2344 - val_perceptual_loss: 3.1765 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 66162.3701 - reconstruction_loss: 17577.7324 - kl_loss: 48863.1055 - perceptual_loss: 8.5692Epoch 25: val_reconstruction_loss did not improve from 5357.44141 (saving disabled)\n",
      "Epoch 25: val_reconstruction_loss did not improve from 5357.44141 (saving disabled)\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 66164.9562 - reconstruction_loss: 17577.7324 - kl_loss: 48863.1055 - perceptual_loss: 8.5692 - val_loss: 36565.9961 - val_reconstruction_loss: 6116.6216 - val_kl_loss: 15222.9404 - val_perceptual_loss: 3.4934 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 68935.5683 - reconstruction_loss: 17589.5645 - kl_loss: 48709.1055 - perceptual_loss: 8.4733Epoch 26: val_reconstruction_loss did not improve from 5357.44141 (saving disabled)\n",
      "Epoch 26: val_reconstruction_loss did not improve from 5357.44141 (saving disabled)\n",
      "110/110 [==============================] - 20s 177ms/step - loss: 68911.8887 - reconstruction_loss: 17589.5645 - kl_loss: 48709.1055 - perceptual_loss: 8.4733 - val_loss: 34641.9766 - val_reconstruction_loss: 6037.5918 - val_kl_loss: 14300.2480 - val_perceptual_loss: 3.8882 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 62149.3639 - reconstruction_loss: 17610.8418 - kl_loss: 48251.4180 - perceptual_loss: 8.6029Epoch 27: val_reconstruction_loss improved from 5357.44141 to 5121.45117 (saving disabled for epochs 1-30)\n",
      "Epoch 27: val_reconstruction_loss improved from 5357.44141 to 5121.45117 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 20s 185ms/step - loss: 62182.8910 - reconstruction_loss: 17610.8418 - kl_loss: 48251.4180 - perceptual_loss: 8.6029 - val_loss: 28982.3184 - val_reconstruction_loss: 5121.4512 - val_kl_loss: 11928.8867 - val_perceptual_loss: 3.0937 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 61408.5343 - reconstruction_loss: 17094.2910 - kl_loss: 46198.3867 - perceptual_loss: 8.3110Epoch 28: val_reconstruction_loss improved from 5121.45117 to 4963.99219 (saving disabled for epochs 1-30)\n",
      "Epoch 28: val_reconstruction_loss improved from 5121.45117 to 4963.99219 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 61425.5833 - reconstruction_loss: 17094.2910 - kl_loss: 46198.3867 - perceptual_loss: 8.3110 - val_loss: 27945.2852 - val_reconstruction_loss: 4963.9922 - val_kl_loss: 11489.0742 - val_perceptual_loss: 3.1443 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 60905.4916 - reconstruction_loss: 17025.6992 - kl_loss: 45204.9102 - perceptual_loss: 8.3721Epoch 29: val_reconstruction_loss did not improve from 4963.99219 (saving disabled)\n",
      "Epoch 29: val_reconstruction_loss did not improve from 4963.99219 (saving disabled)\n",
      "110/110 [==============================] - 20s 180ms/step - loss: 60917.5049 - reconstruction_loss: 17025.6992 - kl_loss: 45204.9102 - perceptual_loss: 8.3721 - val_loss: 28941.2871 - val_reconstruction_loss: 5118.7290 - val_kl_loss: 11909.5127 - val_perceptual_loss: 3.5331 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 63860.6340 - reconstruction_loss: 16860.7598 - kl_loss: 45145.2734 - perceptual_loss: 8.3381Epoch 30: val_reconstruction_loss improved from 4963.99219 to 4801.39697 (saving disabled for epochs 1-30)\n",
      "Epoch 30: val_reconstruction_loss improved from 4963.99219 to 4801.39697 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 20s 179ms/step - loss: 63844.0011 - reconstruction_loss: 16860.7598 - kl_loss: 45145.2734 - perceptual_loss: 8.3381 - val_loss: 24736.0664 - val_reconstruction_loss: 4801.3970 - val_kl_loss: 9965.6855 - val_perceptual_loss: 3.2994 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 65419.7865 - reconstruction_loss: 16896.8711 - kl_loss: 44510.8320 - perceptual_loss: 8.3491\n",
      "Epoch 31: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 21s 187ms/step - loss: 65383.7167 - reconstruction_loss: 16896.8711 - kl_loss: 44510.8320 - perceptual_loss: 8.3491 - val_loss: 34485.1367 - val_reconstruction_loss: 6043.7290 - val_kl_loss: 14219.0449 - val_perceptual_loss: 3.3155 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 57469.0790 - reconstruction_loss: 16555.3613 - kl_loss: 43480.5781 - perceptual_loss: 8.1480\n",
      "Epoch 32: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 21s 189ms/step - loss: 57492.2773 - reconstruction_loss: 16555.3613 - kl_loss: 43480.5781 - perceptual_loss: 8.1480 - val_loss: 31045.2969 - val_reconstruction_loss: 5595.2422 - val_kl_loss: 12723.4326 - val_perceptual_loss: 3.1888 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 59923.3901 - reconstruction_loss: 16711.5059 - kl_loss: 44432.3320 - perceptual_loss: 8.2511\n",
      "Epoch 33: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 59934.4593 - reconstruction_loss: 16711.5059 - kl_loss: 44432.3320 - perceptual_loss: 8.2511 - val_loss: 36603.9141 - val_reconstruction_loss: 5694.0308 - val_kl_loss: 15453.2451 - val_perceptual_loss: 3.3925 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 62528.4554 - reconstruction_loss: 16399.8301 - kl_loss: 42811.5234 - perceptual_loss: 8.1644\n",
      "Epoch 34: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 62498.6451 - reconstruction_loss: 16399.8301 - kl_loss: 42811.5234 - perceptual_loss: 8.1644 - val_loss: 30311.2715 - val_reconstruction_loss: 5837.9375 - val_kl_loss: 12234.6562 - val_perceptual_loss: 4.0211 - lr: 1.0000e-04\n",
      "Epoch 35/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 59820.1443 - reconstruction_loss: 16444.0527 - kl_loss: 42611.9414 - perceptual_loss: 8.1439\n",
      "Epoch 35: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 183ms/step - loss: 59813.3336 - reconstruction_loss: 16444.0527 - kl_loss: 42611.9414 - perceptual_loss: 8.1439 - val_loss: 26957.3633 - val_reconstruction_loss: 5574.2373 - val_kl_loss: 10689.6357 - val_perceptual_loss: 3.8560 - lr: 1.0000e-04\n",
      "Epoch 36/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 52606.9982 - reconstruction_loss: 16053.5215 - kl_loss: 40854.1719 - perceptual_loss: 7.9351\n",
      "Epoch 36: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 179ms/step - loss: 52645.8146 - reconstruction_loss: 16053.5215 - kl_loss: 40854.1719 - perceptual_loss: 7.9351 - val_loss: 29685.8281 - val_reconstruction_loss: 5751.8682 - val_kl_loss: 11965.1680 - val_perceptual_loss: 3.6223 - lr: 1.0000e-04\n",
      "Epoch 37/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 58897.0231 - reconstruction_loss: 16125.4180 - kl_loss: 40425.1055 - perceptual_loss: 7.9900\n",
      "Epoch 37: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 19s 176ms/step - loss: 58875.9552 - reconstruction_loss: 16125.4180 - kl_loss: 40425.1055 - perceptual_loss: 7.9900 - val_loss: 34074.2461 - val_reconstruction_loss: 5388.4595 - val_kl_loss: 14341.4375 - val_perceptual_loss: 2.9116 - lr: 1.0000e-04\n",
      "Epoch 38/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 53289.3819 - reconstruction_loss: 15578.9531 - kl_loss: 38746.4922 - perceptual_loss: 7.7995\n",
      "Epoch 38: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 21s 187ms/step - loss: 53298.7861 - reconstruction_loss: 15578.9531 - kl_loss: 38746.4922 - perceptual_loss: 7.7995 - val_loss: 30591.3086 - val_reconstruction_loss: 5326.4492 - val_kl_loss: 12630.9443 - val_perceptual_loss: 2.9701 - lr: 1.0000e-04\n",
      "Epoch 39/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 53220.2277 - reconstruction_loss: 15651.5957 - kl_loss: 39243.0664 - perceptual_loss: 7.8143\n",
      "Epoch 39: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 53235.3831 - reconstruction_loss: 15651.5957 - kl_loss: 39243.0664 - perceptual_loss: 7.8143 - val_loss: 37668.0195 - val_reconstruction_loss: 5970.6743 - val_kl_loss: 15846.5957 - val_perceptual_loss: 4.1528 - lr: 1.0000e-04\n",
      "Epoch 40/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 51848.5805 - reconstruction_loss: 15847.1787 - kl_loss: 39484.2734 - perceptual_loss: 7.8892\n",
      "Epoch 40: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 180ms/step - loss: 51880.0288 - reconstruction_loss: 15847.1787 - kl_loss: 39484.2734 - perceptual_loss: 7.8892 - val_loss: 27369.4805 - val_reconstruction_loss: 5975.9766 - val_kl_loss: 10694.7402 - val_perceptual_loss: 4.0233 - lr: 1.0000e-04\n",
      "Epoch 41/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 54123.0413 - reconstruction_loss: 15760.6729 - kl_loss: 39107.9648 - perceptual_loss: 7.8944\n",
      "Epoch 41: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 21s 186ms/step - loss: 54129.8296 - reconstruction_loss: 15760.6729 - kl_loss: 39107.9648 - perceptual_loss: 7.8944 - val_loss: 30704.5273 - val_reconstruction_loss: 5604.7725 - val_kl_loss: 12548.2686 - val_perceptual_loss: 3.2185 - lr: 1.0000e-04\n",
      "Epoch 42/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 57597.4775 - reconstruction_loss: 15465.8701 - kl_loss: 38709.6445 - perceptual_loss: 7.7123\n",
      "Epoch 42: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 21s 194ms/step - loss: 57566.7184 - reconstruction_loss: 15465.8701 - kl_loss: 38709.6445 - perceptual_loss: 7.7123 - val_loss: 29323.1348 - val_reconstruction_loss: 4819.3379 - val_kl_loss: 12250.5742 - val_perceptual_loss: 2.6490 - lr: 1.0000e-04\n",
      "Epoch 43/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 53404.4666 - reconstruction_loss: 15637.5283 - kl_loss: 38266.2227 - perceptual_loss: 7.9384\n",
      "Epoch 43: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 21s 187ms/step - loss: 53409.0360 - reconstruction_loss: 15637.5283 - kl_loss: 38266.2227 - perceptual_loss: 7.9384 - val_loss: 33664.2500 - val_reconstruction_loss: 5873.7402 - val_kl_loss: 13893.3838 - val_perceptual_loss: 3.7412 - lr: 1.0000e-04\n",
      "Epoch 44/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 54511.2540 - reconstruction_loss: 15451.3555 - kl_loss: 37422.4922 - perceptual_loss: 7.7202\n",
      "Epoch 44: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 54496.5722 - reconstruction_loss: 15451.3555 - kl_loss: 37422.4922 - perceptual_loss: 7.7202 - val_loss: 31094.0742 - val_reconstruction_loss: 5805.0054 - val_kl_loss: 12642.3770 - val_perceptual_loss: 4.3153 - lr: 1.0000e-04\n",
      "Epoch 45/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 51026.4787 - reconstruction_loss: 15440.4150 - kl_loss: 36932.0352 - perceptual_loss: 7.8047\n",
      "Epoch 45: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 21s 186ms/step - loss: 51038.6750 - reconstruction_loss: 15440.4150 - kl_loss: 36932.0352 - perceptual_loss: 7.8047 - val_loss: 38010.5156 - val_reconstruction_loss: 6499.0527 - val_kl_loss: 15754.0996 - val_perceptual_loss: 3.2656 - lr: 1.0000e-04\n",
      "Epoch 46/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 49003.4619 - reconstruction_loss: 15155.5928 - kl_loss: 36375.9062 - perceptual_loss: 7.5961\n",
      "Epoch 46: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 21s 185ms/step - loss: 49026.3054 - reconstruction_loss: 15155.5928 - kl_loss: 36375.9062 - perceptual_loss: 7.5961 - val_loss: 28621.8711 - val_reconstruction_loss: 5606.1069 - val_kl_loss: 11506.1699 - val_perceptual_loss: 3.4239 - lr: 1.0000e-04\n",
      "Epoch 47/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 49788.3184 - reconstruction_loss: 14825.9834 - kl_loss: 35471.0742 - perceptual_loss: 7.5637\n",
      "Epoch 47: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 183ms/step - loss: 49792.9698 - reconstruction_loss: 14825.9834 - kl_loss: 35471.0742 - perceptual_loss: 7.5637 - val_loss: 35496.2266 - val_reconstruction_loss: 6111.4150 - val_kl_loss: 14690.3037 - val_perceptual_loss: 4.2052 - lr: 1.0000e-04\n",
      "Epoch 48/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 50902.2064 - reconstruction_loss: 15150.1748 - kl_loss: 36421.1406 - perceptual_loss: 7.6018\n",
      "Epoch 48: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 50908.3029 - reconstruction_loss: 15150.1748 - kl_loss: 36421.1406 - perceptual_loss: 7.6018 - val_loss: 32013.7656 - val_reconstruction_loss: 5824.0205 - val_kl_loss: 13092.7500 - val_perceptual_loss: 4.2466 - lr: 1.0000e-04\n",
      "Epoch 49/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 47444.7812 - reconstruction_loss: 14842.1865 - kl_loss: 35102.2852 - perceptual_loss: 7.5300\n",
      "Epoch 49: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 19s 172ms/step - loss: 47467.3688 - reconstruction_loss: 14842.1865 - kl_loss: 35102.2852 - perceptual_loss: 7.5300 - val_loss: 33695.8828 - val_reconstruction_loss: 6299.0107 - val_kl_loss: 13696.3633 - val_perceptual_loss: 4.1442 - lr: 1.0000e-04\n",
      "Epoch 50/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 46928.9762 - reconstruction_loss: 14850.6855 - kl_loss: 34897.0469 - perceptual_loss: 7.4172\n",
      "Epoch 50: val_reconstruction_loss did not improve from 4801.39697\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "110/110 [==============================] - 19s 171ms/step - loss: 46954.4371 - reconstruction_loss: 14850.6855 - kl_loss: 34897.0469 - perceptual_loss: 7.4172 - val_loss: 25817.3867 - val_reconstruction_loss: 5021.8901 - val_kl_loss: 10396.1582 - val_perceptual_loss: 3.1802 - lr: 1.0000e-04\n",
      "Epoch 51/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 43554.6296 - reconstruction_loss: 14142.2959 - kl_loss: 31805.6250 - perceptual_loss: 7.3041\n",
      "Epoch 51: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 183ms/step - loss: 43576.2566 - reconstruction_loss: 14142.2959 - kl_loss: 31805.6250 - perceptual_loss: 7.3041 - val_loss: 27991.3926 - val_reconstruction_loss: 5191.9438 - val_kl_loss: 11398.4307 - val_perceptual_loss: 2.5875 - lr: 5.0000e-05\n",
      "Epoch 52/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 42780.6146 - reconstruction_loss: 14368.6055 - kl_loss: 32974.2734 - perceptual_loss: 7.3810\n",
      "Epoch 52: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 42821.7826 - reconstruction_loss: 14368.6055 - kl_loss: 32974.2734 - perceptual_loss: 7.3810 - val_loss: 27176.1172 - val_reconstruction_loss: 5224.3281 - val_kl_loss: 10974.2998 - val_perceptual_loss: 3.1893 - lr: 5.0000e-05\n",
      "Epoch 53/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 46634.7654 - reconstruction_loss: 14094.2617 - kl_loss: 32037.4453 - perceptual_loss: 7.2940\n",
      "Epoch 53: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 21s 185ms/step - loss: 46630.2991 - reconstruction_loss: 14094.2617 - kl_loss: 32037.4453 - perceptual_loss: 7.2940 - val_loss: 24395.4688 - val_reconstruction_loss: 4917.0229 - val_kl_loss: 9737.6592 - val_perceptual_loss: 3.1267 - lr: 5.0000e-05\n",
      "Epoch 54/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 43646.1340 - reconstruction_loss: 13952.4365 - kl_loss: 31140.5723 - perceptual_loss: 7.1422\n",
      "Epoch 54: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 179ms/step - loss: 43659.2332 - reconstruction_loss: 13952.4365 - kl_loss: 31140.5723 - perceptual_loss: 7.1422 - val_loss: 23169.2031 - val_reconstruction_loss: 4881.6250 - val_kl_loss: 9142.4629 - val_perceptual_loss: 2.6532 - lr: 5.0000e-05\n",
      "Epoch 55/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 43362.3730 - reconstruction_loss: 13762.4014 - kl_loss: 30096.5645 - perceptual_loss: 7.1512\n",
      "Epoch 55: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 43366.9111 - reconstruction_loss: 13762.4014 - kl_loss: 30096.5645 - perceptual_loss: 7.1512 - val_loss: 26858.8516 - val_reconstruction_loss: 5281.7500 - val_kl_loss: 10787.0928 - val_perceptual_loss: 2.9152 - lr: 5.0000e-05\n",
      "Epoch 56/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 44866.7681 - reconstruction_loss: 13816.7139 - kl_loss: 30388.3926 - perceptual_loss: 7.2171\n",
      "Epoch 56: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 182ms/step - loss: 44860.8722 - reconstruction_loss: 13816.7139 - kl_loss: 30388.3926 - perceptual_loss: 7.2171 - val_loss: 26500.0723 - val_reconstruction_loss: 5102.7402 - val_kl_loss: 10697.2754 - val_perceptual_loss: 2.7815 - lr: 5.0000e-05\n",
      "Epoch 57/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 45727.6973 - reconstruction_loss: 14039.2266 - kl_loss: 31143.4082 - perceptual_loss: 7.2067\n",
      "Epoch 57: val_reconstruction_loss did not improve from 4801.39697\n",
      "110/110 [==============================] - 20s 183ms/step - loss: 45722.8518 - reconstruction_loss: 14039.2266 - kl_loss: 31143.4082 - perceptual_loss: 7.2067 - val_loss: 28057.8027 - val_reconstruction_loss: 5315.4243 - val_kl_loss: 11369.5928 - val_perceptual_loss: 3.1929 - lr: 5.0000e-05\n",
      "Epoch 58/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 45362.2588 - reconstruction_loss: 13806.5391 - kl_loss: 30605.5156 - perceptual_loss: 7.1835\n",
      "Epoch 58: val_reconstruction_loss improved from 4801.39697 to 4586.18359, saving model to checkpoints\\fourierTrue_percepTrue_random_32\\checkpoint_fourierTrue_percepTrue_random_32_flrnet_best\n",
      "110/110 [==============================] - 22s 193ms/step - loss: 45353.7631 - reconstruction_loss: 13806.5391 - kl_loss: 30605.5156 - perceptual_loss: 7.1835 - val_loss: 21745.2949 - val_reconstruction_loss: 4586.1836 - val_kl_loss: 8578.1289 - val_perceptual_loss: 2.8527 - lr: 5.0000e-05\n",
      "Epoch 59/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 45988.9474 - reconstruction_loss: 13944.3545 - kl_loss: 31286.1934 - perceptual_loss: 7.2537\n",
      "Epoch 59: val_reconstruction_loss did not improve from 4586.18359\n",
      "110/110 [==============================] - 20s 183ms/step - loss: 45982.1805 - reconstruction_loss: 13944.3545 - kl_loss: 31286.1934 - perceptual_loss: 7.2537 - val_loss: 24001.8242 - val_reconstruction_loss: 5188.2842 - val_kl_loss: 9404.9404 - val_perceptual_loss: 3.6591 - lr: 5.0000e-05\n",
      "Epoch 60/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 42373.8330 - reconstruction_loss: 13710.8848 - kl_loss: 29745.1855 - perceptual_loss: 7.0906\n",
      "Epoch 60: val_reconstruction_loss improved from 4586.18359 to 4363.81592, saving model to checkpoints\\fourierTrue_percepTrue_random_32\\checkpoint_fourierTrue_percepTrue_random_32_flrnet_best\n",
      "110/110 [==============================] - 21s 193ms/step - loss: 42383.6467 - reconstruction_loss: 13710.8848 - kl_loss: 29745.1855 - perceptual_loss: 7.0906 - val_loss: 23717.6348 - val_reconstruction_loss: 4363.8159 - val_kl_loss: 9675.5596 - val_perceptual_loss: 2.6996 - lr: 5.0000e-05\n",
      "Epoch 61/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 44071.5358 - reconstruction_loss: 13792.2900 - kl_loss: 30333.9219 - perceptual_loss: 7.1295\n",
      "Epoch 61: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 20s 177ms/step - loss: 44072.0926 - reconstruction_loss: 13792.2900 - kl_loss: 30333.9219 - perceptual_loss: 7.1295 - val_loss: 28600.3398 - val_reconstruction_loss: 4733.9751 - val_kl_loss: 11931.8066 - val_perceptual_loss: 2.7524 - lr: 5.0000e-05\n",
      "Epoch 62/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 44330.9764 - reconstruction_loss: 13557.8877 - kl_loss: 29657.9492 - perceptual_loss: 7.1037\n",
      "Epoch 62: val_reconstruction_loss did not improve from 4363.81592\n",
      "‚ö†Ô∏è Checkpoint saving failed at epoch 62: Failed to rename: checkpoints\\fourierTrue_percepTrue_random_32\\checkpoint.tmp8dd472b3998a4880b81385bfa12f8925 to: checkpoints\\fourierTrue_percepTrue_random_32\\checkpoint : Access is denied.\n",
      "; Input/output error\n",
      "   Continuing training without saving this checkpoint...\n",
      "110/110 [==============================] - 20s 176ms/step - loss: 44320.9941 - reconstruction_loss: 13557.8877 - kl_loss: 29657.9492 - perceptual_loss: 7.1037 - val_loss: 27921.6641 - val_reconstruction_loss: 5111.9336 - val_kl_loss: 11403.5615 - val_perceptual_loss: 2.6079 - lr: 5.0000e-05\n",
      "Epoch 63/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 44929.8355 - reconstruction_loss: 13717.3223 - kl_loss: 30179.4590 - perceptual_loss: 7.1713\n",
      "Epoch 63: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 44920.5933 - reconstruction_loss: 13717.3223 - kl_loss: 30179.4590 - perceptual_loss: 7.1713 - val_loss: 28227.8340 - val_reconstruction_loss: 5167.1953 - val_kl_loss: 11528.8350 - val_perceptual_loss: 2.9689 - lr: 5.0000e-05\n",
      "Epoch 64/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 42861.1069 - reconstruction_loss: 13599.5850 - kl_loss: 29540.9531 - perceptual_loss: 7.1380\n",
      "Epoch 64: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 176ms/step - loss: 42863.6887 - reconstruction_loss: 13599.5850 - kl_loss: 29540.9531 - perceptual_loss: 7.1380 - val_loss: 27531.3574 - val_reconstruction_loss: 5012.1040 - val_kl_loss: 11258.2314 - val_perceptual_loss: 2.7919 - lr: 5.0000e-05\n",
      "Epoch 65/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 40666.0639 - reconstruction_loss: 13444.4609 - kl_loss: 28899.4727 - perceptual_loss: 7.0238\n",
      "Epoch 65: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 40681.2430 - reconstruction_loss: 13444.4609 - kl_loss: 28899.4727 - perceptual_loss: 7.0238 - val_loss: 24961.4648 - val_reconstruction_loss: 5031.5635 - val_kl_loss: 9963.4316 - val_perceptual_loss: 3.0392 - lr: 5.0000e-05\n",
      "Epoch 66/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 42299.9735 - reconstruction_loss: 13737.6807 - kl_loss: 29567.1074 - perceptual_loss: 7.1019\n",
      "Epoch 66: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 172ms/step - loss: 42309.0897 - reconstruction_loss: 13737.6807 - kl_loss: 29567.1074 - perceptual_loss: 7.1019 - val_loss: 26687.9805 - val_reconstruction_loss: 4852.3369 - val_kl_loss: 10916.4424 - val_perceptual_loss: 2.7596 - lr: 5.0000e-05\n",
      "Epoch 67/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 38978.4715 - reconstruction_loss: 13555.4531 - kl_loss: 28908.8105 - perceptual_loss: 6.9972\n",
      "Epoch 67: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 172ms/step - loss: 39009.9381 - reconstruction_loss: 13555.4531 - kl_loss: 28908.8105 - perceptual_loss: 6.9972 - val_loss: 26147.8809 - val_reconstruction_loss: 4976.2046 - val_kl_loss: 10584.3955 - val_perceptual_loss: 2.8839 - lr: 5.0000e-05\n",
      "Epoch 68/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 44663.3137 - reconstruction_loss: 13468.0986 - kl_loss: 29116.8301 - perceptual_loss: 7.1225\n",
      "Epoch 68: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 44644.6538 - reconstruction_loss: 13468.0986 - kl_loss: 29116.8301 - perceptual_loss: 7.1225 - val_loss: 22786.3965 - val_reconstruction_loss: 4401.3047 - val_kl_loss: 9191.2812 - val_perceptual_loss: 2.5290 - lr: 5.0000e-05\n",
      "Epoch 69/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 40770.4331 - reconstruction_loss: 13316.3545 - kl_loss: 27990.9590 - perceptual_loss: 6.9528\n",
      "Epoch 69: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 40775.3324 - reconstruction_loss: 13316.3545 - kl_loss: 27990.9590 - perceptual_loss: 6.9528 - val_loss: 25538.4141 - val_reconstruction_loss: 4877.9927 - val_kl_loss: 10328.9033 - val_perceptual_loss: 2.6159 - lr: 5.0000e-05\n",
      "Epoch 70/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 39677.8044 - reconstruction_loss: 13311.0703 - kl_loss: 28429.2402 - perceptual_loss: 7.0201\n",
      "Epoch 70: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 171ms/step - loss: 39696.4488 - reconstruction_loss: 13311.0703 - kl_loss: 28429.2402 - perceptual_loss: 7.0201 - val_loss: 26071.2168 - val_reconstruction_loss: 4937.3413 - val_kl_loss: 10565.5781 - val_perceptual_loss: 2.7185 - lr: 5.0000e-05\n",
      "Epoch 71/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 40466.9067 - reconstruction_loss: 13584.1934 - kl_loss: 29067.8242 - perceptual_loss: 6.9650\n",
      "Epoch 71: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 40486.6550 - reconstruction_loss: 13584.1934 - kl_loss: 29067.8242 - perceptual_loss: 6.9650 - val_loss: 26116.4316 - val_reconstruction_loss: 4928.2202 - val_kl_loss: 10592.7764 - val_perceptual_loss: 2.6574 - lr: 5.0000e-05\n",
      "Epoch 72/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 41053.7897 - reconstruction_loss: 13303.9287 - kl_loss: 27771.1895 - perceptual_loss: 6.8808\n",
      "Epoch 72: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 41054.0438 - reconstruction_loss: 13303.9287 - kl_loss: 27771.1895 - perceptual_loss: 6.8808 - val_loss: 22738.8672 - val_reconstruction_loss: 5117.7959 - val_kl_loss: 8808.9834 - val_perceptual_loss: 3.1061 - lr: 5.0000e-05\n",
      "Epoch 73/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 41164.6499 - reconstruction_loss: 13462.1514 - kl_loss: 28528.6348 - perceptual_loss: 6.8722\n",
      "Epoch 73: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 175ms/step - loss: 41172.1544 - reconstruction_loss: 13462.1514 - kl_loss: 28528.6348 - perceptual_loss: 6.8722 - val_loss: 29047.6836 - val_reconstruction_loss: 5400.4531 - val_kl_loss: 11821.9844 - val_perceptual_loss: 3.2622 - lr: 5.0000e-05\n",
      "Epoch 74/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 42516.6078 - reconstruction_loss: 13343.6523 - kl_loss: 27925.1348 - perceptual_loss: 7.0713\n",
      "Epoch 74: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 42505.4299 - reconstruction_loss: 13343.6523 - kl_loss: 27925.1348 - perceptual_loss: 7.0713 - val_loss: 31777.6094 - val_reconstruction_loss: 5108.7329 - val_kl_loss: 13333.0078 - val_perceptual_loss: 2.8612 - lr: 5.0000e-05\n",
      "Epoch 75/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 38568.9687 - reconstruction_loss: 13296.0068 - kl_loss: 28765.3457 - perceptual_loss: 6.9493\n",
      "Epoch 75: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 38600.4942 - reconstruction_loss: 13296.0068 - kl_loss: 28765.3457 - perceptual_loss: 6.9493 - val_loss: 25864.6367 - val_reconstruction_loss: 5138.1729 - val_kl_loss: 10361.7432 - val_perceptual_loss: 2.9791 - lr: 5.0000e-05\n",
      "Epoch 76/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 42997.3506 - reconstruction_loss: 13413.8379 - kl_loss: 28526.6992 - perceptual_loss: 6.9938\n",
      "Epoch 76: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 42987.8927 - reconstruction_loss: 13413.8379 - kl_loss: 28526.6992 - perceptual_loss: 6.9938 - val_loss: 27985.9004 - val_reconstruction_loss: 5004.9224 - val_kl_loss: 11489.2422 - val_perceptual_loss: 2.4935 - lr: 5.0000e-05\n",
      "Epoch 77/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 39887.5845 - reconstruction_loss: 13263.1367 - kl_loss: 27939.2695 - perceptual_loss: 6.8397\n",
      "Epoch 77: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 20s 177ms/step - loss: 39899.4914 - reconstruction_loss: 13263.1367 - kl_loss: 27939.2695 - perceptual_loss: 6.8397 - val_loss: 21267.5527 - val_reconstruction_loss: 4476.3672 - val_kl_loss: 8394.2012 - val_perceptual_loss: 2.7827 - lr: 5.0000e-05\n",
      "Epoch 78/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 42523.0052 - reconstruction_loss: 13360.9492 - kl_loss: 28397.1445 - perceptual_loss: 6.9931\n",
      "Epoch 78: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 20s 176ms/step - loss: 42516.1771 - reconstruction_loss: 13360.9492 - kl_loss: 28397.1445 - perceptual_loss: 6.9931 - val_loss: 34890.0039 - val_reconstruction_loss: 5756.3271 - val_kl_loss: 14565.2764 - val_perceptual_loss: 3.1224 - lr: 5.0000e-05\n",
      "Epoch 79/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 40867.4790 - reconstruction_loss: 13323.3047 - kl_loss: 27813.2051 - perceptual_loss: 6.9450\n",
      "Epoch 79: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 40869.9652 - reconstruction_loss: 13323.3047 - kl_loss: 27813.2051 - perceptual_loss: 6.9450 - val_loss: 25106.8320 - val_reconstruction_loss: 4871.6934 - val_kl_loss: 10116.2334 - val_perceptual_loss: 2.6720 - lr: 5.0000e-05\n",
      "Epoch 80/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 39129.5390 - reconstruction_loss: 13392.4541 - kl_loss: 28070.0996 - perceptual_loss: 6.9314\n",
      "Epoch 80: val_reconstruction_loss did not improve from 4363.81592\n",
      "\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 39150.6197 - reconstruction_loss: 13392.4541 - kl_loss: 28070.0996 - perceptual_loss: 6.9314 - val_loss: 24989.5781 - val_reconstruction_loss: 4577.9229 - val_kl_loss: 10204.4639 - val_perceptual_loss: 2.7293 - lr: 5.0000e-05\n",
      "Epoch 81/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 39533.3678 - reconstruction_loss: 12926.6924 - kl_loss: 26604.4277 - perceptual_loss: 6.8151\n",
      "Epoch 81: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 39533.4087 - reconstruction_loss: 12926.6924 - kl_loss: 26604.4277 - perceptual_loss: 6.8151 - val_loss: 24554.7461 - val_reconstruction_loss: 4729.2725 - val_kl_loss: 9911.3379 - val_perceptual_loss: 2.7969 - lr: 2.5000e-05\n",
      "Epoch 82/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 38553.8167 - reconstruction_loss: 12880.3398 - kl_loss: 25998.5137 - perceptual_loss: 6.7097\n",
      "Epoch 82: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 172ms/step - loss: 38556.8055 - reconstruction_loss: 12880.3398 - kl_loss: 25998.5137 - perceptual_loss: 6.7097 - val_loss: 30735.2930 - val_reconstruction_loss: 5390.2476 - val_kl_loss: 12671.0303 - val_perceptual_loss: 2.9842 - lr: 2.5000e-05\n",
      "Epoch 83/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 40538.1021 - reconstruction_loss: 12820.3936 - kl_loss: 26091.4805 - perceptual_loss: 6.7275\n",
      "Epoch 83: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 175ms/step - loss: 40523.5120 - reconstruction_loss: 12820.3936 - kl_loss: 26091.4805 - perceptual_loss: 6.7275 - val_loss: 26199.4863 - val_reconstruction_loss: 4919.9941 - val_kl_loss: 10638.3906 - val_perceptual_loss: 2.7115 - lr: 2.5000e-05\n",
      "Epoch 84/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 35988.8037 - reconstruction_loss: 12757.9180 - kl_loss: 25816.5645 - perceptual_loss: 6.7426\n",
      "Epoch 84: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 20s 176ms/step - loss: 36012.1588 - reconstruction_loss: 12757.9180 - kl_loss: 25816.5645 - perceptual_loss: 6.7426 - val_loss: 22927.5547 - val_reconstruction_loss: 4655.3779 - val_kl_loss: 9134.7607 - val_perceptual_loss: 2.6564 - lr: 2.5000e-05\n",
      "Epoch 85/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 39218.2647 - reconstruction_loss: 12771.5996 - kl_loss: 26181.3984 - perceptual_loss: 6.7241\n",
      "Epoch 85: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 175ms/step - loss: 39215.9355 - reconstruction_loss: 12771.5996 - kl_loss: 26181.3984 - perceptual_loss: 6.7241 - val_loss: 27946.7969 - val_reconstruction_loss: 5084.0850 - val_kl_loss: 11429.9111 - val_perceptual_loss: 2.8878 - lr: 2.5000e-05\n",
      "Epoch 86/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 39674.3749 - reconstruction_loss: 12846.4287 - kl_loss: 26304.5742 - perceptual_loss: 6.6996\n",
      "Epoch 86: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 39669.7203 - reconstruction_loss: 12846.4287 - kl_loss: 26304.5742 - perceptual_loss: 6.6996 - val_loss: 25428.6230 - val_reconstruction_loss: 4871.5312 - val_kl_loss: 10277.2061 - val_perceptual_loss: 2.6794 - lr: 2.5000e-05\n",
      "Epoch 87/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 40400.4132 - reconstruction_loss: 12729.5000 - kl_loss: 26050.0156 - perceptual_loss: 6.6757\n",
      "Epoch 87: val_reconstruction_loss did not improve from 4363.81592\n",
      "‚ö†Ô∏è Checkpoint saving failed at epoch 87: Failed to rename: checkpoints\\fourierTrue_percepTrue_random_32\\checkpoint.tmpbb149d16ae1c4357825bb0e860f24416 to: checkpoints\\fourierTrue_percepTrue_random_32\\checkpoint : Access is denied.\n",
      "; Input/output error\n",
      "   Continuing training without saving this checkpoint...\n",
      "110/110 [==============================] - 20s 176ms/step - loss: 40385.8708 - reconstruction_loss: 12729.5000 - kl_loss: 26050.0156 - perceptual_loss: 6.6757 - val_loss: 27262.4512 - val_reconstruction_loss: 4956.2441 - val_kl_loss: 11151.7656 - val_perceptual_loss: 2.6751 - lr: 2.5000e-05\n",
      "Epoch 88/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 39383.9849 - reconstruction_loss: 12622.4521 - kl_loss: 25327.9297 - perceptual_loss: 6.6878\n",
      "Epoch 88: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 20s 175ms/step - loss: 39371.1298 - reconstruction_loss: 12622.4521 - kl_loss: 25327.9297 - perceptual_loss: 6.6878 - val_loss: 25039.9746 - val_reconstruction_loss: 4959.0640 - val_kl_loss: 10038.9229 - val_perceptual_loss: 3.0635 - lr: 2.5000e-05\n",
      "Epoch 89/150\n",
      "110/110 [==============================] - ETA: 0s - loss: 39673.7796 - reconstruction_loss: 12625.7275 - kl_loss: 25685.5254 - perceptual_loss: 6.6133\n",
      "Epoch 89: val_reconstruction_loss did not improve from 4363.81592\n",
      "110/110 [==============================] - 20s 175ms/step - loss: 39661.5642 - reconstruction_loss: 12625.7275 - kl_loss: 25685.5254 - perceptual_loss: 6.6133 - val_loss: 27458.8594 - val_reconstruction_loss: 5138.3496 - val_kl_loss: 11158.7256 - val_perceptual_loss: 3.0580 - lr: 2.5000e-05\n",
      "Epoch 90/150\n",
      " 93/110 [========================>.....] - ETA: 1s - loss: 36892.5658 - reconstruction_loss: 12369.1572 - kl_loss: 24932.0586 - perceptual_loss: 6.3491"
     ]
    }
   ],
   "source": [
    "# Train FLRNet model using FLRTrainer\n",
    "if train_flrnet_model:\n",
    "    print(\"\\nüîÑ Starting FLRNet Training using FLRTrainer...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Train FLRNet using the trainer\n",
    "    flr_model = trainer.train_flr_net(\n",
    "        train_dataset=flrnet_train_dataset,\n",
    "        val_dataset=flrnet_test_dataset,\n",
    "        n_sensors=config['n_sensors'],\n",
    "        epochs=config['flr_epochs'],\n",
    "        learning_rate=config['flr_learning_rate'],\n",
    "        pretrained_vae=vae_model,  # Use the VAE model we just trained\n",
    "        latent_dims=config['latent_dims'],\n",
    "        n_base_features=config['n_base_features'],\n",
    "        use_perceptual_loss=config['use_perceptual_loss'],\n",
    "        patience=config['patience'],\n",
    "        reduce_lr_patience=config['reduce_lr_patience']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ FLRNet training completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping FLRNet training. Loading existing model...\")\n",
    "    # The trainer will handle loading existing FLRNet weights automatically\n",
    "    flr_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e2163b",
   "metadata": {},
   "source": [
    "## FLRNet Validation and Visualization\n",
    "\n",
    "After training the FLRNet model, we can evaluate its performance by:\n",
    "1. Testing sensor-to-field reconstruction accuracy\n",
    "2. Comparing predicted vs ground truth flow fields\n",
    "3. Analyzing reconstruction quality metrics\n",
    "4. Visualizing sensor positions overlaid on predictions\n",
    "\n",
    "This section validates the main FLRNet model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load FLRNet Model\n",
    "print(\"üöÄ Loading trained FLRNet model from checkpoint using FLRTrainer...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the same checkpoint directory\n",
    "checkpoint_directory = Path(config['checkpoint_dir'])\n",
    "print(f\"üìÇ Using checkpoint directory: {checkpoint_directory}\")\n",
    "\n",
    "# List available checkpoints for debugging\n",
    "if checkpoint_directory.exists():\n",
    "    print(f\"üìã Available FLRNet checkpoint files:\")\n",
    "    flrnet_files = [f for f in sorted(checkpoint_directory.iterdir()) if 'flrnet' in f.name.lower()]\n",
    "    for file in flrnet_files:\n",
    "        print(f\"   - {file.name}\")\n",
    "    \n",
    "    if not flrnet_files:\n",
    "        print(\"   ‚ö†Ô∏è  No FLRNet checkpoint files found\")\n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint directory not found: {checkpoint_directory}\")\n",
    "\n",
    "# Use the trainer's built-in checkpoint loading method for FLRNet\n",
    "flr_model = trainer.load_flrnet_from_checkpoint(\n",
    "    checkpoint_dir=checkpoint_directory,\n",
    "    n_sensors=config['n_sensors'],\n",
    "    latent_dims=config['latent_dims'],\n",
    "    n_base_features=config['n_base_features'],\n",
    "    use_perceptual_loss=config['use_perceptual_loss'],\n",
    "    pretrained_vae=vae_model,  # Use the VAE model we just loaded\n",
    "    freeze_autoencoder=True,   # Keep autoencoder frozen for inference\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if flr_model is not None:\n",
    "    print(\"\\nüéâ FLRNet model loaded successfully!\")\n",
    "    print(\"üìà Model is ready for sensor-to-field reconstruction\")\n",
    "    \n",
    "    # Verify the model works with a test prediction\n",
    "    try:\n",
    "        # Get a small batch for testing\n",
    "        test_batch = next(iter(flrnet_test_dataset.take(1)))\n",
    "        \n",
    "        # Handle different input formats based on Fourier configuration\n",
    "        if config['use_fourier']:\n",
    "            # Fourier FLRNet expects (sensor_data, field_data, coordinates)\n",
    "            sensor_data, field_data, coord_data = test_batch\n",
    "            print(f\"‚úÖ Fourier FLRNet input format detected:\")\n",
    "            print(f\"   - Sensor data shape: {sensor_data[:1].shape}\")\n",
    "            print(f\"   - Field data shape: {field_data[:1].shape}\")\n",
    "            print(f\"   - Coordinate data shape: {coord_data[:1].shape}\")\n",
    "            \n",
    "            # Test prediction with all inputs (training format)\n",
    "            test_prediction = flr_model([sensor_data[:1], field_data[:1], coord_data[:1]], training=False)\n",
    "            print(f\"‚úÖ Fourier FLRNet verification successful!\")\n",
    "            \n",
    "            # Test prediction with just sensor data (inference format)\n",
    "            sensor_only_prediction = flr_model(sensor_data[:1], training=False)\n",
    "            print(f\"‚úÖ Sensor-only prediction also works!\")\n",
    "            \n",
    "        else:\n",
    "            # Standard FLRNet expects (sensor_data, field_data)\n",
    "            sensor_data, field_data = test_batch\n",
    "            print(f\"‚úÖ Standard FLRNet input format detected:\")\n",
    "            print(f\"   - Sensor data shape: {sensor_data[:1].shape}\")\n",
    "            print(f\"   - Field data shape: {field_data[:1].shape}\")\n",
    "            \n",
    "            # Test prediction\n",
    "            test_prediction = flr_model([sensor_data[:1], field_data[:1]], training=False)\n",
    "            print(f\"‚úÖ Standard FLRNet verification successful!\")\n",
    "            \n",
    "            # Test prediction with just sensor data (inference format)\n",
    "            sensor_only_prediction = flr_model(sensor_data[:1], training=False)\n",
    "            print(f\"‚úÖ Sensor-only prediction also works!\")\n",
    "        \n",
    "        print(f\"   - Output shape: {test_prediction.shape}\")\n",
    "        print(f\"   - Output range: [{test_prediction.numpy().min():.4f}, {test_prediction.numpy().max():.4f}]\")\n",
    "        \n",
    "        # Check if model has the new perceptual loss features\n",
    "        if hasattr(flr_model, 'use_perceptual_loss'):\n",
    "            print(f\"   - Perceptual loss enabled: {flr_model.use_perceptual_loss}\")\n",
    "            print(f\"   - Available metrics: {[metric.name for metric in flr_model.metrics]}\")\n",
    "        else:\n",
    "            print(\"   - Legacy model (no perceptual loss configuration)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Model verification failed: {str(e)}\")\n",
    "        print(\"Model loaded but may have compatibility issues\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to load FLRNet model\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Check if FLRNet training completed successfully\")\n",
    "    print(\"   2. Verify FLRNet checkpoint files exist in the directory\")\n",
    "    print(\"   3. Ensure model configuration matches training setup\")\n",
    "    print(\"   4. Make sure VAE model is loaded first (FLRNet depends on it)\")\n",
    "    print(\"   5. Try running the FLRNet training cell if checkpoints are missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue FLRNet Training from Checkpoint\n",
    "# Set this to True if you want to continue training from the loaded checkpoint\n",
    "continue_flrnet_training = False  # Change to True to enable\n",
    "\n",
    "# Toggle for perceptual loss during continuation (can be different from original training)\n",
    "use_perceptual_loss_continuation = True  # Change to False to disable perceptual loss\n",
    "\n",
    "if continue_flrnet_training and flr_model is not None:\n",
    "    print(\"üîÑ Continuing FLRNet training from loaded checkpoint...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration for continued training\n",
    "    additional_epochs = 50\n",
    "    learning_rate = 1e-5  # Lower learning rate for fine-tuning\n",
    "    \n",
    "    print(f\"üìã Continuation Configuration:\")\n",
    "    print(f\"   - Additional epochs: {additional_epochs}\")\n",
    "    print(f\"   - Learning rate: {learning_rate} (reduced for fine-tuning)\")\n",
    "    print(f\"   - Perceptual loss: {'‚úÖ ENABLED' if use_perceptual_loss_continuation else '‚ùå DISABLED'}\")\n",
    "    print(f\"   - Model weights: ‚úÖ PRESERVED from checkpoint\")\n",
    "    print(f\"   - VAE dependency: {'‚úÖ LOADED' if vae_model is not None else '‚ùå MISSING'}\")\n",
    "    \n",
    "    # Ensure VAE model is available (required for FLRNet)\n",
    "    if vae_model is None:\n",
    "        print(\"\\nüîÑ Loading VAE model first (required for FLRNet)...\")\n",
    "        vae_model = trainer.load_vae_from_checkpoint(\n",
    "            checkpoint_dir=checkpoint_directory,\n",
    "            latent_dims=config['latent_dims'],\n",
    "            n_base_features=config['n_base_features'],\n",
    "            use_perceptual_loss=config['use_perceptual_loss'],\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    if vae_model is not None:\n",
    "        print(\"\\nüöÄ Method 1: Using FLRTrainer (Recommended)\")\n",
    "        \n",
    "        # Set the loaded models in the trainer\n",
    "        trainer.flr_model = flr_model\n",
    "        trainer.vae_model = vae_model\n",
    "        \n",
    "        # Continue training with the trainer's enhanced method\n",
    "        continued_flrnet = trainer.continue_flrnet_training(\n",
    "            train_dataset=flrnet_train_dataset,\n",
    "            val_dataset=flrnet_test_dataset,\n",
    "            epochs=additional_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            patience=config['patience'],\n",
    "            reduce_lr_patience=config['reduce_lr_patience'],\n",
    "            use_perceptual_loss=use_perceptual_loss_continuation  # New option for perceptual loss\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ FLRNet training continuation completed!\")\n",
    "        \n",
    "        # Update the model reference\n",
    "        if continued_flrnet is not None:\n",
    "            flr_model = continued_flrnet\n",
    "            print(\"üîÑ Model reference updated to continued version\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Could not load VAE model - required for FLRNet training\")\n",
    "        print(\"   FLRNet training requires a pre-trained VAE model\")\n",
    "        \n",
    "elif continue_flrnet_training and flr_model is None:\n",
    "    print(\"‚ùå Cannot continue FLRNet training: No model loaded from checkpoint\")\n",
    "    print(\"   First load a model using the FLRNet checkpoint loading cell above\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  FLRNet training continuation is disabled\")\n",
    "    print(\"   Set continue_flrnet_training = True to enable\")\n",
    "    if not continue_flrnet_training:\n",
    "        print(f\"   Current perceptual loss setting: {'‚úÖ ENABLED' if use_perceptual_loss_continuation else '‚ùå DISABLED'}\")\n",
    "        print(\"   (can be changed via use_perceptual_loss_continuation variable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced FLRNet Test Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if trainer.flr_model is not None or 'test_flr_model' in locals():\n",
    "    print(\"üé® === FLRNet Test Visualization ===\")\n",
    "    \n",
    "    # Use the test model if available, otherwise use trainer model\n",
    "    flr_model_to_use = test_flr_model if 'test_flr_model' in locals() else trainer.flr_model\n",
    "    print(f\"Model type: {'Fourier-aware' if config['use_fourier'] else 'Standard'} FLRNet\")\n",
    "    \n",
    "    # Get test data batch directly from the properly formatted dataset\n",
    "    test_batch = next(iter(flrnet_test_dataset.take(1)))\n",
    "    print(\"Using FLRNet test dataset\")\n",
    "    \n",
    "    # Handle different input formats based on Fourier configuration\n",
    "    if config['use_fourier']:\n",
    "        # Fourier FLRNet expects (sensor_data, field_data, coordinates)\n",
    "        sensor_data, field_data, coord_data = test_batch\n",
    "        print(f\"‚úÖ Fourier FLRNet inputs detected:\")\n",
    "        print(f\"   Sensor data shape: {sensor_data.shape}\")\n",
    "        print(f\"   Field data shape: {field_data.shape}\")\n",
    "        print(f\"   Coordinate data shape: {coord_data.shape}\")\n",
    "        \n",
    "        # Generate FLRNet predictions\n",
    "        print(\"\\nüîÆ Generating FLRNet reconstructions...\")\n",
    "        try:\n",
    "            # Limit to manageable batch size for visualization\n",
    "            max_samples = min(4, sensor_data.shape[0])\n",
    "            \n",
    "            limited_sensor = sensor_data[:max_samples]\n",
    "            limited_field = field_data[:max_samples]\n",
    "            limited_coord = coord_data[:max_samples]\n",
    "            \n",
    "            predictions = flr_model_to_use([limited_sensor, limited_field, limited_coord], training=False)\n",
    "            print(f\"‚úÖ Reconstruction successful! Shape: {predictions.shape}\")\n",
    "            \n",
    "            # Use limited_field as targets for comparison\n",
    "            targets = limited_field\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå FLRNet prediction failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    else:\n",
    "        # Standard FLRNet expects (sensor_data, field_data)\n",
    "        sensor_data, field_data = test_batch\n",
    "        print(f\"‚úÖ Standard FLRNet inputs:\")\n",
    "        print(f\"   Sensor data shape: {sensor_data.shape}\")\n",
    "        print(f\"   Field data shape: {field_data.shape}\")\n",
    "        \n",
    "        # Generate FLRNet predictions\n",
    "        print(\"\\nüîÆ Generating FLRNet reconstructions...\")\n",
    "        try:\n",
    "            # Limit to manageable batch size for visualization\n",
    "            max_samples = min(4, sensor_data.shape[0])\n",
    "            \n",
    "            limited_sensor = sensor_data[:max_samples]\n",
    "            limited_field = field_data[:max_samples]\n",
    "            \n",
    "            predictions = flr_model_to_use([limited_sensor, limited_field], training=False)\n",
    "            print(f\"‚úÖ Reconstruction successful! Shape: {predictions.shape}\")\n",
    "            \n",
    "            # Use limited_field as targets for comparison\n",
    "            targets = limited_field\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå FLRNet prediction failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Calculate reconstruction metrics\n",
    "    mse = np.mean((targets.numpy() - predictions) ** 2)\n",
    "    mae = np.mean(np.abs(targets.numpy() - predictions))\n",
    "    max_error = np.max(np.abs(targets.numpy() - predictions))\n",
    "    \n",
    "    print(f\"\\nüìä FLRNet Reconstruction Metrics:\")\n",
    "    print(f\"   MSE: {mse:.6f} ({'Excellent' if mse < 0.01 else 'Good' if mse < 0.05 else 'Needs improvement'})\")\n",
    "    print(f\"   MAE: {mae:.6f}\")\n",
    "    print(f\"   Max Error: {max_error:.6f}\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    n_samples = max_samples\n",
    "    fig, axes = plt.subplots(4, n_samples, figsize=(5*n_samples, 16))\n",
    "    \n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Sensor readings visualization (create a sensor field overlay)\n",
    "        ax1 = axes[0, i]\n",
    "        # Create a field to visualize sensor positions and values\n",
    "        sensor_field = np.zeros((config['input_shape'][0], config['input_shape'][1]))\n",
    "        \n",
    "        # If we have sensor positions from dataset, use them\n",
    "        if 'sensor_positions' in locals():\n",
    "            for j, (x_pos, y_pos) in enumerate(sensor_positions):\n",
    "                if j < len(limited_sensor[i]):\n",
    "                    # Convert normalized positions to pixel coordinates\n",
    "                    x_idx = int(x_pos * (config['input_shape'][1] - 1))\n",
    "                    y_idx = int(y_pos * (config['input_shape'][0] - 1))\n",
    "                    if 0 <= x_idx < config['input_shape'][1] and 0 <= y_idx < config['input_shape'][0]:\n",
    "                        sensor_field[y_idx, x_idx] = limited_sensor[i, j]\n",
    "        \n",
    "        im1 = ax1.imshow(sensor_field, cmap='RdBu_r', origin='lower')\n",
    "        ax1.set_title(f'Sensor Readings {i+1}', fontweight='bold')\n",
    "        ax1.set_xlabel('X Position')\n",
    "        ax1.set_ylabel('Y Position')\n",
    "        plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
    "        \n",
    "        # Add sensor positions as scatter points if available\n",
    "        if 'sensor_positions' in locals():\n",
    "            sensor_x = sensor_positions[:, 0] * (config['input_shape'][1] - 1)\n",
    "            sensor_y = sensor_positions[:, 1] * (config['input_shape'][0] - 1)\n",
    "            ax1.scatter(sensor_x, sensor_y, c='black', s=50, marker='o', edgecolors='white', linewidth=1)\n",
    "        \n",
    "        # Ground truth field\n",
    "        im2 = axes[1, i].imshow(targets[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "        axes[1, i].set_title(f'Ground Truth Field {i+1}', fontweight='bold')\n",
    "        axes[1, i].set_xlabel('X Position')\n",
    "        axes[1, i].set_ylabel('Y Position')\n",
    "        plt.colorbar(im2, ax=axes[1, i], shrink=0.8)\n",
    "        \n",
    "        # FLRNet prediction\n",
    "        im3 = axes[2, i].imshow(predictions[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "        axes[2, i].set_title(f'FLRNet Prediction {i+1}', fontweight='bold')\n",
    "        axes[2, i].set_xlabel('X Position')\n",
    "        axes[2, i].set_ylabel('Y Position')\n",
    "        plt.colorbar(im3, ax=axes[2, i], shrink=0.8)\n",
    "        \n",
    "        # Error map\n",
    "        error = np.abs(targets[i, :, :, 0] - predictions[i, :, :, 0])\n",
    "        im4 = axes[3, i].imshow(error, cmap='hot', origin='lower')\n",
    "        axes[3, i].set_title(f'Reconstruction Error {i+1}', fontweight='bold')\n",
    "        axes[3, i].set_xlabel('X Position')\n",
    "        axes[3, i].set_ylabel('Y Position')\n",
    "        plt.colorbar(im4, ax=axes[3, i], shrink=0.8)\n",
    "        \n",
    "        # Add error statistics as text\n",
    "        sample_mse = np.mean(error**2)\n",
    "        sample_max = np.max(error)\n",
    "        axes[3, i].text(0.02, 0.98, f'MSE: {sample_mse:.4f}\\nMax: {sample_max:.4f}', \n",
    "                       transform=axes[3, i].transAxes, verticalalignment='top',\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                       fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'FLRNet Reconstruction Results - {config_name}', y=1.02, fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(f\"\\nüìà Statistical Comparison:\")\n",
    "    orig_stats = {\n",
    "        'mean': np.mean(targets.numpy()),\n",
    "        'std': np.std(targets.numpy()),\n",
    "        'min': np.min(targets.numpy()),\n",
    "        'max': np.max(targets.numpy())\n",
    "    }\n",
    "    \n",
    "    pred_stats = {\n",
    "        'mean': np.mean(predictions.numpy()),\n",
    "        'std': np.std(predictions.numpy()),\n",
    "        'min': np.min(predictions.numpy()),\n",
    "        'max': np.max(predictions.numpy())\n",
    "    }\n",
    "    \n",
    "    print(f\"   Original  - Mean: {orig_stats['mean']:.4f}, Std: {orig_stats['std']:.4f}, Range: [{orig_stats['min']:.4f}, {orig_stats['max']:.4f}]\")\n",
    "    print(f\"   Predicted - Mean: {pred_stats['mean']:.4f}, Std: {pred_stats['std']:.4f}, Range: [{pred_stats['min']:.4f}, {pred_stats['max']:.4f}]\")\n",
    "    \n",
    "    # Distribution comparison\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Value distributions\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(targets.numpy().flatten(), bins=50, alpha=0.7, label='Ground Truth', density=True, color='blue')\n",
    "    plt.hist(predictions.numpy().flatten(), bins=50, alpha=0.7, label='FLRNet Prediction', density=True, color='red')\n",
    "    plt.xlabel('Field Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Value Distribution Comparison', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error distribution\n",
    "    plt.subplot(1, 3, 2)\n",
    "    errors = np.abs(targets.numpy() - predictions).flatten()\n",
    "    plt.hist(errors, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "    plt.xlabel('Absolute Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Reconstruction Error Distribution', fontweight='bold')\n",
    "    plt.axvline(np.mean(errors), color='darkred', linestyle='--', linewidth=2, label=f'Mean: {np.mean(errors):.4f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Relative error distribution\n",
    "    plt.subplot(1, 3, 3)\n",
    "    relative_errors = errors / (np.abs(targets.numpy().flatten()) + 1e-8)\n",
    "    plt.hist(relative_errors, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "    plt.xlabel('Relative Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Relative Error Distribution', fontweight='bold')\n",
    "    plt.axvline(np.mean(relative_errors), color='darkorange', linestyle='--', linewidth=2, label=f'Mean: {np.mean(relative_errors):.4f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('FLRNet Performance Analysis', y=1.02, fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    # Sensor analysis\n",
    "    print(f\"\\nüì° Sensor Analysis:\")\n",
    "    print(f\"   Number of sensors: {limited_sensor.shape[1]}\")\n",
    "    print(f\"   Sensor value range: [{limited_sensor.numpy().min():.4f}, {limited_sensor.numpy().max():.4f}]\")\n",
    "    print(f\"   Sensor value std: {limited_sensor.numpy().std():.4f}\")\n",
    "    \n",
    "    # Show sensor readings distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.boxplot([limited_sensor[i, :].numpy() for i in range(n_samples)], \n",
    "                labels=[f'Sample {i+1}' for i in range(n_samples)])\n",
    "    plt.ylabel('Sensor Reading Value')\n",
    "    plt.title('Sensor Readings Distribution per Sample', fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(n_samples):\n",
    "        plt.plot(limited_sensor[i, :].numpy(), 'o-', label=f'Sample {i+1}', alpha=0.7)\n",
    "    plt.xlabel('Sensor Index')\n",
    "    plt.ylabel('Sensor Reading Value')\n",
    "    plt.title('Sensor Readings by Position', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sensor Input Analysis', y=1.02, fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    # Reconstruction quality analysis per sample\n",
    "    print(f\"\\nüîó Reconstruction Quality Analysis:\")\n",
    "    sample_mses = []\n",
    "    sensor_means = []\n",
    "    sensor_stds = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        sample_error = np.abs(targets[i, :, :, 0] - predictions[i, :, :, 0])\n",
    "        sample_mse = np.mean(sample_error**2)\n",
    "        sample_mses.append(sample_mse)\n",
    "        sensor_means.append(np.mean(limited_sensor[i, :]))\n",
    "        sensor_stds.append(np.std(limited_sensor[i, :]))\n",
    "    \n",
    "    print(f\"   Sample MSEs: {[f'{mse:.6f}' for mse in sample_mses]}\")\n",
    "    print(f\"   Best reconstruction: Sample {np.argmin(sample_mses) + 1} (MSE: {min(sample_mses):.6f})\")\n",
    "    print(f\"   Worst reconstruction: Sample {np.argmax(sample_mses) + 1} (MSE: {max(sample_mses):.6f})\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    correlation_field = np.corrcoef(targets.numpy().flatten(), predictions.numpy().flatten())[0, 1]\n",
    "    print(f\"   Field correlation coefficient: {correlation_field:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéâ FLRNet visualization completed successfully!\")\n",
    "    print(f\"   üìä Reconstruction Quality: {'üèÜ Excellent' if mse < 0.01 else '‚úÖ Good' if mse < 0.05 else '‚ö†Ô∏è Needs improvement'}\")\n",
    "    print(f\"   üåä Fourier Features: {'‚úÖ Working correctly' if config['use_fourier'] else '‚ûñ Not used'}\")\n",
    "    print(f\"   üì° Sensor Count: {limited_sensor.shape[1]} sensors\")\n",
    "    print(f\"   üîó Field Correlation: {correlation_field:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No FLRNet model available for visualization\")\n",
    "    print(\"Please run the FLRNet model creation cell above first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
