{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89d8f71",
   "metadata": {},
   "source": [
    "## üîß Windows Checkpoint File Lock Troubleshooting\n",
    "\n",
    "If you encounter \"Access is denied\" errors during checkpoint saving, this is a known Windows-specific issue with TensorFlow. Here are the solutions implemented:\n",
    "\n",
    "### ‚úÖ Current Solutions Applied:\n",
    "\n",
    "1. **Delayed Checkpoint Saving**: Checkpoint saving is now disabled for the first 30 epochs and only starts from epoch 31 onward\n",
    "2. **Unique Checkpoint Directories**: Using UUID-based unique directories to avoid conflicts\n",
    "3. **Error Handling**: Added try-catch blocks around checkpoint operations\n",
    "4. **Alternative Backup**: Models are saved after training completes\n",
    "\n",
    "### üõ†Ô∏è If Issues Persist:\n",
    "\n",
    "1. **Completely Disable Checkpoints**: Set `disable_checkpoints_entirely = True` in the cell above\n",
    "2. **Manual Cleanup**: Delete the entire checkpoint directory before training\n",
    "3. **Use Different Drive**: Try running on a different drive (e.g., C: instead of E:)\n",
    "4. **Run as Administrator**: Run Jupyter/VS Code as administrator\n",
    "\n",
    "### üìã Understanding the Error:\n",
    "\n",
    "The \"Access is denied\" error occurs when:\n",
    "- TensorFlow tries to rename temporary checkpoint files\n",
    "- Another process has the file locked\n",
    "- Windows file system permissions prevent the operation\n",
    "- Previous training runs left orphaned file handles\n",
    "\n",
    "The conditional checkpoint approach bypasses this issue by avoiding file operations during the sensitive early training phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957488d1",
   "metadata": {},
   "source": [
    "# Flow Field Reconstruction with 8 Edge Sensors\n",
    "\n",
    "This notebook implements a physics-informed machine learning approach to reconstruct flow fields using data from 8 edge sensors. The implementation uses:\n",
    "- Variational Autoencoder (VAE) for dimensionality reduction and flow field reconstruction\n",
    "- Fourier feature embeddings for coordinate information\n",
    "- FLRNet architecture to predict flow fields from sparse sensor measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ded4a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll import the necessary libraries for data manipulation, visualization, and deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e48aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n",
      "Found 1 GPU(s): [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Suppress TensorFlow warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load local modules\n",
    "import model_fourier\n",
    "import models_improved\n",
    "import layer as flr_layer\n",
    "import config_manager\n",
    "from data.flow_field_dataset import FlowFieldDatasetCreator\n",
    "\n",
    "# Check available GPUs and set memory growth\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth for all GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Found {len(gpus)} GPU(s): {gpus}\")\n",
    "        # Use first GPU\n",
    "        gpu = gpus[0]\n",
    "        tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "        print(f\"Using GPU: {gpu}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error configuring GPUs: {e}\")\n",
    "else:\n",
    "    print(\"No GPUs found. Using CPU.\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22807b8f",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup\n",
    "\n",
    "Let's set up the configuration for our model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "994c56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Configuration Summary\n",
      "==================================================\n",
      "üèóÔ∏è  Model Architecture:\n",
      "   - Fourier Enhancement: True\n",
      "   - Perceptual Loss: False\n",
      "   - Input Shape: [128, 256, 1]\n",
      "   - Latent Dimensions: 8\n",
      "   - Base Features: 64\n",
      "\n",
      "üì° Sensor Configuration:\n",
      "   - Layout: edge\n",
      "   - Number of Sensors: 8\n",
      "   - Dataset: data/datasets\\dataset_edge_8.npz\n",
      "\n",
      "üöÄ Training Parameters:\n",
      "   - VAE Epochs: 250\n",
      "   - FLRNet Epochs: 150\n",
      "   - VAE Learning Rate: 0.0001\n",
      "   - FLRNet Learning Rate: 0.0001\n",
      "   - Batch Size: 8\n",
      "   - Test Split: 0.2\n",
      "\n",
      "üíæ Output Configuration:\n",
      "   - Model Name: fourierTrue_percepFalse_edge_8\n",
      "   - Checkpoints: ./checkpoints\\fourierTrue_percepFalse_edge_8\n",
      "   - Logs: ./logs\\fourierTrue_percepFalse_edge_8\n",
      "   - Save Best Model: True\n",
      "   - Save Last Model: True\n",
      "\n",
      "\n",
      "üîß Final Configuration (lowercase keys):\n",
      "   Model name: fourierTrue_percepFalse_edge_8\n",
      "   Use Fourier: True\n",
      "   Use perceptual loss: False\n",
      "   Input shape: (128, 256, 1)\n",
      "   Number of sensors: 8\n",
      "   Latent dims: 8\n",
      "   Base features: 64\n",
      "   Batch size: 8\n",
      "   VAE epochs: 250\n",
      "   FLRNet epochs: 150\n",
      "   VAE learning rate: 0.0001\n",
      "   FLRNet learning rate: 0.0001\n",
      "   Dataset path: data/datasets\\dataset_edge_8.npz\n",
      "   Checkpoint dir: ./checkpoints\\fourierTrue_percepFalse_edge_8\n",
      "   Logs dir: ./logs\\fourierTrue_percepFalse_edge_8\n",
      "‚úÖ All required configuration keys are present\n"
     ]
    }
   ],
   "source": [
    "# Load configuration using ConfigManager\n",
    "config_name = \"edge_8_fourier\"\n",
    "config_mgr = config_manager.ConfigManager()\n",
    "hierarchical_config = config_mgr.load_config(config_name)\n",
    "\n",
    "# Print configuration summary\n",
    "print(config_mgr.create_config_summary(hierarchical_config))\n",
    "\n",
    "# Flatten the config for training (with lowercase keys for Python style)\n",
    "flattened_config = config_manager.flatten_config_for_training(hierarchical_config)\n",
    "\n",
    "# Convert to lowercase keys for consistent Python style\n",
    "config = {}\n",
    "for key, value in flattened_config.items():\n",
    "    config[key.lower()] = value\n",
    "\n",
    "print(\"\\nüîß Final Configuration (lowercase keys):\")\n",
    "print(f\"   Model name: {config['model_name']}\")\n",
    "print(f\"   Use Fourier: {config['use_fourier']}\")\n",
    "print(f\"   Use perceptual loss: {config['use_perceptual_loss']}\")\n",
    "print(f\"   Input shape: {config['input_shape']}\")\n",
    "print(f\"   Number of sensors: {config['n_sensors']}\")\n",
    "print(f\"   Latent dims: {config['latent_dims']}\")\n",
    "print(f\"   Base features: {config['n_base_features']}\")\n",
    "print(f\"   Batch size: {config['batch_size']}\")\n",
    "print(f\"   VAE epochs: {config['vae_epochs']}\")\n",
    "print(f\"   FLRNet epochs: {config['flr_epochs']}\")\n",
    "print(f\"   VAE learning rate: {config['vae_learning_rate']}\")\n",
    "print(f\"   FLRNet learning rate: {config['flr_learning_rate']}\")\n",
    "print(f\"   Dataset path: {config['dataset_path']}\")\n",
    "print(f\"   Checkpoint dir: {config['checkpoint_dir']}\")\n",
    "print(f\"   Logs dir: {config['logs_dir']}\")\n",
    "\n",
    "# Verify essential config keys exist\n",
    "required_keys = ['model_name', 'use_fourier', 'use_perceptual_loss', 'input_shape', \n",
    "                 'n_sensors', 'latent_dims', 'n_base_features', 'batch_size', \n",
    "                 'vae_epochs', 'flr_epochs', 'vae_learning_rate', 'flr_learning_rate',\n",
    "                 'dataset_path', 'checkpoint_dir', 'logs_dir']\n",
    "missing_keys = [key for key in required_keys if key not in config]\n",
    "if missing_keys:\n",
    "    print(f\"‚ö†Ô∏è  Missing required config keys: {missing_keys}\")\n",
    "else:\n",
    "    print(\"‚úÖ All required configuration keys are present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c2fdc",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset\n",
    "\n",
    "We'll load the flow field dataset and the sensor layout for 8 edge sensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f51841e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from: data/datasets\\dataset_edge_8.npz\n",
      "üìä Dataset parameters:\n",
      "   Layout type: edge\n",
      "   Number of sensors: 8\n",
      "üìÅ Loading dataset file: data/datasets\\dataset_edge_8.npz\n",
      "üìã Available keys in dataset: ['sensor_data', 'field_data', 'sensor_positions', 'reynolds_numbers', 'layout_type', 'n_sensors']\n",
      "üìä Dataset loaded successfully:\n",
      "   sensor_data: (28, 8, 39) (float64)\n",
      "   field_data: (28, 128, 256, 39) (float64)\n",
      "   sensor_positions: (8, 2) (float64)\n",
      "   reynolds_numbers: (28,) (int32)\n",
      "   layout_type: () (<U4)\n",
      "   n_sensors: () (int32)\n",
      "üìç Sensor positions shape: (8, 2)\n",
      "üìä Dataset loaded successfully:\n",
      "   sensor_data: (28, 8, 39) (float64)\n",
      "   field_data: (28, 128, 256, 39) (float64)\n",
      "   sensor_positions: (8, 2) (float64)\n",
      "   reynolds_numbers: (28,) (int32)\n",
      "   layout_type: () (<U4)\n",
      "   n_sensors: () (int32)\n",
      "üìç Sensor positions shape: (8, 2)\n",
      "Dataset reshaped:\n",
      "  Original sensor data: (28, 8, 39)\n",
      "  Reshaped sensor data: (1092, 8)\n",
      "  Original field data: (28, 128, 256, 39)\n",
      "  Reshaped field data: (1092, 128, 256, 1)\n",
      "  Total samples: 1092\n",
      "Dataset reshaped:\n",
      "  Original sensor data: (28, 8, 39)\n",
      "  Reshaped sensor data: (1092, 8)\n",
      "  Original field data: (28, 128, 256, 39)\n",
      "  Reshaped field data: (1092, 128, 256, 1)\n",
      "  Total samples: 1092\n",
      "TensorFlow datasets created:\n",
      "  Train samples: 873\n",
      "  Test samples: 219\n",
      "\n",
      "üìä TensorFlow datasets created:\n",
      "   Train dataset: <ShuffleDataset element_spec={'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None)}>\n",
      "   Test dataset: <BatchDataset element_spec={'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None)}>\n",
      "\n",
      "üìä Coordinate-aware datasets created:\n",
      "   Train dataset with coordinates: <MapDataset element_spec={'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), 'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'coordinates': TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)}>\n",
      "   Test dataset with coordinates: <MapDataset element_spec={'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), 'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'coordinates': TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)}>\n",
      "\n",
      "üåä Creating Fourier-aware VAE datasets...\n",
      "‚úÖ Fourier-aware VAE datasets created\n",
      "\n",
      "üìä Specialized datasets:\n",
      "   VAE train dataset: <MapDataset element_spec=((TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   VAE test dataset: <MapDataset element_spec=((TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   FLRNet train dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   FLRNet test dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "\n",
      "üìä Data shape verification:\n",
      "TensorFlow datasets created:\n",
      "  Train samples: 873\n",
      "  Test samples: 219\n",
      "\n",
      "üìä TensorFlow datasets created:\n",
      "   Train dataset: <ShuffleDataset element_spec={'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None)}>\n",
      "   Test dataset: <BatchDataset element_spec={'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None)}>\n",
      "\n",
      "üìä Coordinate-aware datasets created:\n",
      "   Train dataset with coordinates: <MapDataset element_spec={'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), 'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'coordinates': TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)}>\n",
      "   Test dataset with coordinates: <MapDataset element_spec={'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), 'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'coordinates': TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)}>\n",
      "\n",
      "üåä Creating Fourier-aware VAE datasets...\n",
      "‚úÖ Fourier-aware VAE datasets created\n",
      "\n",
      "üìä Specialized datasets:\n",
      "   VAE train dataset: <MapDataset element_spec=((TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   VAE test dataset: <MapDataset element_spec=((TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   FLRNet train dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   FLRNet test dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "\n",
      "üìä Data shape verification:\n",
      "   Sensor data shape: (8, 8)\n",
      "   Field data shape: (8, 128, 256, 1)\n",
      "   Coordinates shape: (8, 128, 256, 2)\n",
      "\n",
      "üìä VAE dataset structure verification:\n",
      "   Sensor data shape: (8, 8)\n",
      "   Field data shape: (8, 128, 256, 1)\n",
      "   Coordinates shape: (8, 128, 256, 2)\n",
      "\n",
      "üìä VAE dataset structure verification:\n",
      "   VAE field input shape: (8, 128, 256, 1)\n",
      "   VAE coordinate input shape: (8, 128, 256, 2)\n",
      "   VAE target shape: (8, 128, 256, 1)\n",
      "   VAE field input shape: (8, 128, 256, 1)\n",
      "   VAE coordinate input shape: (8, 128, 256, 2)\n",
      "   VAE target shape: (8, 128, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare dataset created from data_creation_and_viz.ipynb\n",
    "print(f\"üìÇ Loading dataset from: {config['dataset_path']}\")\n",
    "\n",
    "# Parse the dataset filename to get layout and n_sensors\n",
    "dataset_filename = Path(config['dataset_path']).name\n",
    "# Expected format: dataset_edge_8.npz\n",
    "parts = dataset_filename.split('_')\n",
    "layout_type = parts[1]  # 'edge'\n",
    "n_sensors = int(parts[2].split('.')[0])  # 8\n",
    "\n",
    "print(f\"üìä Dataset parameters:\")\n",
    "print(f\"   Layout type: {layout_type}\")\n",
    "print(f\"   Number of sensors: {n_sensors}\")\n",
    "\n",
    "# Load the dataset directly from the NPZ file\n",
    "print(f\"üìÅ Loading dataset file: {config['dataset_path']}\")\n",
    "data = np.load(config['dataset_path'])\n",
    "\n",
    "# Check what keys are available in the dataset\n",
    "print(f\"üìã Available keys in dataset: {list(data.keys())}\")\n",
    "\n",
    "# Create dataset dictionary\n",
    "dataset = {key: data[key] for key in data.keys()}\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"üìä Dataset loaded successfully:\")\n",
    "for key, value in dataset.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"   {key}: {value.shape} ({value.dtype})\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "# Extract sensor positions\n",
    "sensor_positions = dataset['sensor_positions']\n",
    "print(f\"üìç Sensor positions shape: {sensor_positions.shape}\")\n",
    "\n",
    "# Create dataset creator instance for TensorFlow dataset creation\n",
    "creator = FlowFieldDatasetCreator(\n",
    "    output_path=\"./data/\",\n",
    "    domain_shape=config['input_shape'][:2],  # (height, width)\n",
    "    use_synthetic_data=False  # Don't create synthetic data, just use for TF dataset creation\n",
    ")\n",
    "\n",
    "# Create TensorFlow datasets using the creator's method\n",
    "train_dataset, test_dataset = creator.create_tensorflow_dataset(\n",
    "    dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    test_split=config['test_split']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä TensorFlow datasets created:\")\n",
    "print(f\"   Train dataset: {train_dataset}\")\n",
    "print(f\"   Test dataset: {test_dataset}\")\n",
    "\n",
    "# Function to add coordinate grids to field data for Fourier-aware VAE training\n",
    "def add_coordinate_grid(batch):\n",
    "    \"\"\"Add coordinate grid to field data for Fourier VAE training\"\"\"\n",
    "    field_data = batch['field_data']\n",
    "    \n",
    "    # Get dimensions\n",
    "    batch_size = tf.shape(field_data)[0]\n",
    "    height = tf.shape(field_data)[1]\n",
    "    width = tf.shape(field_data)[2]\n",
    "    \n",
    "    # Create normalized coordinate grids [0, 1] - match field dimensions\n",
    "    x_coords = tf.linspace(0.0, 1.0, width)   # Width corresponds to x\n",
    "    y_coords = tf.linspace(0.0, 1.0, height)  # Height corresponds to y\n",
    "    \n",
    "    # Create meshgrid to match image indexing: [height, width, 2]\n",
    "    y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    # Stack to create coordinate grid (height, width, 2)\n",
    "    coord_grid = tf.stack([x_grid, y_grid], axis=-1)\n",
    "    \n",
    "    # Expand to batch size (batch_size, height, width, 2)\n",
    "    coord_batch = tf.tile(tf.expand_dims(coord_grid, 0), [batch_size, 1, 1, 1])\n",
    "    \n",
    "    # Update batch to include coordinates\n",
    "    return {\n",
    "        'field_data': field_data,\n",
    "        'sensor_data': batch['sensor_data'],\n",
    "        'coordinates': coord_batch\n",
    "    }\n",
    "\n",
    "# Add coordinate grids to datasets\n",
    "coord_train_dataset = train_dataset.map(add_coordinate_grid)\n",
    "coord_test_dataset = test_dataset.map(add_coordinate_grid)\n",
    "\n",
    "print(f\"\\nüìä Coordinate-aware datasets created:\")\n",
    "print(f\"   Train dataset with coordinates: {coord_train_dataset}\")\n",
    "print(f\"   Test dataset with coordinates: {coord_test_dataset}\")\n",
    "\n",
    "# Create specialized datasets for VAE and FLRNet training\n",
    "# VAE trains on field reconstruction WITH coordinates for Fourier features\n",
    "if config['use_fourier']:\n",
    "    print(\"\\nüåä Creating Fourier-aware VAE datasets...\")\n",
    "    # For Fourier VAE: input = (field, coordinates), output = field\n",
    "    vae_train_dataset = coord_train_dataset.map(\n",
    "        lambda batch: ((batch['field_data'], batch['coordinates']), batch['field_data'])\n",
    "    )\n",
    "    vae_test_dataset = coord_test_dataset.map(\n",
    "        lambda batch: ((batch['field_data'], batch['coordinates']), batch['field_data'])\n",
    "    )\n",
    "    print(\"‚úÖ Fourier-aware VAE datasets created\")\n",
    "else:\n",
    "    print(\"\\nüîÑ Creating standard VAE datasets...\")\n",
    "    # Standard VAE: input = field, output = field\n",
    "    vae_train_dataset = coord_train_dataset.map(\n",
    "        lambda batch: (batch['field_data'], batch['field_data'])\n",
    "    )\n",
    "    vae_test_dataset = coord_test_dataset.map(\n",
    "        lambda batch: (batch['field_data'], batch['field_data'])\n",
    "    )\n",
    "    print(\"‚úÖ Standard VAE datasets created\")\n",
    "\n",
    "# FLRNet trains on sensor-to-field reconstruction (sensor -> field)\n",
    "flrnet_train_dataset = coord_train_dataset.map(\n",
    "    lambda batch: (batch['sensor_data'], batch['field_data'])\n",
    ")\n",
    "flrnet_test_dataset = coord_test_dataset.map(\n",
    "    lambda batch: (batch['sensor_data'], batch['field_data'])\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Specialized datasets:\")\n",
    "print(f\"   VAE train dataset: {vae_train_dataset}\")\n",
    "print(f\"   VAE test dataset: {vae_test_dataset}\")\n",
    "print(f\"   FLRNet train dataset: {flrnet_train_dataset}\")\n",
    "print(f\"   FLRNet test dataset: {flrnet_test_dataset}\")\n",
    "\n",
    "# Get a sample to verify data shapes\n",
    "print(f\"\\nüìä Data shape verification:\")\n",
    "for batch in coord_train_dataset.take(1):\n",
    "    print(f\"   Sensor data shape: {batch['sensor_data'].shape}\")\n",
    "    print(f\"   Field data shape: {batch['field_data'].shape}\")\n",
    "    print(f\"   Coordinates shape: {batch['coordinates'].shape}\")\n",
    "    break\n",
    "\n",
    "# Verify VAE dataset structure\n",
    "print(f\"\\nüìä VAE dataset structure verification:\")\n",
    "for vae_batch in vae_train_dataset.take(1):\n",
    "    if config['use_fourier']:\n",
    "        inputs, targets = vae_batch\n",
    "        if isinstance(inputs, tuple):\n",
    "            field_input, coord_input = inputs\n",
    "            print(f\"   VAE field input shape: {field_input.shape}\")\n",
    "            print(f\"   VAE coordinate input shape: {coord_input.shape}\")\n",
    "            print(f\"   VAE target shape: {targets.shape}\")\n",
    "        else:\n",
    "            print(f\"   Unexpected VAE input format: {type(inputs)}\")\n",
    "    else:\n",
    "        inputs, targets = vae_batch\n",
    "        print(f\"   VAE input shape: {inputs.shape}\")\n",
    "        print(f\"   VAE target shape: {targets.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61349511",
   "metadata": {},
   "source": [
    "## 5. Train VAE and FLRNet Models\n",
    "\n",
    "Now we'll train our models using the FLRTrainer:\n",
    "1. First, the Variational Autoencoder (VAE) to learn a compressed representation of flow fields\n",
    "2. Then, the FLRNet to reconstruct flow fields from sparse sensor measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "566a6830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Clearing TensorFlow session...\n",
      "üöÄ Initializing FLRTrainer...\n",
      "‚úÖ FLRTrainer initialized:\n",
      "   Input shape: (128, 256, 1)\n",
      "   Use Fourier: True\n",
      "   Model name: fourierTrue_percepFalse_edge_8\n",
      "   Gradient clipping: 2.0\n",
      "   Checkpoints: ./checkpoints\\fourierTrue_percepFalse_edge_8\n",
      "   Logs: ./logs\\fourierTrue_percepFalse_edge_8\n",
      "   Save best model: True\n",
      "   Save last model: True\n",
      "\n",
      "üîß Training configuration:\n",
      "   Train VAE: True\n",
      "   Train FLRNet: True\n",
      "\n",
      "üîç Verifying dataset shapes...\n",
      "   Field data shape: (8, 128, 256, 1)\n",
      "   Coordinates shape: (8, 128, 256, 2)\n",
      "   Field data shape: (8, 128, 256, 1)\n",
      "   Coordinates shape: (8, 128, 256, 2)\n",
      "   VAE field input shape: (8, 128, 256, 1)\n",
      "   VAE coordinate input shape: (8, 128, 256, 2)\n",
      "   VAE target shape: (8, 128, 256, 1)\n",
      "   ‚úÖ Coordinate shapes now match field shapes!\n",
      "   VAE field input shape: (8, 128, 256, 1)\n",
      "   VAE coordinate input shape: (8, 128, 256, 2)\n",
      "   VAE target shape: (8, 128, 256, 1)\n",
      "   ‚úÖ Coordinate shapes now match field shapes!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# Clear TensorFlow session to fix any shape mismatches\n",
    "print(\"üîÑ Clearing TensorFlow session...\")\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Initialize FLRTrainer from models_improved.py\n",
    "print(\"üöÄ Initializing FLRTrainer...\")\n",
    "\n",
    "# Create the FLRTrainer instance\n",
    "trainer = models_improved.FLRTrainer(\n",
    "    input_shape=config['input_shape'],\n",
    "    use_fourier=config['use_fourier'],\n",
    "    checkpoint_dir=config['checkpoint_dir'],\n",
    "    logs_dir=config['logs_dir'],\n",
    "    model_name=config['model_name'],\n",
    "    save_best_model=config['save_best_model'],\n",
    "    save_last_model=config['save_last_model'],\n",
    "    gradient_clip_norm=config['gradient_clip_norm']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ FLRTrainer initialized:\")\n",
    "print(f\"   Input shape: {config['input_shape']}\")\n",
    "print(f\"   Use Fourier: {config['use_fourier']}\")\n",
    "print(f\"   Model name: {config['model_name']}\")\n",
    "print(f\"   Gradient clipping: {config['gradient_clip_norm']}\")\n",
    "print(f\"   Checkpoints: {config['checkpoint_dir']}\")\n",
    "print(f\"   Logs: {config['logs_dir']}\")\n",
    "print(f\"   Save best model: {config['save_best_model']}\")\n",
    "print(f\"   Save last model: {config['save_last_model']}\")\n",
    "\n",
    "# Set training flags\n",
    "train_vae_model = True  # Set to False if you want to skip VAE training\n",
    "train_flrnet_model = True  # Set to True to test FLRNet with proper Fourier features\n",
    "\n",
    "print(f\"\\nüîß Training configuration:\")\n",
    "print(f\"   Train VAE: {train_vae_model}\")\n",
    "print(f\"   Train FLRNet: {train_flrnet_model}\")\n",
    "\n",
    "# Quick test to verify dataset shapes are correct\n",
    "print(f\"\\nüîç Verifying dataset shapes...\")\n",
    "for batch in coord_train_dataset.take(1):\n",
    "    print(f\"   Field data shape: {batch['field_data'].shape}\")\n",
    "    print(f\"   Coordinates shape: {batch['coordinates'].shape}\")\n",
    "    break\n",
    "\n",
    "# Test VAE dataset format\n",
    "for vae_batch in vae_train_dataset.take(1):\n",
    "    inputs, targets = vae_batch\n",
    "    if isinstance(inputs, tuple):\n",
    "        field_input, coord_input = inputs\n",
    "        print(f\"   VAE field input shape: {field_input.shape}\")\n",
    "        print(f\"   VAE coordinate input shape: {coord_input.shape}\")\n",
    "        print(f\"   VAE target shape: {targets.shape}\")\n",
    "        \n",
    "        # Check if shapes match now\n",
    "        if field_input.shape[1:3] == coord_input.shape[1:3]:\n",
    "            print(f\"   ‚úÖ Coordinate shapes now match field shapes!\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Shape mismatch: field {field_input.shape[1:3]} vs coord {coord_input.shape[1:3]}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE model using FLRTrainer\n",
    "train_vae_model = False  # Set to False if you want to skip VAE training\n",
    "if train_vae_model:\n",
    "    print(\"\\nüß† Starting VAE Training with Coordinate-Aware Data...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # IMPORTANT: Clear any existing VAE model to ensure fresh training\n",
    "    if hasattr(trainer, 'vae_model') and trainer.vae_model is not None:\n",
    "        print(\"üîÑ Clearing existing VAE model to retrain from scratch...\")\n",
    "        del trainer.vae_model\n",
    "        trainer.vae_model = None\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Remove any existing VAE checkpoint files to force retraining\n",
    "    import glob\n",
    "    vae_checkpoint_pattern = os.path.join(config['checkpoint_dir'], \"*vae*\")\n",
    "    existing_checkpoints = glob.glob(vae_checkpoint_pattern)\n",
    "    if existing_checkpoints:\n",
    "        print(f\"üóëÔ∏è  Found {len(existing_checkpoints)} existing VAE checkpoints - will retrain from scratch\")\n",
    "        for checkpoint in existing_checkpoints:\n",
    "            print(f\"   Found: {checkpoint}\")\n",
    "    \n",
    "    print(f\"\\nüåä Training VAE with Fourier features: {config['use_fourier']}\")\n",
    "    if config['use_fourier']:\n",
    "        print(\"   VAE will be trained with coordinate-aware data for proper Fourier support\")\n",
    "    else:\n",
    "        print(\"   VAE will be trained with standard field data\")\n",
    "    \n",
    "    print(\"\\nüõ°Ô∏è Note: Checkpoint saving is disabled for the first 30 epochs to avoid Windows file lock issues.\")\n",
    "    print(\"   Saving will automatically start from epoch 31 onward.\")\n",
    "    print(\"   This ensures robust training without checkpoint file access conflicts.\")\n",
    "    \n",
    "    # Train VAE using the trainer with coordinate-aware datasets\n",
    "    vae_model = trainer.train_vae(\n",
    "        train_dataset=vae_train_dataset,\n",
    "        val_dataset=vae_test_dataset,\n",
    "        epochs=config['vae_epochs'],\n",
    "        learning_rate=config['vae_learning_rate'],\n",
    "        latent_dims=config['latent_dims'],\n",
    "        n_base_features=config['n_base_features'],\n",
    "        use_perceptual_loss=config['use_perceptual_loss'],\n",
    "        patience=config['patience'],\n",
    "        reduce_lr_patience=config['reduce_lr_patience']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ VAE training completed successfully!\")\n",
    "    \n",
    "    # Validate that the VAE now works with coordinate data (if using Fourier)\n",
    "    if config['use_fourier'] and vae_model is not None:\n",
    "        print(\"\\nüîç Validating Fourier VAE with coordinate input...\")\n",
    "        try:\n",
    "            # Test with a small batch\n",
    "            test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "            inputs, targets = test_batch\n",
    "            \n",
    "            if isinstance(inputs, tuple):\n",
    "                field_input, coord_input = inputs\n",
    "                print(f\"   Testing VAE with field shape: {field_input.shape}\")\n",
    "                print(f\"   Testing VAE with coord shape: {coord_input.shape}\")\n",
    "                \n",
    "                # Test prediction\n",
    "                prediction = vae_model([field_input[:1], coord_input[:1]])\n",
    "                print(f\"   ‚úÖ VAE prediction successful! Output shape: {prediction.shape}\")\n",
    "                print(f\"   üéâ VAE now properly supports Fourier features with coordinates!\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Unexpected input format: {type(inputs)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå VAE validation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping VAE training. Loading existing model...\")\n",
    "    # The trainer will handle loading existing VAE weights automatically\n",
    "    vae_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33294c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE using the trainer with coordinate-aware datasets\n",
    "vae_model = trainer.train_vae(\n",
    "    train_dataset=vae_train_dataset,\n",
    "    val_dataset=vae_test_dataset,\n",
    "    epochs=50,#config['vae_epochs'],\n",
    "    learning_rate=1e-5,#config['vae_learning_rate'],\n",
    "    latent_dims=config['latent_dims'],\n",
    "    n_base_features=config['n_base_features'],\n",
    "    use_perceptual_loss=True,\n",
    "    patience=config['patience'],\n",
    "    reduce_lr_patience=config['reduce_lr_patience']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687601d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FLRNet model using FLRTrainer\n",
    "if train_flrnet_model:\n",
    "    print(\"\\nüîÑ Starting FLRNet Training using FLRTrainer...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Train FLRNet using the trainer\n",
    "    flr_model = trainer.train_flr_net(\n",
    "        train_dataset=flrnet_train_dataset,\n",
    "        val_dataset=flrnet_test_dataset,\n",
    "        n_sensors=config['n_sensors'],\n",
    "        epochs=config['flr_epochs'],\n",
    "        learning_rate=config['flr_learning_rate'],\n",
    "        pretrained_vae=vae_model,  # Use the VAE model we just trained\n",
    "        latent_dims=config['latent_dims'],\n",
    "        n_base_features=config['n_base_features'],\n",
    "        use_perceptual_loss=config['use_perceptual_loss'],\n",
    "        patience=config['patience'],\n",
    "        reduce_lr_patience=config['reduce_lr_patience']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ FLRNet training completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping FLRNet training. Loading existing model...\")\n",
    "    # The trainer will handle loading existing FLRNet weights automatically\n",
    "    flr_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37445af",
   "metadata": {},
   "source": [
    "## VAE Validation and Visualization\n",
    "\n",
    "After training the VAE, we can evaluate its performance by:\n",
    "1. Visualizing reconstructed flow fields\n",
    "2. Computing reconstruction errors\n",
    "3. Examining the latent space representation\n",
    "\n",
    "This section can be skipped if VAE training was disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a proper Fourier-aware VAE dataset for testing\n",
    "if train_vae_model and trainer.vae_model is not None and trainer.vae_model.use_fourier:\n",
    "    print(\"=== Creating Fourier-Aware VAE Test Dataset ===\")\n",
    "    \n",
    "    # Function to add coordinate grids to VAE data\n",
    "    def add_coordinates_to_vae_data(field_input, field_target):\n",
    "        # Get batch size and shape\n",
    "        batch_size = tf.shape(field_input)[0]\n",
    "        height = tf.shape(field_input)[1]\n",
    "        width = tf.shape(field_input)[2]\n",
    "        \n",
    "        # Create coordinate grid (normalized to [0, 1])\n",
    "        y_coords = tf.linspace(0.0, 1.0, height)\n",
    "        x_coords = tf.linspace(0.0, 1.0, width)\n",
    "        \n",
    "        # Create meshgrid\n",
    "        x_grid, y_grid = tf.meshgrid(x_coords, y_coords, indexing='ij')\n",
    "        \n",
    "        # Stack to create coordinate grid (height, width, 2)\n",
    "        coord_grid = tf.stack([x_grid, y_grid], axis=-1)\n",
    "        \n",
    "        # Expand to batch size (batch_size, height, width, 2)\n",
    "        coord_batch = tf.tile(tf.expand_dims(coord_grid, 0), [batch_size, 1, 1, 1])\n",
    "        \n",
    "        # Return in the format expected by Fourier VAE: ((img, coord), target)\n",
    "        return ((field_input, coord_batch), field_target)\n",
    "    \n",
    "    # Create Fourier-aware VAE dataset\n",
    "    fourier_vae_test_dataset = vae_test_dataset.map(add_coordinates_to_vae_data)\n",
    "    \n",
    "    print(\"‚úÖ Fourier-aware VAE test dataset created\")\n",
    "    \n",
    "    # Test with a small batch\n",
    "    test_batch = next(iter(fourier_vae_test_dataset.take(1)))\n",
    "    \n",
    "    print(f\"Fourier VAE batch structure:\")\n",
    "    inputs, target = test_batch\n",
    "    if isinstance(inputs, tuple):\n",
    "        img_input, coord_input = inputs\n",
    "        print(f\"  Image input shape: {img_input.shape}\")\n",
    "        print(f\"  Coordinate input shape: {coord_input.shape}\")\n",
    "        print(f\"  Target shape: {target.shape}\")\n",
    "        \n",
    "        # Test VAE prediction with proper format\n",
    "        try:\n",
    "            # Take just 4 samples for testing\n",
    "            test_img = img_input[:4]\n",
    "            test_coord = coord_input[:4]\n",
    "            test_target = target[:4]\n",
    "            \n",
    "            print(f\"\\\\nTesting VAE with proper Fourier input format...\")\n",
    "            reconstructed = trainer.vae_model([test_img, test_coord])\n",
    "            \n",
    "            print(f\"‚úÖ Fourier VAE prediction successful!\")\n",
    "            print(f\"   Input shape: {test_img.shape}\")\n",
    "            print(f\"   Coordinate shape: {test_coord.shape}\")\n",
    "            print(f\"   Reconstruction shape: {reconstructed.shape}\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = tf.reduce_mean(tf.square(test_target - reconstructed)).numpy()\n",
    "            mae = tf.reduce_mean(tf.abs(test_target - reconstructed)).numpy()\n",
    "            \n",
    "            print(f\"   Reconstruction MSE: {mse:.6f}\")\n",
    "            print(f\"   Reconstruction MAE: {mae:.6f}\")\n",
    "            \n",
    "            # Visualization\n",
    "            plt.figure(figsize=(16, 8))\n",
    "            \n",
    "            # Show first 4 samples: original and reconstructed\n",
    "            for i in range(4):\n",
    "                # Original\n",
    "                plt.subplot(2, 4, i + 1)\n",
    "                plt.imshow(test_img[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "                plt.title(f'Original {i+1}')\n",
    "                plt.colorbar()\n",
    "                \n",
    "                # Reconstructed\n",
    "                plt.subplot(2, 4, i + 5)\n",
    "                plt.imshow(reconstructed[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "                plt.title(f'Fourier Reconstructed {i+1}')\n",
    "                plt.colorbar()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle('VAE with Fourier Features: Original vs Reconstructed', y=1.02)\n",
    "            plt.show()\n",
    "            \n",
    "            # Error visualization\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            for i in range(4):\n",
    "                plt.subplot(1, 4, i + 1)\n",
    "                error = tf.abs(test_img[i, :, :, 0] - reconstructed[i, :, :, 0]).numpy()\n",
    "                plt.imshow(error, cmap='hot', origin='lower')\n",
    "                plt.title(f'Error {i+1}')\n",
    "                plt.colorbar()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle('VAE Fourier Reconstruction Errors', y=1.02)\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"\\\\nüéâ VAE with Fourier features validation completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Fourier VAE prediction failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"‚ùå Unexpected data format\")\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping Fourier VAE test (VAE not trained or not using Fourier features)\")\n",
    "\n",
    "# VAE Validation and Visualization\n",
    "print(\"=== VAE Model Validation ===\")\n",
    "\n",
    "if train_vae_model and trainer.vae_model is not None:\n",
    "    print(f\"üéØ Validating VAE model (Fourier: {config['use_fourier']})\")\n",
    "    \n",
    "    try:\n",
    "        # Get a test batch from the existing VAE test dataset\n",
    "        test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "        inputs, targets = test_batch\n",
    "        \n",
    "        if config['use_fourier']:\n",
    "            # For Fourier VAE, inputs should be a tuple (field, coordinates)\n",
    "            if isinstance(inputs, tuple) and len(inputs) == 2:\n",
    "                field_input, coord_input = inputs\n",
    "                \n",
    "                print(f\"‚úÖ Fourier VAE dataset structure correct:\")\n",
    "                print(f\"   Field input shape: {field_input.shape}\")\n",
    "                print(f\"   Coordinate input shape: {coord_input.shape}\")\n",
    "                print(f\"   Target shape: {targets.shape}\")\n",
    "                \n",
    "                # Test with first 4 samples\n",
    "                test_field = field_input[:4]\n",
    "                test_coord = coord_input[:4]\n",
    "                test_targets = targets[:4]\n",
    "                \n",
    "                # VAE prediction with coordinates\n",
    "                reconstructed = trainer.vae_model([test_field, test_coord])\n",
    "                \n",
    "                print(f\"‚úÖ Fourier VAE prediction successful!\")\n",
    "                print(f\"   Reconstruction shape: {reconstructed.shape}\")\n",
    "                \n",
    "                # Calculate metrics\n",
    "                mse = tf.reduce_mean(tf.square(test_targets - reconstructed)).numpy()\n",
    "                mae = tf.reduce_mean(tf.abs(test_targets - reconstructed)).numpy()\n",
    "                \n",
    "                print(f\"   Reconstruction MSE: {mse:.6f}\")\n",
    "                print(f\"   Reconstruction MAE: {mae:.6f}\")\n",
    "                \n",
    "                # Visualization\n",
    "                plt.figure(figsize=(16, 8))\n",
    "                \n",
    "                # Show original vs reconstructed\n",
    "                for i in range(4):\n",
    "                    # Original\n",
    "                    plt.subplot(2, 4, i + 1)\n",
    "                    plt.imshow(test_field[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "                    plt.title(f'Original {i+1}')\n",
    "                    plt.colorbar()\n",
    "                    \n",
    "                    # Reconstructed\n",
    "                    plt.subplot(2, 4, i + 5)\n",
    "                    plt.imshow(reconstructed[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "                    plt.title(f'VAE Fourier Recon {i+1}')\n",
    "                    plt.colorbar()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.suptitle('VAE with Fourier Features: Original vs Reconstructed', y=1.02)\n",
    "                plt.show()\n",
    "                \n",
    "                # Error visualization\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                for i in range(4):\n",
    "                    plt.subplot(1, 4, i + 1)\n",
    "                    error = tf.abs(test_field[i, :, :, 0] - reconstructed[i, :, :, 0]).numpy()\n",
    "                    plt.imshow(error, cmap='hot', origin='lower')\n",
    "                    plt.title(f'Error {i+1}')\n",
    "                    plt.colorbar()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.suptitle('VAE Fourier Reconstruction Errors', y=1.02)\n",
    "                plt.show()\n",
    "                \n",
    "                print(\"üéâ FOURIER VAE VALIDATION SUCCESSFUL!\")\n",
    "                print(\"   The VAE has been properly retrained with coordinate-aware data!\")\n",
    "                print(\"   Fourier features are working correctly!\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"‚ùå Unexpected Fourier VAE input format: {type(inputs)}\")\n",
    "                print(\"   Expected tuple (field, coordinates)\")\n",
    "        else:\n",
    "            # For standard VAE, inputs should be just the field data\n",
    "            print(f\"‚úÖ Standard VAE dataset structure:\")\n",
    "            print(f\"   Input shape: {inputs.shape}\")\n",
    "            print(f\"   Target shape: {targets.shape}\")\n",
    "            \n",
    "            # Test with first 4 samples\n",
    "            test_inputs = inputs[:4]\n",
    "            test_targets = targets[:4]\n",
    "            \n",
    "            # VAE prediction without coordinates\n",
    "            reconstructed = trainer.vae_model(test_inputs)\n",
    "            \n",
    "            print(f\"‚úÖ Standard VAE prediction successful!\")\n",
    "            print(f\"   Reconstruction shape: {reconstructed.shape}\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = tf.reduce_mean(tf.square(test_targets - reconstructed)).numpy()\n",
    "            mae = tf.reduce_mean(tf.abs(test_targets - reconstructed)).numpy()\n",
    "            \n",
    "            print(f\"   Reconstruction MSE: {mse:.6f}\")\n",
    "            print(f\"   Reconstruction MAE: {mae:.6f}\")\n",
    "            \n",
    "            print(\"‚úÖ STANDARD VAE VALIDATION SUCCESSFUL!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå VAE validation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping VAE validation (VAE training disabled or model not available)\")\n",
    "\n",
    "# Test the retrained VAE with coordinate inputs\n",
    "print(\"=== Testing Retrained VAE with Coordinate Inputs ===\")\n",
    "\n",
    "if trainer.vae_model is not None and config['use_fourier']:\n",
    "    print(\"üåä Testing Fourier-aware VAE...\")\n",
    "    \n",
    "    try:\n",
    "        # Get a test batch from the VAE dataset\n",
    "        test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "        inputs, targets = test_batch\n",
    "        \n",
    "        if isinstance(inputs, tuple) and len(inputs) == 2:\n",
    "            field_input, coord_input = inputs\n",
    "            \n",
    "            print(f\"‚úÖ VAE dataset structure is correct:\")\n",
    "            print(f\"   Field input shape: {field_input.shape}\")\n",
    "            print(f\"   Coordinate input shape: {coord_input.shape}\")\n",
    "            print(f\"   Target shape: {targets.shape}\")\n",
    "            \n",
    "            # Test with just 1 sample for quick verification\n",
    "            test_field = field_input[:1]\n",
    "            test_coord = coord_input[:1]\n",
    "            test_target = targets[:1]\n",
    "            \n",
    "            # Test VAE prediction\n",
    "            print(f\"\\nüîç Testing VAE prediction...\")\n",
    "            reconstructed = trainer.vae_model([test_field, test_coord])\n",
    "            \n",
    "            print(f\"‚úÖ SUCCESS! VAE now works with coordinate input!\")\n",
    "            print(f\"   Input field shape: {test_field.shape}\")\n",
    "            print(f\"   Input coord shape: {test_coord.shape}\")\n",
    "            print(f\"   Reconstruction shape: {reconstructed.shape}\")\n",
    "            \n",
    "            # Calculate error\n",
    "            mse = tf.reduce_mean(tf.square(test_target - reconstructed)).numpy()\n",
    "            print(f\"   Reconstruction MSE: {mse:.6f}\")\n",
    "            \n",
    "            # Visualization of the first sample\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            # Original\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(test_field[0, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "            plt.title('Original Field')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            # Reconstructed\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(reconstructed[0, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "            plt.title('VAE Reconstruction (with Fourier)')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            # Error\n",
    "            plt.subplot(1, 3, 3)\n",
    "            error = tf.abs(test_field[0, :, :, 0] - reconstructed[0, :, :, 0]).numpy()\n",
    "            plt.imshow(error, cmap='hot', origin='lower')\n",
    "            plt.title('Reconstruction Error')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle('VAE Successfully Retrained with Fourier Features!', y=1.05)\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nüéâ MISSION ACCOMPLISHED!\")\n",
    "            print(f\"   The VAE has been successfully retrained with coordinate-aware data!\")\n",
    "            print(f\"   Fourier features are now working properly!\")\n",
    "            print(f\"   The model expects 2-channel coordinate input as designed!\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ùå Unexpected VAE dataset format: {type(inputs)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå VAE test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå VAE model not available or not using Fourier features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ce05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN VAE Validation (No Cached Functions)\n",
    "print(\"=== CLEAN VAE Validation Test ===\")\n",
    "\n",
    "if 'trainer' in locals() and trainer.vae_model is not None:\n",
    "    print(f\"üéØ Testing VAE model (Fourier: {config['use_fourier']})\")\n",
    "    \n",
    "    try:\n",
    "        # Clear any old cached functions by getting fresh data\n",
    "        test_data_iter = iter(vae_test_dataset.take(1))\n",
    "        test_batch = next(test_data_iter)\n",
    "        inputs, targets = test_batch\n",
    "        \n",
    "        if config['use_fourier']:\n",
    "            # Expected format: ((field, coordinates), targets)\n",
    "            if isinstance(inputs, tuple) and len(inputs) == 2:\n",
    "                field_input, coord_input = inputs\n",
    "                \n",
    "                print(f\"‚úÖ VAE input structure correct:\")\n",
    "                print(f\"   Field: {field_input.shape}\")\n",
    "                print(f\"   Coord: {coord_input.shape}\")\n",
    "                print(f\"   Target: {targets.shape}\")\n",
    "                \n",
    "                # Test with just 1 sample\n",
    "                test_field = field_input[:1]\n",
    "                test_coord = coord_input[:1]\n",
    "                test_target = targets[:1]\n",
    "                \n",
    "                # VAE prediction\n",
    "                reconstructed = trainer.vae_model([test_field, test_coord])\n",
    "                \n",
    "                print(f\"‚úÖ VAE prediction successful!\")\n",
    "                print(f\"   Reconstruction shape: {reconstructed.shape}\")\n",
    "                \n",
    "                # Calculate MSE\n",
    "                mse = tf.reduce_mean(tf.square(test_target - reconstructed)).numpy()\n",
    "                print(f\"   MSE: {mse:.6f}\")\n",
    "                \n",
    "                print(\"üéâ SUCCESS: VAE with Fourier features is working correctly!\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"‚ùå Unexpected input format: {type(inputs)}\")\n",
    "                \n",
    "        else:\n",
    "            # Standard VAE test\n",
    "            print(f\"‚úÖ Standard VAE input: {inputs.shape}\")\n",
    "            reconstructed = trainer.vae_model(inputs[:1])\n",
    "            print(f\"‚úÖ Standard VAE working!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå VAE test failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No trainer or VAE model available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a427fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate how the VAE model was actually built during training\n",
    "print(\"=== VAE Model Architecture Investigation ===\")\n",
    "\n",
    "if trainer.vae_model is not None:\n",
    "    print(f\"VAE model use_fourier: {trainer.vae_model.use_fourier}\")\n",
    "    \n",
    "    # Check the encoder structure\n",
    "    encoder = trainer.vae_model.encoder\n",
    "    print(f\"\\\\nEncoder type: {type(encoder)}\")\n",
    "    print(f\"Encoder use_fourier: {encoder.use_fourier}\")\n",
    "    \n",
    "    # Check if encoder has fourier layer\n",
    "    if hasattr(encoder, 'fourier_layer') and encoder.fourier_layer:\n",
    "        fourier_layer = encoder.fourier_layer\n",
    "        print(f\"\\\\nFourier layer found: {fourier_layer}\")\n",
    "        print(f\"  Type: {type(fourier_layer)}\")\n",
    "        if hasattr(fourier_layer, 'built') and fourier_layer.built:\n",
    "            print(f\"  Layer is built: {fourier_layer.built}\")\n",
    "            # Check the projection kernel\n",
    "            if hasattr(fourier_layer, 'proj_kernel'):\n",
    "                proj_kernel = fourier_layer.proj_kernel\n",
    "                print(f\"  Projection kernel: {proj_kernel}\")\n",
    "                if hasattr(proj_kernel, 'kernel') and proj_kernel.kernel is not None:\n",
    "                    kernel_shape = proj_kernel.kernel.shape\n",
    "                    print(f\"  Conv2D kernel shape: {kernel_shape}\")\n",
    "                    print(f\"  Expected input channels: {kernel_shape[2]}\")\n",
    "                    print(f\"  Output channels: {kernel_shape[3]}\")\n",
    "    else:\n",
    "        print(\"\\\\nNo Fourier layer found in encoder\")\n",
    "    \n",
    "    # Test what happens if we try to call the model with a simple input\n",
    "    print(f\"\\\\n=== Testing with Simple Field Input ===\")\n",
    "    try:\n",
    "        # Create a simple test input (just the field)\n",
    "        test_field = tf.zeros((1, 128, 256, 1))\n",
    "        \n",
    "        # Try calling just the encoder with the field (no coordinates)\n",
    "        print(\"Testing encoder with field input only...\")\n",
    "        # encoder_out = encoder(test_field)\n",
    "        # print(f\"Encoder output successful with field only!\")\n",
    "        \n",
    "        # Try calling the full VAE model with just field input\n",
    "        print(\"Testing full VAE with field input only...\")\n",
    "        vae_out = trainer.vae_model(test_field)\n",
    "        print(f\"‚úÖ VAE works with field input only! Output shape: {vae_out.shape}\")\n",
    "        print(\"This suggests the model was trained without proper Fourier coordinates!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Simple field input failed: {e}\")\n",
    "        \n",
    "    # Check how the training dataset actually looked\n",
    "    print(f\"\\\\n=== VAE Training Dataset Analysis ===\")\n",
    "    sample_train_batch = next(iter(vae_train_dataset.take(1)))\n",
    "    print(f\"Training batch structure: {type(sample_train_batch)}\")\n",
    "    if isinstance(sample_train_batch, tuple):\n",
    "        print(f\"  Input shape: {sample_train_batch[0].shape}\")\n",
    "        print(f\"  Target shape: {sample_train_batch[1].shape}\")\n",
    "        print(\"  The training data contains only field data, no coordinates!\")\n",
    "        print(\"  This explains why Fourier features don't work properly.\")\n",
    "        \n",
    "else:\n",
    "    print(\"No VAE model available\")\n",
    "\n",
    "# Verify that the VAE model architecture is now correct for Fourier features\n",
    "print(\"=== VAE Model Architecture Verification ===\")\n",
    "\n",
    "if trainer.vae_model is not None:\n",
    "    print(f\"VAE model use_fourier: {trainer.vae_model.use_fourier}\")\n",
    "    \n",
    "    if trainer.vae_model.use_fourier:\n",
    "        print(\"\\nüåä Analyzing Fourier-aware VAE architecture...\")\n",
    "        \n",
    "        # Check the encoder structure\n",
    "        encoder = trainer.vae_model.encoder\n",
    "        print(f\"Encoder type: {type(encoder)}\")\n",
    "        print(f\"Encoder use_fourier: {encoder.use_fourier}\")\n",
    "        \n",
    "        # Check if encoder has fourier layer\n",
    "        if hasattr(encoder, 'fourier_layer') and encoder.fourier_layer:\n",
    "            fourier_layer = encoder.fourier_layer\n",
    "            print(f\"\\n‚úÖ Fourier layer found: {fourier_layer}\")\n",
    "            print(f\"  Type: {type(fourier_layer)}\")\n",
    "            \n",
    "            if hasattr(fourier_layer, 'built') and fourier_layer.built:\n",
    "                print(f\"  Layer is built: {fourier_layer.built}\")\n",
    "                \n",
    "                # Check the projection kernel\n",
    "                if hasattr(fourier_layer, 'proj_kernel'):\n",
    "                    proj_kernel = fourier_layer.proj_kernel\n",
    "                    print(f\"  Projection kernel: {proj_kernel}\")\n",
    "                    if hasattr(proj_kernel, 'kernel') and proj_kernel.kernel is not None:\n",
    "                        kernel_shape = proj_kernel.kernel.shape\n",
    "                        print(f\"  Conv2D kernel shape: {kernel_shape}\")\n",
    "                        print(f\"  Expected input channels: {kernel_shape[2]}\")\n",
    "                        print(f\"  Output channels: {kernel_shape[3]}\")\n",
    "                        \n",
    "                        # Verify proper 2-channel input for coordinates\n",
    "                        if kernel_shape[2] == 2:\n",
    "                            print(f\"  ‚úÖ Fourier layer correctly expects 2-channel coordinate input!\")\n",
    "                        else:\n",
    "                            print(f\"  ‚ö†Ô∏è  Fourier layer expects {kernel_shape[2]} channels, should be 2\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  Fourier layer not yet built\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå No Fourier layer found in encoder\")\n",
    "        \n",
    "        # Test the model with coordinate input to build the layers\n",
    "        print(f\"\\nüîç Testing model to ensure proper layer construction...\")\n",
    "        try:\n",
    "            # Create test inputs with the right format\n",
    "            test_field = tf.zeros((1, config['input_shape'][0], config['input_shape'][1], 1))\n",
    "            test_coord = tf.zeros((1, config['input_shape'][0], config['input_shape'][1], 2))\n",
    "            \n",
    "            print(f\"  Test field shape: {test_field.shape}\")\n",
    "            print(f\"  Test coordinate shape: {test_coord.shape}\")\n",
    "            \n",
    "            # This should build the layers properly\n",
    "            output = trainer.vae_model([test_field, test_coord])\n",
    "            print(f\"  ‚úÖ Model successfully processes coordinate input! Output shape: {output.shape}\")\n",
    "            \n",
    "            # Now check the Fourier layer again\n",
    "            if hasattr(encoder, 'fourier_layer') and encoder.fourier_layer:\n",
    "                fourier_layer = encoder.fourier_layer\n",
    "                if hasattr(fourier_layer, 'proj_kernel') and fourier_layer.proj_kernel.kernel is not None:\n",
    "                    kernel_shape = fourier_layer.proj_kernel.kernel.shape\n",
    "                    print(f\"  Final Fourier kernel shape: {kernel_shape}\")\n",
    "                    if kernel_shape[2] == 2:\n",
    "                        print(f\"  üéâ SUCCESS: Fourier layer now properly expects 2-channel coordinates!\")\n",
    "                    else:\n",
    "                        print(f\"  ‚ùå ISSUE: Fourier layer still expects {kernel_shape[2]} channels\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Model test failed: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\nüîÑ Standard VAE (no Fourier features)\")\n",
    "        \n",
    "        # Test standard VAE\n",
    "        try:\n",
    "            test_field = tf.zeros((1, config['input_shape'][0], config['input_shape'][1], 1))\n",
    "            output = trainer.vae_model(test_field)\n",
    "            print(f\"‚úÖ Standard VAE works correctly! Output shape: {output.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Standard VAE test failed: {e}\")\n",
    "    \n",
    "    # Show model summary for reference\n",
    "    print(f\"\\nüìã VAE Model Summary:\")\n",
    "    try:\n",
    "        trainer.vae_model.summary()\n",
    "    except:\n",
    "        print(\"Could not display model summary\")\n",
    "        \n",
    "else:\n",
    "    print(\"No VAE model available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e2163b",
   "metadata": {},
   "source": [
    "## FLRNet Validation and Visualization\n",
    "\n",
    "After training the FLRNet model, we can evaluate its performance by:\n",
    "1. Testing sensor-to-field reconstruction accuracy\n",
    "2. Comparing predicted vs ground truth flow fields\n",
    "3. Analyzing reconstruction quality metrics\n",
    "4. Visualizing sensor positions overlaid on predictions\n",
    "\n",
    "This section validates the main FLRNet model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLRNet Validation and Visualization\n",
    "if train_flrnet_model and trainer.flr_model is not None:\n",
    "    print(\"=== FLRNet Model Validation ===\")\n",
    "    \n",
    "    # Get test samples for evaluation\n",
    "    test_batch = next(iter(flrnet_test_dataset.batch(8)))\n",
    "    sensor_readings = test_batch[0]  # Input sensor readings\n",
    "    ground_truth = test_batch[1]     # Target flow fields\n",
    "    \n",
    "    # Get FLRNet predictions\n",
    "    predictions = trainer.flr_model.predict(sensor_readings)\n",
    "    \n",
    "    # Calculate reconstruction metrics\n",
    "    mse = np.mean((ground_truth.numpy() - predictions) ** 2)\n",
    "    mae = np.mean(np.abs(ground_truth.numpy() - predictions))\n",
    "    \n",
    "    # Calculate relative error\n",
    "    relative_error = np.mean(np.abs(ground_truth.numpy() - predictions) / (np.abs(ground_truth.numpy()) + 1e-8))\n",
    "    \n",
    "    print(f\"FLRNet Reconstruction MSE: {mse:.6f}\")\n",
    "    print(f\"FLRNet Reconstruction MAE: {mae:.6f}\")\n",
    "    print(f\"FLRNet Relative Error: {relative_error:.6f}\")\n",
    "    \n",
    "    # Visualize predictions vs ground truth\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Show first 4 samples: sensor readings, ground truth, predictions, and errors\n",
    "    for i in range(4):\n",
    "        # Sensor readings visualization\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        field_viz = np.zeros((h, w))\n",
    "        # Place sensor readings on the field for visualization\n",
    "        for j, (x_pos, y_pos) in enumerate(sensor_positions):\n",
    "            if j < len(sensor_readings[i]):\n",
    "                field_viz[int(y_pos), int(x_pos)] = sensor_readings[i, j]\n",
    "        plt.imshow(field_viz, cmap='RdBu_r', origin='lower')\n",
    "        plt.title(f'Sensor Readings {i+1}')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Ground truth\n",
    "        plt.subplot(4, 4, i + 5)\n",
    "        plt.imshow(ground_truth[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "        plt.title(f'Ground Truth {i+1}')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Predictions\n",
    "        plt.subplot(4, 4, i + 9)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "        plt.title(f'FLRNet Prediction {i+1}')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Error\n",
    "        plt.subplot(4, 4, i + 13)\n",
    "        error = np.abs(ground_truth[i, :, :, 0] - predictions[i, :, :, 0])\n",
    "        plt.imshow(error, cmap='hot', origin='lower')\n",
    "        plt.title(f'Prediction Error {i+1}')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('FLRNet: Sensor Readings ‚Üí Ground Truth vs Predictions', y=0.98)\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional visualization: Overlay sensor positions on predictions\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    for i in range(2):\n",
    "        # Prediction with sensor overlay\n",
    "        plt.subplot(1, 2, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "        \n",
    "        # Overlay sensor positions\n",
    "        sensor_x = sensor_positions[:, 0]\n",
    "        sensor_y = sensor_positions[:, 1]\n",
    "        plt.scatter(sensor_x, sensor_y, c='black', s=100, marker='o', edgecolors='white', linewidth=2)\n",
    "        \n",
    "        # Add sensor value annotations\n",
    "        for j, (x, y) in enumerate(sensor_positions):\n",
    "            if j < len(sensor_readings[i]):\n",
    "                plt.annotate(f'{sensor_readings[i, j]:.2f}', \n",
    "                           (x, y), xytext=(5, 5), textcoords='offset points',\n",
    "                           fontsize=8, color='white', weight='bold')\n",
    "        \n",
    "        plt.title(f'FLRNet Prediction {i+1} with Sensor Positions')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('FLRNet Predictions with Sensor Position Overlay', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"FLRNet validation completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping FLRNet validation (FLRNet training was disabled or model not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742eb60",
   "metadata": {},
   "source": [
    "## Training Summary and Model Saving\n",
    "\n",
    "Complete the training workflow by saving trained models and summarizing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Summary and Model Saving\n",
    "print(\"=== Training Summary ===\")\n",
    "\n",
    "# Print configuration summary\n",
    "print(f\"Configuration: {config_name}\")\n",
    "print(f\"Layout type: {layout_type}\")\n",
    "print(f\"Number of sensors: {n_sensors}\")\n",
    "print(f\"VAE training: {'Enabled' if train_vae_model else 'Disabled'}\")\n",
    "print(f\"FLRNet training: {'Enabled' if train_flrnet_model else 'Disabled'}\")\n",
    "\n",
    "# Save trained models\n",
    "if trainer.vae_model is not None:\n",
    "    vae_filename = f\"vae_model_{layout_type}_{n_sensors}_sensors.h5\"\n",
    "    trainer.vae_model.save(vae_filename)\n",
    "    print(f\"VAE model saved as: {vae_filename}\")\n",
    "\n",
    "if trainer.flr_model is not None:\n",
    "    flrnet_filename = f\"flrnet_model_{layout_type}_{n_sensors}_sensors.h5\"\n",
    "    trainer.flr_model.save(flrnet_filename)\n",
    "    print(f\"FLRNet model saved as: {flrnet_filename}\")\n",
    "\n",
    "print(\"\\n=== Training Complete ===\")\n",
    "print(\"All requested models have been trained and validated.\")\n",
    "print(\"Models have been saved for future use.\")\n",
    "\n",
    "# üîÑ FLRNet Training Continuation\n",
    "# Toggle for enabling FLRNet training continuation (set to True to continue from checkpoint)\n",
    "continue_flrnet_training = False  # Change to True to enable\n",
    "\n",
    "# Toggle for perceptual loss during continuation (can be different from original training)\n",
    "use_perceptual_loss_flrnet_continuation = True  # Change to False to disable perceptual loss\n",
    "\n",
    "if continue_flrnet_training:\n",
    "    if trainer.flr_model is not None:\n",
    "        print(\"üîÑ Continuing FLRNet training from loaded checkpoint...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Configuration for continued training\n",
    "        additional_epochs = 50\n",
    "        learning_rate = 1e-6  # Lower learning rate for fine-tuning\n",
    "        \n",
    "        print(f\"üìã Continuation Configuration:\")\n",
    "        print(f\"   - Additional epochs: {additional_epochs}\")\n",
    "        print(f\"   - Learning rate: {learning_rate} (reduced for fine-tuning)\")\n",
    "        print(f\"   - Perceptual loss: {'‚úÖ ENABLED' if use_perceptual_loss_flrnet_continuation else '‚ùå DISABLED'}\")\n",
    "        print(f\"   - Model weights: ‚úÖ PRESERVED from checkpoint\")\n",
    "        \n",
    "        # Continue training using the proper method that preserves weights\n",
    "        continued_flrnet = trainer.continue_flrnet_training(\n",
    "            train_dataset=flrnet_train_dataset,\n",
    "            val_dataset=flrnet_test_dataset,\n",
    "            epochs=additional_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            patience=config['patience'],\n",
    "            reduce_lr_patience=config['reduce_lr_patience'],\n",
    "            use_perceptual_loss=use_perceptual_loss_flrnet_continuation  # New option for perceptual loss\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ FLRNet training continuation completed!\")\n",
    "        \n",
    "    elif trainer.vae_model is not None:\n",
    "        print(\"‚ùå Cannot continue FLRNet training: No FLRNet model loaded from checkpoint\")\n",
    "        print(\"   First load an FLRNet model using the checkpoint loading cell above\")\n",
    "    else:\n",
    "        print(\"‚ùå Cannot continue FLRNet training: No models loaded from checkpoint\")\n",
    "        print(\"   First load models using the checkpoint loading cell above\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  FLRNet training continuation is disabled\")\n",
    "    print(\"   Set continue_flrnet_training = True to enable\")\n",
    "    if not continue_flrnet_training:\n",
    "        print(f\"   Current perceptual loss setting: {'‚úÖ ENABLED' if use_perceptual_loss_flrnet_continuation else '‚ùå DISABLED'}\")\n",
    "        print(\"   (can be changed via use_perceptual_loss_flrnet_continuation variable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load VAE Model\n",
    "checkpoint_dir = \"E:/Research/Physics-informed-machine-learning/flow_field_recon_parc/checkpoints/fourierTrue_percepTrue_edge_8\"\n",
    "\n",
    "checkpoint_path = Path(checkpoint_dir)\n",
    "vae_checkpoint_path = checkpoint_path / f\"checkpoint_{config['model_name']}_vae_best\"\n",
    "print(f\"\\nüìÅ Looking for VAE checkpoint: {vae_checkpoint_path}\")\n",
    "\n",
    "if vae_checkpoint_path.exists():\n",
    "    print(\"‚úÖ VAE checkpoint found, creating model...\")\n",
    "    \n",
    "    # Create VAE model architecture\n",
    "    vae_model = models_improved.FLRVAE(\n",
    "        input_shape=config['input_shape'],\n",
    "        latent_dims=config['latent_dims'],\n",
    "        n_base_features=config['n_base_features'],\n",
    "        use_fourier=config['use_fourier'],\n",
    "        use_perceptual_loss=config['use_perceptual_loss']\n",
    "    )\n",
    "    \n",
    "    # Build the model by calling it once with dummy input\n",
    "    dummy_input = tf.zeros((1,) + config['input_shape'])\n",
    "    if config['use_fourier']:\n",
    "        dummy_coord = tf.zeros((1, config['input_shape'][0], config['input_shape'][1], 2))\n",
    "        _ = vae_model([dummy_input, dummy_coord])\n",
    "        print(\"üåä VAE model built for Fourier features\")\n",
    "    else:\n",
    "        _ = vae_model(dummy_input)\n",
    "        print(\"üîÑ VAE model built for standard features\")\n",
    "    \n",
    "    # Load weights\n",
    "    vae_model.load_weights(str(vae_checkpoint_path))\n",
    "    print(f\"‚úÖ VAE weights loaded successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå VAE checkpoint not found at: {vae_checkpoint_path}\")\n",
    "    vae_model = None\n",
    "print(\"üé® === VAE Test Visualization ===\")\n",
    "print(f\"Model type: {'Fourier-aware' if trainer.vae_model.use_fourier else 'Standard'} VAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f87895",
   "metadata": {},
   "source": [
    "## VAE Test Visualization\n",
    "\n",
    "Comprehensive visualization of the VAE model performance with detailed comparisons between original and reconstructed flow fields. This section tests the VAE with both coordinate-aware and standard inputs to validate proper Fourier feature integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20fc2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced VAE Test Visualization (replace the existing cell)\n",
    "\n",
    "# Comprehensive VAE Test Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if trainer.vae_model is not None:\n",
    "    print(\"üé® === VAE Test Visualization ===\")\n",
    "    print(f\"Model type: {'Fourier-aware' if trainer.vae_model.use_fourier else 'Standard'} VAE\")\n",
    "    \n",
    "    # Get test data batch directly from the properly formatted dataset\n",
    "    test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "    print(\"Using coordinate-aware VAE test dataset\")\n",
    "    \n",
    "    # Extract inputs and targets\n",
    "    if isinstance(test_batch, tuple) and len(test_batch) == 2:\n",
    "        test_inputs, test_targets = test_batch\n",
    "        \n",
    "        # Handle coordinate-aware inputs for Fourier VAE\n",
    "        if isinstance(test_inputs, (list, tuple)) and len(test_inputs) == 2:\n",
    "            test_fields, test_coords = test_inputs\n",
    "            print(f\"‚úÖ Fourier VAE inputs detected:\")\n",
    "            print(f\"   Field shape: {test_fields.shape}\")\n",
    "            print(f\"   Coordinate shape: {test_coords.shape}\")\n",
    "            input_for_prediction = [test_fields, test_coords]\n",
    "        else:\n",
    "            test_fields = test_inputs\n",
    "            print(f\"‚úÖ Standard VAE input: {test_fields.shape}\")\n",
    "            input_for_prediction = test_fields\n",
    "        \n",
    "        print(f\"   Target shape: {test_targets.shape}\")\n",
    "        \n",
    "        # Generate VAE reconstructions\n",
    "        print(\"\\nüîÆ Generating VAE reconstructions...\")\n",
    "        try:\n",
    "            # Limit to manageable batch size for visualization\n",
    "            max_samples = min(4, test_fields.shape[0])\n",
    "            \n",
    "            if isinstance(input_for_prediction, list):\n",
    "                limited_input = [test_fields[:max_samples], test_coords[:max_samples]]\n",
    "            else:\n",
    "                limited_input = test_fields[:max_samples]\n",
    "            limited_targets = test_targets[:max_samples]\n",
    "            \n",
    "            reconstructions = trainer.vae_model.predict(limited_input, verbose=0)\n",
    "            print(f\"‚úÖ Reconstruction successful! Shape: {reconstructions.shape}\")\n",
    "            \n",
    "            # Calculate reconstruction metrics\n",
    "            mse = np.mean((limited_targets.numpy() - reconstructions) ** 2)\n",
    "            mae = np.mean(np.abs(limited_targets.numpy() - reconstructions))\n",
    "            max_error = np.max(np.abs(limited_targets.numpy() - reconstructions))\n",
    "            \n",
    "            print(f\"\\nüìä VAE Reconstruction Metrics:\")\n",
    "            print(f\"   MSE: {mse:.6f} ({'Excellent' if mse < 0.01 else 'Good' if mse < 0.05 else 'Needs improvement'})\")\n",
    "            print(f\"   MAE: {mae:.6f}\")\n",
    "            print(f\"   Max Error: {max_error:.6f}\")\n",
    "            \n",
    "            # Create comprehensive visualization\n",
    "            n_samples = max_samples\n",
    "            fig, axes = plt.subplots(3, n_samples, figsize=(5*n_samples, 12))\n",
    "            \n",
    "            if n_samples == 1:\n",
    "                axes = axes.reshape(-1, 1)\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                # Original field\n",
    "                im1 = axes[0, i].imshow(limited_targets[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "                axes[0, i].set_title(f'Original Field {i+1}', fontweight='bold')\n",
    "                axes[0, i].set_xlabel('X Position')\n",
    "                axes[0, i].set_ylabel('Y Position')\n",
    "                plt.colorbar(im1, ax=axes[0, i], shrink=0.8)\n",
    "                \n",
    "                # Reconstructed field\n",
    "                im2 = axes[1, i].imshow(reconstructions[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "                axes[1, i].set_title(f'VAE Reconstruction {i+1}', fontweight='bold')\n",
    "                axes[1, i].set_xlabel('X Position')\n",
    "                axes[1, i].set_ylabel('Y Position')\n",
    "                plt.colorbar(im2, ax=axes[1, i], shrink=0.8)\n",
    "                \n",
    "                # Error map\n",
    "                error = np.abs(limited_targets[i, :, :, 0] - reconstructions[i, :, :, 0])\n",
    "                im3 = axes[2, i].imshow(error, cmap='hot', origin='lower')\n",
    "                axes[2, i].set_title(f'Reconstruction Error {i+1}', fontweight='bold')\n",
    "                axes[2, i].set_xlabel('X Position')\n",
    "                axes[2, i].set_ylabel('Y Position')\n",
    "                plt.colorbar(im3, ax=axes[2, i], shrink=0.8)\n",
    "                \n",
    "                # Add error statistics as text\n",
    "                sample_mse = np.mean(error**2)\n",
    "                sample_max = np.max(error)\n",
    "                axes[2, i].text(0.02, 0.98, f'MSE: {sample_mse:.4f}\\nMax: {sample_max:.4f}', \n",
    "                               transform=axes[2, i].transAxes, verticalalignment='top',\n",
    "                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                               fontsize=9, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f'VAE Reconstruction Results - {config_name}', y=1.02, fontsize=16, fontweight='bold')\n",
    "            plt.show()\n",
    "            \n",
    "            # Statistical analysis\n",
    "            print(f\"\\nüìà Statistical Comparison:\")\n",
    "            orig_stats = {\n",
    "                'mean': np.mean(limited_targets.numpy()),\n",
    "                'std': np.std(limited_targets.numpy()),\n",
    "                'min': np.min(limited_targets.numpy()),\n",
    "                'max': np.max(limited_targets.numpy())\n",
    "            }\n",
    "            \n",
    "            recon_stats = {\n",
    "                'mean': np.mean(reconstructions),\n",
    "                'std': np.std(reconstructions),\n",
    "                'min': np.min(reconstructions),\n",
    "                'max': np.max(reconstructions)\n",
    "            }\n",
    "            \n",
    "            print(f\"   Original  - Mean: {orig_stats['mean']:.4f}, Std: {orig_stats['std']:.4f}, Range: [{orig_stats['min']:.4f}, {orig_stats['max']:.4f}]\")\n",
    "            print(f\"   Reconstructed - Mean: {recon_stats['mean']:.4f}, Std: {recon_stats['std']:.4f}, Range: [{recon_stats['min']:.4f}, {recon_stats['max']:.4f}]\")\n",
    "            \n",
    "            # Distribution comparison\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Value distributions\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.hist(limited_targets.numpy().flatten(), bins=50, alpha=0.7, label='Original', density=True, color='blue')\n",
    "            plt.hist(reconstructions.flatten(), bins=50, alpha=0.7, label='Reconstructed', density=True, color='red')\n",
    "            plt.xlabel('Field Value')\n",
    "            plt.ylabel('Density')\n",
    "            plt.title('Value Distribution Comparison', fontweight='bold')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Error distribution\n",
    "            plt.subplot(1, 3, 2)\n",
    "            errors = np.abs(limited_targets.numpy() - reconstructions).flatten()\n",
    "            plt.hist(errors, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "            plt.xlabel('Absolute Error')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Reconstruction Error Distribution', fontweight='bold')\n",
    "            plt.axvline(np.mean(errors), color='darkred', linestyle='--', linewidth=2, label=f'Mean: {np.mean(errors):.4f}')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Relative error distribution\n",
    "            plt.subplot(1, 3, 3)\n",
    "            relative_errors = errors / (np.abs(limited_targets.numpy().flatten()) + 1e-8)\n",
    "            plt.hist(relative_errors, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "            plt.xlabel('Relative Error')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Relative Error Distribution', fontweight='bold')\n",
    "            plt.axvline(np.mean(relative_errors), color='darkorange', linestyle='--', linewidth=2, label=f'Mean: {np.mean(relative_errors):.4f}')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle('VAE Performance Analysis', y=1.02, fontsize=14, fontweight='bold')\n",
    "            plt.show()\n",
    "            \n",
    "            # Fourier feature analysis (if applicable)\n",
    "            if trainer.vae_model.use_fourier and isinstance(test_inputs, (list, tuple)):\n",
    "                print(f\"\\nüåä Fourier Feature Analysis:\")\n",
    "                print(f\"   Coordinate grid statistics:\")\n",
    "                print(f\"   X coordinates - Range: [{np.min(test_coords[0, :, :, 0]):.3f}, {np.max(test_coords[0, :, :, 0]):.3f}]\")\n",
    "                print(f\"   Y coordinates - Range: [{np.min(test_coords[0, :, :, 1]):.3f}, {np.max(test_coords[0, :, :, 1]):.3f}]\")\n",
    "                \n",
    "                # Show coordinate grids for first sample\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "                \n",
    "                im1 = axes[0].imshow(test_coords[0, :, :, 0], cmap='viridis', origin='lower')\n",
    "                axes[0].set_title('X Coordinate Grid', fontweight='bold')\n",
    "                axes[0].set_xlabel('X Position')\n",
    "                axes[0].set_ylabel('Y Position')\n",
    "                plt.colorbar(im1, ax=axes[0])\n",
    "                \n",
    "                im2 = axes[1].imshow(test_coords[0, :, :, 1], cmap='plasma', origin='lower')\n",
    "                axes[1].set_title('Y Coordinate Grid', fontweight='bold')\n",
    "                axes[1].set_xlabel('X Position')\n",
    "                axes[1].set_ylabel('Y Position')\n",
    "                plt.colorbar(im2, ax=axes[1])\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.suptitle('Fourier Feature Coordinate Grids', y=1.02, fontsize=14, fontweight='bold')\n",
    "                plt.show()\n",
    "            \n",
    "            print(f\"\\nüéâ VAE visualization completed successfully!\")\n",
    "            print(f\"   üìä Reconstruction Quality: {'üèÜ Excellent' if mse < 0.01 else '‚úÖ Good' if mse < 0.05 else '‚ö†Ô∏è Needs improvement'}\")\n",
    "            print(f\"   üåä Fourier Features: {'‚úÖ Working correctly' if trainer.vae_model.use_fourier else '‚ûñ Not used'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå VAE prediction failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    else:\n",
    "        print(f\"‚ùå Unexpected test batch structure: {type(test_batch)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No VAE model available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277de7f",
   "metadata": {},
   "source": [
    "## Load Trained Models from Checkpoint\n",
    "\n",
    "Now let's load the trained models from saved checkpoints using the FLRTrainer's built-in methods. The trainer provides several loading options:\n",
    "\n",
    "- **`load_vae_from_checkpoint()`**: Load only the VAE model\n",
    "- **`load_flrnet_from_checkpoint()`**: Load only the FLRNet model  \n",
    "- **`load_models_from_checkpoint()`**: Load both models at once\n",
    "\n",
    "### Features:\n",
    "- **Automatic checkpoint detection**: Finds the best available checkpoint (best ‚Üí last ‚Üí final_weights)\n",
    "- **Robust path handling**: Uses correct TensorFlow checkpoint format (no file extensions)\n",
    "- **Error handling**: Graceful fallback and clear error messages\n",
    "- **Architecture consistency**: Ensures loaded model matches trainer configuration\n",
    "- **Smart dependencies**: FLRNet loading automatically handles VAE dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d837893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading trained VAE model from checkpoint using FLRTrainer...\n",
      "============================================================\n",
      "üìÇ Using checkpoint directory: checkpoints\\fourierTrue_percepFalse_edge_8\n",
      "üìã Available checkpoint files:\n",
      "   - checkpoint\n",
      "   - checkpoint_fourierTrue_percepFalse_edge_8_vae_best.data-00000-of-00001\n",
      "   - checkpoint_fourierTrue_percepFalse_edge_8_vae_best.index\n",
      "   - checkpoint_fourierTrue_percepFalse_edge_8_vae_final_weights.data-00000-of-00001\n",
      "   - checkpoint_fourierTrue_percepFalse_edge_8_vae_final_weights.index\n",
      "   - checkpoint_fourierTrue_percepFalse_edge_8_vae_last.data-00000-of-00001\n",
      "   - checkpoint_fourierTrue_percepFalse_edge_8_vae_last.index\n",
      "üîç Loading VAE model from checkpoint directory: checkpoints\\fourierTrue_percepFalse_edge_8\n",
      "‚úÖ Found vae_best checkpoint: checkpoints\\fourierTrue_percepFalse_edge_8\\checkpoint_fourierTrue_percepFalse_edge_8_vae_best\n",
      "üåä VAE model built for Fourier features\n",
      "üìã VAE Model Architecture:\n",
      "   - Input shape: (128, 256, 1)\n",
      "   - Latent dims: 8\n",
      "   - Base features: 64\n",
      "   - Use Fourier: True\n",
      "   - Perceptual loss: False\n",
      "‚úÖ Successfully loaded VAE model from vae_best checkpoint!\n",
      "\n",
      "üéâ VAE model loaded successfully!\n",
      "üìà Model is ready for inference and visualization\n",
      "‚úÖ Fourier VAE verification successful!\n",
      "   - Field input shape: (1, 128, 256, 1)\n",
      "   - Coordinate input shape: (1, 128, 256, 2)\n",
      "   - Output shape: (1, 128, 256, 1)\n",
      "   - Output range: [-0.1778, 1.5635]\n",
      "‚úÖ Successfully loaded VAE model from vae_best checkpoint!\n",
      "\n",
      "üéâ VAE model loaded successfully!\n",
      "üìà Model is ready for inference and visualization\n",
      "‚úÖ Fourier VAE verification successful!\n",
      "   - Field input shape: (1, 128, 256, 1)\n",
      "   - Coordinate input shape: (1, 128, 256, 2)\n",
      "   - Output shape: (1, 128, 256, 1)\n",
      "   - Output range: [-0.1778, 1.5635]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the VAE model using the trainer's built-in method\n",
    "print(\"üöÄ Loading trained VAE model from checkpoint using FLRTrainer...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fix checkpoint path - use the directory, not the specific file\n",
    "checkpoint_directory = Path(config['checkpoint_dir'])\n",
    "print(f\"üìÇ Using checkpoint directory: {checkpoint_directory}\")\n",
    "\n",
    "# List available checkpoints for debugging\n",
    "if checkpoint_directory.exists():\n",
    "    print(f\"üìã Available checkpoint files:\")\n",
    "    for file in sorted(checkpoint_directory.iterdir()):\n",
    "        print(f\"   - {file.name}\")\n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint directory not found: {checkpoint_directory}\")\n",
    "\n",
    "# Use the trainer's built-in checkpoint loading method\n",
    "vae_model = trainer.load_vae_from_checkpoint(\n",
    "    checkpoint_dir=checkpoint_directory,\n",
    "    latent_dims=config['latent_dims'],\n",
    "    n_base_features=config['n_base_features'],\n",
    "    use_perceptual_loss=config['use_perceptual_loss'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if vae_model is not None:\n",
    "    print(\"\\nüéâ VAE model loaded successfully!\")\n",
    "    print(\"üìà Model is ready for inference and visualization\")\n",
    "    \n",
    "    # Verify the model works with a test prediction\n",
    "    try:\n",
    "        # Get a small batch for testing\n",
    "        test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "        test_inputs, test_targets = test_batch\n",
    "        \n",
    "        # Handle different input formats (Fourier vs Standard VAE)\n",
    "        if isinstance(test_inputs, (list, tuple)) and len(test_inputs) == 2:\n",
    "            # Fourier VAE expects [field, coordinates]\n",
    "            test_field, test_coord = test_inputs\n",
    "            test_prediction = vae_model([test_field[:1], test_coord[:1]])\n",
    "            print(f\"‚úÖ Fourier VAE verification successful!\")\n",
    "            print(f\"   - Field input shape: {test_field[:1].shape}\")\n",
    "            print(f\"   - Coordinate input shape: {test_coord[:1].shape}\")\n",
    "        else:\n",
    "            # Standard VAE expects just field data\n",
    "            test_prediction = vae_model(test_inputs[:1])\n",
    "            print(f\"‚úÖ Standard VAE verification successful!\")\n",
    "            print(f\"   - Input shape: {test_inputs[:1].shape}\")\n",
    "        \n",
    "        print(f\"   - Output shape: {test_prediction.shape}\")\n",
    "        print(f\"   - Output range: [{test_prediction.numpy().min():.4f}, {test_prediction.numpy().max():.4f}]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Model verification failed: {str(e)}\")\n",
    "        print(\"Model loaded but may have compatibility issues\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to load VAE model\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Check if training completed successfully\")\n",
    "    print(\"   2. Verify checkpoint files exist in the directory\")\n",
    "    print(\"   3. Ensure model configuration matches training setup\")\n",
    "    print(\"   4. Try running the training cell again if checkpoints are missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20499860",
   "metadata": {},
   "source": [
    "## üîÑ Training Continuation with Advanced Options\n",
    "\n",
    "### What is Training Continuation?\n",
    "- **Loading for Inference**: Loads model weights for evaluation/visualization only\n",
    "- **Training Continuation**: Loads model weights AND continues training with preserved state\n",
    "- **Key Benefit**: Preserves trained weights instead of resetting to random initialization\n",
    "\n",
    "### üéõÔ∏è Perceptual Loss Control\n",
    "You can now **override the perceptual loss setting** during training continuation:\n",
    "\n",
    "**For VAE Continuation:**\n",
    "- `use_perceptual_loss_continuation = True`: Enable perceptual loss (better visual quality)\n",
    "- `use_perceptual_loss_continuation = False`: Disable perceptual loss (faster training, MSE only)\n",
    "\n",
    "**For FLRNet Continuation:**\n",
    "- `use_perceptual_loss_flrnet_continuation = True`: Enable perceptual loss in VAE component\n",
    "- `use_perceptual_loss_flrnet_continuation = False`: Disable perceptual loss in VAE component\n",
    "\n",
    "### üîÑ Continuation vs New Training\n",
    "| Method | Model Weights | Optimizer State | Use Case |\n",
    "|--------|--------------|----------------|----------|\n",
    "| `train_vae()` | ‚ùå Reset to random | ‚ùå New optimizer | Fresh training |\n",
    "| `continue_vae_training()` | ‚úÖ Preserved | ‚ùå New optimizer | Fine-tuning/Resume |\n",
    "\n",
    "### ‚ö†Ô∏è Important Notes\n",
    "- **Perceptual Loss Override**: When changing perceptual loss settings, the system automatically initializes/removes metric trackers\n",
    "- **Metric Compatibility**: The system ensures metric trackers match the current perceptual loss setting\n",
    "- **Safe Overrides**: You can safely change perceptual loss settings between continuation sessions\n",
    "\n",
    "**Important**: Continuation methods preserve model weights but create fresh optimizers (standard practice for fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aeeab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Pre-Continuation Weight Preservation Verification\n",
    "print(\"üîç === Model Weight Preservation Check ===\")\n",
    "\n",
    "if vae_model is not None:\n",
    "    print(\"Testing if model weights are preserved during continuation setup...\")\n",
    "    \n",
    "    # Get a test sample to establish baseline\n",
    "    test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "    test_inputs, test_targets = test_batch\n",
    "    \n",
    "    if isinstance(test_inputs, tuple):\n",
    "        test_field, test_coord = test_inputs\n",
    "        test_input = [test_field[:1], test_coord[:1]]\n",
    "        input_format = \"Fourier (field + coordinates)\"\n",
    "    else:\n",
    "        test_input = test_inputs[:1]\n",
    "        input_format = \"Standard (field only)\"\n",
    "    \n",
    "    print(f\"üìä Input format: {input_format}\")\n",
    "    \n",
    "    # Test prediction BEFORE any training operations\n",
    "    print(\"\\nüìä BEFORE continuation setup:\")\n",
    "    prediction_before = vae_model.predict(test_input, verbose=0)\n",
    "    mse_before = np.mean((test_targets[:1].numpy() - prediction_before) ** 2)\n",
    "    prediction_range_before = [prediction_before.min(), prediction_before.max()]\n",
    "    \n",
    "    print(f\"   MSE: {mse_before:.6f}\")\n",
    "    print(f\"   Prediction range: [{prediction_range_before[0]:.4f}, {prediction_range_before[1]:.4f}]\")\n",
    "    \n",
    "    # Store some layer weights for comparison\n",
    "    try:\n",
    "        layer_weights_before = []\n",
    "        weight_layer_names = []\n",
    "        for i, layer in enumerate(vae_model.layers):\n",
    "            if hasattr(layer, 'get_weights') and layer.get_weights():\n",
    "                weights = layer.get_weights()\n",
    "                if len(weights) > 0 and weights[0].size > 0:\n",
    "                    layer_weights_before.append(weights[0].copy())\n",
    "                    weight_layer_names.append(f\"Layer_{i}_{layer.name}\")\n",
    "                    if len(layer_weights_before) >= 3:  # Just store first 3 layers with weights\n",
    "                        break\n",
    "        \n",
    "        print(f\"   Stored weights from {len(layer_weights_before)} layers for comparison\")\n",
    "        \n",
    "        # Check if MSE indicates model is working correctly\n",
    "        if mse_before > 1.0:\n",
    "            print(\"‚ùå WARNING: High MSE detected! Model may have been reset or not properly loaded!\")\n",
    "            print(\"   Expected: Low MSE if weights were preserved from checkpoint\")\n",
    "            print(\"   Actual: High MSE suggests random/reset weights\")\n",
    "            print(f\"   Recommendation: Check checkpoint loading process\")\n",
    "        elif mse_before < 0.1:\n",
    "            print(\"‚úÖ EXCELLENT: Very low MSE suggests weights were properly loaded from checkpoint\")\n",
    "        else:\n",
    "            print(\"‚úÖ GOOD: Reasonable MSE suggests weights were loaded correctly\")\n",
    "        \n",
    "        # Store baseline for later comparison\n",
    "        baseline_mse = mse_before\n",
    "        baseline_weights = layer_weights_before\n",
    "        baseline_prediction_range = prediction_range_before\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not extract weights for comparison: {e}\")\n",
    "        baseline_mse = mse_before\n",
    "        baseline_weights = None\n",
    "        baseline_prediction_range = prediction_range_before\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No VAE model available for testing\")\n",
    "    baseline_mse = None\n",
    "    baseline_weights = None\n",
    "    baseline_prediction_range = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7adad25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Continuing VAE training from loaded checkpoint...\n",
      "============================================================\n",
      "üìã Continuation Configuration:\n",
      "   - Additional epochs: 100\n",
      "   - Learning rate: 0.0001 (reduced for fine-tuning)\n",
      "   - Perceptual loss: ‚úÖ ENABLED\n",
      "   - Model weights: ‚úÖ PRESERVED from checkpoint\n",
      "\n",
      "üöÄ Method 1: Using FLRTrainer (Recommended)\n",
      "üîÑ Continuing VAE training from loaded checkpoint...\n",
      "============================================================\n",
      "üìã Current Model Configuration:\n",
      "   - Input shape: (128, 256, 1)\n",
      "   - Latent dims: 8\n",
      "   - Base features: 64\n",
      "   - Use Fourier: True\n",
      "   - Perceptual loss: True (KEPT from checkpoint)\n",
      "\n",
      "üéØ Training Configuration:\n",
      "   - Additional epochs: 100\n",
      "   - Learning rate: 0.0001\n",
      "   - Patience: 100\n",
      "   - Model weights: ‚úÖ PRESERVED from checkpoint\n",
      "üõ°Ô∏è Checkpoint saving will be disabled for epochs 1-30, enabled from epoch 31\n",
      "üõ°Ô∏è Checkpoint saving will be disabled for epochs 1-30, enabled from epoch 31\n",
      "\n",
      "üöÄ Starting training continuation...\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "  6/110 [>.............................] - ETA: 11s - loss: 124674.4440 - reconstruction_loss: 130598.6641 - kl_loss: 884.5735 - perceptual_loss: 126.6582WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0656s vs `on_train_batch_end` time: 0.0822s). Check your callbacks.\n",
      "  6/110 [>.............................] - ETA: 11s - loss: 124674.4440 - reconstruction_loss: 130598.6641 - kl_loss: 884.5735 - perceptual_loss: 126.6582WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0656s vs `on_train_batch_end` time: 0.0822s). Check your callbacks.\n",
      "110/110 [==============================] - ETA: 0s - loss: 79734.7587 - reconstruction_loss: 60611.3086 - kl_loss: 1179.9141 - perceptual_loss: 123.8262Epoch 1: val_reconstruction_loss improved from inf to 18091.86523 (saving disabled for epochs 1-30)\n",
      "Epoch 1: val_reconstruction_loss improved from inf to 18091.86523 (saving disabled for epochs 1-30)\n",
      "Epoch 1: val_reconstruction_loss improved from inf to 18091.86523 (saving disabled for epochs 1-30)\n",
      "Epoch 1: val_reconstruction_loss improved from inf to 18091.86523 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 22s 167ms/step - loss: 79574.2209 - reconstruction_loss: 60611.3086 - kl_loss: 1179.9141 - perceptual_loss: 123.8262 - val_loss: 18686.7773 - val_reconstruction_loss: 18091.8652 - val_kl_loss: 551.5827 - val_perceptual_loss: 43.3296 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 22s 167ms/step - loss: 79574.2209 - reconstruction_loss: 60611.3086 - kl_loss: 1179.9141 - perceptual_loss: 123.8262 - val_loss: 18686.7773 - val_reconstruction_loss: 18091.8652 - val_kl_loss: 551.5827 - val_perceptual_loss: 43.3296 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 42834.4162 - reconstruction_loss: 38960.6484 - kl_loss: 1704.9644 - perceptual_loss: 97.6890Epoch 2: val_reconstruction_loss improved from 18091.86523 to 12146.84375 (saving disabled for epochs 1-30)\n",
      "Epoch 2: val_reconstruction_loss improved from 18091.86523 to 12146.84375 (saving disabled for epochs 1-30)\n",
      "Epoch 2: val_reconstruction_loss improved from 18091.86523 to 12146.84375 (saving disabled for epochs 1-30)\n",
      "Epoch 2: val_reconstruction_loss improved from 18091.86523 to 12146.84375 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 42815.7575 - reconstruction_loss: 38960.6484 - kl_loss: 1704.9644 - perceptual_loss: 97.6890 - val_loss: 12733.8486 - val_reconstruction_loss: 12146.8438 - val_kl_loss: 552.6664 - val_perceptual_loss: 34.3385 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 42815.7575 - reconstruction_loss: 38960.6484 - kl_loss: 1704.9644 - perceptual_loss: 97.6890 - val_loss: 12733.8486 - val_reconstruction_loss: 12146.8438 - val_kl_loss: 552.6664 - val_perceptual_loss: 34.3385 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 34746.3975 - reconstruction_loss: 31778.7754 - kl_loss: 1846.5420 - perceptual_loss: 71.7504Epoch 3: val_reconstruction_loss improved from 12146.84375 to 9659.78125 (saving disabled for epochs 1-30)\n",
      "Epoch 3: val_reconstruction_loss improved from 12146.84375 to 9659.78125 (saving disabled for epochs 1-30)\n",
      "Epoch 3: val_reconstruction_loss improved from 12146.84375 to 9659.78125 (saving disabled for epochs 1-30)\n",
      "Epoch 3: val_reconstruction_loss improved from 12146.84375 to 9659.78125 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 157ms/step - loss: 34736.9442 - reconstruction_loss: 31778.7754 - kl_loss: 1846.5420 - perceptual_loss: 71.7504 - val_loss: 10284.8389 - val_reconstruction_loss: 9659.7812 - val_kl_loss: 600.0358 - val_perceptual_loss: 25.0210 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 17s 157ms/step - loss: 34736.9442 - reconstruction_loss: 31778.7754 - kl_loss: 1846.5420 - perceptual_loss: 71.7504 - val_loss: 10284.8389 - val_reconstruction_loss: 9659.7812 - val_kl_loss: 600.0358 - val_perceptual_loss: 25.0210 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 30477.9262 - reconstruction_loss: 27140.3066 - kl_loss: 1881.5383 - perceptual_loss: 52.1906Epoch 4: val_reconstruction_loss did not improve from 9659.78125 (saving disabled)\n",
      "Epoch 4: val_reconstruction_loss did not improve from 9659.78125 (saving disabled)\n",
      "Epoch 4: val_reconstruction_loss did not improve from 9659.78125 (saving disabled)\n",
      "Epoch 4: val_reconstruction_loss did not improve from 9659.78125 (saving disabled)\n",
      "110/110 [==============================] - 18s 157ms/step - loss: 30465.2785 - reconstruction_loss: 27140.3066 - kl_loss: 1881.5383 - perceptual_loss: 52.1906 - val_loss: 10784.5684 - val_reconstruction_loss: 10083.1748 - val_kl_loss: 682.4892 - val_perceptual_loss: 18.9039 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 157ms/step - loss: 30465.2785 - reconstruction_loss: 27140.3066 - kl_loss: 1881.5383 - perceptual_loss: 52.1906 - val_loss: 10784.5684 - val_reconstruction_loss: 10083.1748 - val_kl_loss: 682.4892 - val_perceptual_loss: 18.9039 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 25433.0807 - reconstruction_loss: 21728.2773 - kl_loss: 2329.3381 - perceptual_loss: 39.8790Epoch 5: val_reconstruction_loss improved from 9659.78125 to 6633.42871 (saving disabled for epochs 1-30)\n",
      "Epoch 5: val_reconstruction_loss improved from 9659.78125 to 6633.42871 (saving disabled for epochs 1-30)\n",
      "Epoch 5: val_reconstruction_loss improved from 9659.78125 to 6633.42871 (saving disabled for epochs 1-30)\n",
      "Epoch 5: val_reconstruction_loss improved from 9659.78125 to 6633.42871 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 155ms/step - loss: 25421.0484 - reconstruction_loss: 21728.2773 - kl_loss: 2329.3381 - perceptual_loss: 39.8790 - val_loss: 7506.5137 - val_reconstruction_loss: 6633.4287 - val_kl_loss: 860.1731 - val_perceptual_loss: 12.9115 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 17s 155ms/step - loss: 25421.0484 - reconstruction_loss: 21728.2773 - kl_loss: 2329.3381 - perceptual_loss: 39.8790 - val_loss: 7506.5137 - val_reconstruction_loss: 6633.4287 - val_kl_loss: 860.1731 - val_perceptual_loss: 12.9115 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 21052.0756 - reconstruction_loss: 18692.4766 - kl_loss: 2493.9312 - perceptual_loss: 29.3873Epoch 6: val_reconstruction_loss did not improve from 6633.42871 (saving disabled)\n",
      "Epoch 6: val_reconstruction_loss did not improve from 6633.42871 (saving disabled)\n",
      "Epoch 6: val_reconstruction_loss did not improve from 6633.42871 (saving disabled)\n",
      "Epoch 6: val_reconstruction_loss did not improve from 6633.42871 (saving disabled)\n",
      "110/110 [==============================] - 17s 156ms/step - loss: 21053.5505 - reconstruction_loss: 18692.4766 - kl_loss: 2493.9312 - perceptual_loss: 29.3873 - val_loss: 10365.7207 - val_reconstruction_loss: 9502.2988 - val_kl_loss: 852.3035 - val_perceptual_loss: 11.1185 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 17s 156ms/step - loss: 21053.5505 - reconstruction_loss: 18692.4766 - kl_loss: 2493.9312 - perceptual_loss: 29.3873 - val_loss: 10365.7207 - val_reconstruction_loss: 9502.2988 - val_kl_loss: 852.3035 - val_perceptual_loss: 11.1185 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 22424.6966 - reconstruction_loss: 19147.2207 - kl_loss: 2252.4954 - perceptual_loss: 25.1180Epoch 7: val_reconstruction_loss improved from 6633.42871 to 6091.03809 (saving disabled for epochs 1-30)\n",
      "Epoch 7: val_reconstruction_loss improved from 6633.42871 to 6091.03809 (saving disabled for epochs 1-30)\n",
      "Epoch 7: val_reconstruction_loss improved from 6633.42871 to 6091.03809 (saving disabled for epochs 1-30)\n",
      "Epoch 7: val_reconstruction_loss improved from 6633.42871 to 6091.03809 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 157ms/step - loss: 22415.6887 - reconstruction_loss: 19147.2207 - kl_loss: 2252.4954 - perceptual_loss: 25.1180 - val_loss: 6847.9468 - val_reconstruction_loss: 6091.0381 - val_kl_loss: 746.7710 - val_perceptual_loss: 10.1378 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 17s 157ms/step - loss: 22415.6887 - reconstruction_loss: 19147.2207 - kl_loss: 2252.4954 - perceptual_loss: 25.1180 - val_loss: 6847.9468 - val_reconstruction_loss: 6091.0381 - val_kl_loss: 746.7710 - val_perceptual_loss: 10.1378 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 18952.4857 - reconstruction_loss: 16720.2266 - kl_loss: 2375.1929 - perceptual_loss: 21.4859Epoch 8: val_reconstruction_loss improved from 6091.03809 to 5074.11816 (saving disabled for epochs 1-30)\n",
      "Epoch 8: val_reconstruction_loss improved from 6091.03809 to 5074.11816 (saving disabled for epochs 1-30)\n",
      "Epoch 8: val_reconstruction_loss improved from 6091.03809 to 5074.11816 (saving disabled for epochs 1-30)\n",
      "Epoch 8: val_reconstruction_loss improved from 6091.03809 to 5074.11816 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 157ms/step - loss: 18953.9669 - reconstruction_loss: 16720.2266 - kl_loss: 2375.1929 - perceptual_loss: 21.4859 - val_loss: 5985.9434 - val_reconstruction_loss: 5074.1182 - val_kl_loss: 903.6982 - val_perceptual_loss: 8.1267 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 17s 157ms/step - loss: 18953.9669 - reconstruction_loss: 16720.2266 - kl_loss: 2375.1929 - perceptual_loss: 21.4859 - val_loss: 5985.9434 - val_reconstruction_loss: 5074.1182 - val_kl_loss: 903.6982 - val_perceptual_loss: 8.1267 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 17785.3034 - reconstruction_loss: 15360.2783 - kl_loss: 2433.8977 - perceptual_loss: 21.0595Epoch 9: val_reconstruction_loss did not improve from 5074.11816 (saving disabled)\n",
      "Epoch 9: val_reconstruction_loss did not improve from 5074.11816 (saving disabled)\n",
      "Epoch 9: val_reconstruction_loss did not improve from 5074.11816 (saving disabled)\n",
      "Epoch 9: val_reconstruction_loss did not improve from 5074.11816 (saving disabled)\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 17785.5731 - reconstruction_loss: 15360.2783 - kl_loss: 2433.8977 - perceptual_loss: 21.0595 - val_loss: 6178.6890 - val_reconstruction_loss: 5401.2646 - val_kl_loss: 767.3610 - val_perceptual_loss: 10.0636 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 17785.5731 - reconstruction_loss: 15360.2783 - kl_loss: 2433.8977 - perceptual_loss: 21.0595 - val_loss: 6178.6890 - val_reconstruction_loss: 5401.2646 - val_kl_loss: 767.3610 - val_perceptual_loss: 10.0636 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 16615.1227 - reconstruction_loss: 13671.3311 - kl_loss: 2252.5916 - perceptual_loss: 16.8998Epoch 10: val_reconstruction_loss improved from 5074.11816 to 4928.56543 (saving disabled for epochs 1-30)\n",
      "Epoch 10: val_reconstruction_loss improved from 5074.11816 to 4928.56543 (saving disabled for epochs 1-30)\n",
      "Epoch 10: val_reconstruction_loss improved from 5074.11816 to 4928.56543 (saving disabled for epochs 1-30)\n",
      "Epoch 10: val_reconstruction_loss improved from 5074.11816 to 4928.56543 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 16609.0480 - reconstruction_loss: 13671.3311 - kl_loss: 2252.5916 - perceptual_loss: 16.8998 - val_loss: 5770.8184 - val_reconstruction_loss: 4928.5654 - val_kl_loss: 835.4959 - val_perceptual_loss: 6.7569 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 16609.0480 - reconstruction_loss: 13671.3311 - kl_loss: 2252.5916 - perceptual_loss: 16.8998 - val_loss: 5770.8184 - val_reconstruction_loss: 4928.5654 - val_kl_loss: 835.4959 - val_perceptual_loss: 6.7569 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 15102.1453 - reconstruction_loss: 13144.5293 - kl_loss: 2273.0779 - perceptual_loss: 15.6186Epoch 11: val_reconstruction_loss improved from 4928.56543 to 4326.01221 (saving disabled for epochs 1-30)\n",
      "Epoch 11: val_reconstruction_loss improved from 4928.56543 to 4326.01221 (saving disabled for epochs 1-30)\n",
      "Epoch 11: val_reconstruction_loss improved from 4928.56543 to 4326.01221 (saving disabled for epochs 1-30)\n",
      "Epoch 11: val_reconstruction_loss improved from 4928.56543 to 4326.01221 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 15105.1281 - reconstruction_loss: 13144.5293 - kl_loss: 2273.0779 - perceptual_loss: 15.6186 - val_loss: 5177.0693 - val_reconstruction_loss: 4326.0122 - val_kl_loss: 844.8766 - val_perceptual_loss: 6.1806 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 15105.1281 - reconstruction_loss: 13144.5293 - kl_loss: 2273.0779 - perceptual_loss: 15.6186 - val_loss: 5177.0693 - val_reconstruction_loss: 4326.0122 - val_kl_loss: 844.8766 - val_perceptual_loss: 6.1806 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 15739.0065 - reconstruction_loss: 13553.8418 - kl_loss: 2315.7839 - perceptual_loss: 15.2942Epoch 12: val_reconstruction_loss improved from 4326.01221 to 4208.07568 (saving disabled for epochs 1-30)\n",
      "Epoch 12: val_reconstruction_loss improved from 4326.01221 to 4208.07568 (saving disabled for epochs 1-30)\n",
      "Epoch 12: val_reconstruction_loss improved from 4326.01221 to 4208.07568 (saving disabled for epochs 1-30)\n",
      "Epoch 12: val_reconstruction_loss improved from 4326.01221 to 4208.07568 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 15740.3209 - reconstruction_loss: 13553.8418 - kl_loss: 2315.7839 - perceptual_loss: 15.2942 - val_loss: 5049.7246 - val_reconstruction_loss: 4208.0757 - val_kl_loss: 835.5286 - val_perceptual_loss: 6.1200 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 15740.3209 - reconstruction_loss: 13553.8418 - kl_loss: 2315.7839 - perceptual_loss: 15.2942 - val_loss: 5049.7246 - val_reconstruction_loss: 4208.0757 - val_kl_loss: 835.5286 - val_perceptual_loss: 6.1200 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 16123.5120 - reconstruction_loss: 13552.3271 - kl_loss: 2255.8513 - perceptual_loss: 17.4918Epoch 13: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 13: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 13: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 13: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 16120.8287 - reconstruction_loss: 13552.3271 - kl_loss: 2255.8513 - perceptual_loss: 17.4918 - val_loss: 5405.7876 - val_reconstruction_loss: 4586.5879 - val_kl_loss: 813.2883 - val_perceptual_loss: 5.9118 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 16120.8287 - reconstruction_loss: 13552.3271 - kl_loss: 2255.8513 - perceptual_loss: 17.4918 - val_loss: 5405.7876 - val_reconstruction_loss: 4586.5879 - val_kl_loss: 813.2883 - val_perceptual_loss: 5.9118 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 14398.5245 - reconstruction_loss: 11915.2021 - kl_loss: 2159.4524 - perceptual_loss: 13.8385Epoch 14: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 14: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 14: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 14: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "110/110 [==============================] - 18s 166ms/step - loss: 14395.7314 - reconstruction_loss: 11915.2021 - kl_loss: 2159.4524 - perceptual_loss: 13.8385 - val_loss: 5001.4219 - val_reconstruction_loss: 4229.8208 - val_kl_loss: 765.9583 - val_perceptual_loss: 5.6424 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 166ms/step - loss: 14395.7314 - reconstruction_loss: 11915.2021 - kl_loss: 2159.4524 - perceptual_loss: 13.8385 - val_loss: 5001.4219 - val_reconstruction_loss: 4229.8208 - val_kl_loss: 765.9583 - val_perceptual_loss: 5.6424 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 14177.2477 - reconstruction_loss: 11939.6045 - kl_loss: 2100.6741 - perceptual_loss: 14.2508Epoch 15: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 15: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 15: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 15: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "110/110 [==============================] - 19s 170ms/step - loss: 14176.1421 - reconstruction_loss: 11939.6045 - kl_loss: 2100.6741 - perceptual_loss: 14.2508 - val_loss: 5137.2070 - val_reconstruction_loss: 4393.9209 - val_kl_loss: 737.4756 - val_perceptual_loss: 5.8107 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 19s 170ms/step - loss: 14176.1421 - reconstruction_loss: 11939.6045 - kl_loss: 2100.6741 - perceptual_loss: 14.2508 - val_loss: 5137.2070 - val_reconstruction_loss: 4393.9209 - val_kl_loss: 737.4756 - val_perceptual_loss: 5.8107 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 15082.5016 - reconstruction_loss: 13062.3291 - kl_loss: 2013.7933 - perceptual_loss: 13.1597Epoch 16: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 16: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 16: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 16: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 15082.5627 - reconstruction_loss: 13062.3291 - kl_loss: 2013.7933 - perceptual_loss: 13.1597 - val_loss: 5173.7471 - val_reconstruction_loss: 4479.1953 - val_kl_loss: 688.9017 - val_perceptual_loss: 5.6501 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 15082.5627 - reconstruction_loss: 13062.3291 - kl_loss: 2013.7933 - perceptual_loss: 13.1597 - val_loss: 5173.7471 - val_reconstruction_loss: 4479.1953 - val_kl_loss: 688.9017 - val_perceptual_loss: 5.6501 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 15121.3067 - reconstruction_loss: 12953.9639 - kl_loss: 1992.4373 - perceptual_loss: 12.9878Epoch 17: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 17: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 17: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 17: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 15119.8480 - reconstruction_loss: 12953.9639 - kl_loss: 1992.4373 - perceptual_loss: 12.9878 - val_loss: 5853.8579 - val_reconstruction_loss: 5087.2822 - val_kl_loss: 761.1558 - val_perceptual_loss: 5.4200 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 15119.8480 - reconstruction_loss: 12953.9639 - kl_loss: 1992.4373 - perceptual_loss: 12.9878 - val_loss: 5853.8579 - val_reconstruction_loss: 5087.2822 - val_kl_loss: 761.1558 - val_perceptual_loss: 5.4200 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 15045.3419 - reconstruction_loss: 12630.7793 - kl_loss: 2047.1096 - perceptual_loss: 12.7520Epoch 18: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 18: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 18: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 18: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 15042.1463 - reconstruction_loss: 12630.7793 - kl_loss: 2047.1096 - perceptual_loss: 12.7520 - val_loss: 5892.3696 - val_reconstruction_loss: 5129.5059 - val_kl_loss: 757.8776 - val_perceptual_loss: 4.9864 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 15042.1463 - reconstruction_loss: 12630.7793 - kl_loss: 2047.1096 - perceptual_loss: 12.7520 - val_loss: 5892.3696 - val_reconstruction_loss: 5129.5059 - val_kl_loss: 757.8776 - val_perceptual_loss: 4.9864 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 14302.5302 - reconstruction_loss: 11879.3770 - kl_loss: 2007.2250 - perceptual_loss: 12.4273Epoch 19: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 19: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 19: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 19: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 14298.8951 - reconstruction_loss: 11879.3770 - kl_loss: 2007.2250 - perceptual_loss: 12.4273 - val_loss: 5433.5552 - val_reconstruction_loss: 4717.9390 - val_kl_loss: 710.4239 - val_perceptual_loss: 5.1925 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 14298.8951 - reconstruction_loss: 11879.3770 - kl_loss: 2007.2250 - perceptual_loss: 12.4273 - val_loss: 5433.5552 - val_reconstruction_loss: 4717.9390 - val_kl_loss: 710.4239 - val_perceptual_loss: 5.1925 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 12983.1162 - reconstruction_loss: 10665.9512 - kl_loss: 2030.5055 - perceptual_loss: 11.9551Epoch 20: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 20: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 20: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 20: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 12980.6414 - reconstruction_loss: 10665.9512 - kl_loss: 2030.5055 - perceptual_loss: 11.9551 - val_loss: 5082.6016 - val_reconstruction_loss: 4374.1758 - val_kl_loss: 703.4700 - val_perceptual_loss: 4.9557 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 12980.6414 - reconstruction_loss: 10665.9512 - kl_loss: 2030.5055 - perceptual_loss: 11.9551 - val_loss: 5082.6016 - val_reconstruction_loss: 4374.1758 - val_kl_loss: 703.4700 - val_perceptual_loss: 4.9557 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 13310.5396 - reconstruction_loss: 10571.7021 - kl_loss: 2006.9835 - perceptual_loss: 11.6840Epoch 21: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 21: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 21: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "Epoch 21: val_reconstruction_loss did not improve from 4208.07568 (saving disabled)\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 13304.0516 - reconstruction_loss: 10571.7021 - kl_loss: 2006.9835 - perceptual_loss: 11.6840 - val_loss: 5196.0522 - val_reconstruction_loss: 4451.0635 - val_kl_loss: 740.3430 - val_perceptual_loss: 4.6458 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 13304.0516 - reconstruction_loss: 10571.7021 - kl_loss: 2006.9835 - perceptual_loss: 11.6840 - val_loss: 5196.0522 - val_reconstruction_loss: 4451.0635 - val_kl_loss: 740.3430 - val_perceptual_loss: 4.6458 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 12409.9732 - reconstruction_loss: 10274.2334 - kl_loss: 1993.0103 - perceptual_loss: 11.2664Epoch 22: val_reconstruction_loss improved from 4208.07568 to 3559.18262 (saving disabled for epochs 1-30)\n",
      "Epoch 22: val_reconstruction_loss improved from 4208.07568 to 3559.18262 (saving disabled for epochs 1-30)\n",
      "Epoch 22: val_reconstruction_loss improved from 4208.07568 to 3559.18262 (saving disabled for epochs 1-30)\n",
      "Epoch 22: val_reconstruction_loss improved from 4208.07568 to 3559.18262 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 12408.7888 - reconstruction_loss: 10274.2334 - kl_loss: 1993.0103 - perceptual_loss: 11.2664 - val_loss: 4261.8140 - val_reconstruction_loss: 3559.1826 - val_kl_loss: 698.1898 - val_perceptual_loss: 4.4413 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 12408.7888 - reconstruction_loss: 10274.2334 - kl_loss: 1993.0103 - perceptual_loss: 11.2664 - val_loss: 4261.8140 - val_reconstruction_loss: 3559.1826 - val_kl_loss: 698.1898 - val_perceptual_loss: 4.4413 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 11427.4173 - reconstruction_loss: 9578.8291 - kl_loss: 2076.8257 - perceptual_loss: 10.7861Epoch 23: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "Epoch 23: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "Epoch 23: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "Epoch 23: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 11429.5707 - reconstruction_loss: 9578.8291 - kl_loss: 2076.8257 - perceptual_loss: 10.7861 - val_loss: 4409.2192 - val_reconstruction_loss: 3662.3018 - val_kl_loss: 742.6389 - val_perceptual_loss: 4.2786 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 11429.5707 - reconstruction_loss: 9578.8291 - kl_loss: 2076.8257 - perceptual_loss: 10.7861 - val_loss: 4409.2192 - val_reconstruction_loss: 3662.3018 - val_kl_loss: 742.6389 - val_perceptual_loss: 4.2786 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 11600.0558 - reconstruction_loss: 9428.8564 - kl_loss: 2075.7356 - perceptual_loss: 10.5183Epoch 24: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "Epoch 24: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "Epoch 24: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "Epoch 24: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 11599.2905 - reconstruction_loss: 9428.8564 - kl_loss: 2075.7356 - perceptual_loss: 10.5183 - val_loss: 4393.9736 - val_reconstruction_loss: 3591.9932 - val_kl_loss: 797.9141 - val_perceptual_loss: 4.0664 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 11599.2905 - reconstruction_loss: 9428.8564 - kl_loss: 2075.7356 - perceptual_loss: 10.5183 - val_loss: 4393.9736 - val_reconstruction_loss: 3591.9932 - val_kl_loss: 797.9141 - val_perceptual_loss: 4.0664 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 12262.6160 - reconstruction_loss: 9879.5742 - kl_loss: 2070.6899 - perceptual_loss: 11.5136Epoch 25: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "Epoch 25: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "Epoch 25: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "Epoch 25: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 12259.9057 - reconstruction_loss: 9879.5742 - kl_loss: 2070.6899 - perceptual_loss: 11.5136 - val_loss: 4405.8701 - val_reconstruction_loss: 3608.1123 - val_kl_loss: 793.8831 - val_perceptual_loss: 3.8744 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 12259.9057 - reconstruction_loss: 9879.5742 - kl_loss: 2070.6899 - perceptual_loss: 11.5136 - val_loss: 4405.8701 - val_reconstruction_loss: 3608.1123 - val_kl_loss: 793.8831 - val_perceptual_loss: 3.8744 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 11836.9906 - reconstruction_loss: 9639.5293 - kl_loss: 2098.5557 - perceptual_loss: 10.3887Epoch 26: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "Epoch 26: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "Epoch 26: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n",
      "Epoch 26: val_reconstruction_loss did not improve from 3559.18262 (saving disabled)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mvae_model \u001b[38;5;241m=\u001b[39m vae_model\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Continue training with reduced learning rate\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     continued_vae \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinue_vae_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae_train_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae_test_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduce_lr_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreduce_lr_patience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_perceptual_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_perceptual_loss_continuation\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# New option for perceptual loss\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ VAE training continuation completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m continue_vae_training \u001b[38;5;129;01mand\u001b[39;00m vae_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Research\\Physics-informed-machine-learning\\flow_field_recon_parc\\models_improved.py:1243\u001b[0m, in \u001b[0;36mFLRTrainer.continue_vae_training\u001b[1;34m(self, train_dataset, val_dataset, epochs, learning_rate, patience, reduce_lr_patience, use_perceptual_loss, **kwargs)\u001b[0m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;66;03m# Continue training\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müöÄ Starting training continuation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1243\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;66;03m# Save final model using weights if save_last_model is enabled\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_last_model:\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1435\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1432\u001b[0m   val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   1433\u001b[0m   epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[1;32m-> 1435\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1436\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:416\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    414\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_logs(logs)\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m--> 416\u001b[0m   \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:2506\u001b[0m, in \u001b[0;36mTensorBoard.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   2503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_epoch_metrics(epoch, logs)\n\u001b[0;32m   2505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistogram_freq \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistogram_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2506\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_freq \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2509\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_embeddings(epoch)\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:2577\u001b[0m, in \u001b[0;36mTensorBoard._log_weights\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m   2575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_images:\n\u001b[0;32m   2576\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_weight_as_image(weight, weight_name, epoch)\n\u001b[1;32m-> 2577\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:345\u001b[0m, in \u001b[0;36m_ResourceSummaryWriter.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 345\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_summary_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_summary_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_summary_ops.py:193\u001b[0m, in \u001b[0;36mflush_summary_writer\u001b[1;34m(writer, name)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    192\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFlushSummaryWriter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    196\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Continue VAE Training from Checkpoint\n",
    "# Set this to True if you want to continue training from the loaded checkpoint\n",
    "continue_vae_training = True  # Change to True to enable\n",
    "\n",
    "# Toggle for perceptual loss during continuation (can be different from original training)\n",
    "use_perceptual_loss_continuation = True  # Change to False to disable perceptual loss\n",
    "\n",
    "if continue_vae_training and vae_model is not None:\n",
    "    print(\"üîÑ Continuing VAE training from loaded checkpoint...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration for continued training\n",
    "    additional_epochs = 100\n",
    "    learning_rate = 1e-4  # Lower learning rate for fine-tuning\n",
    "    \n",
    "    print(f\"üìã Continuation Configuration:\")\n",
    "    print(f\"   - Additional epochs: {additional_epochs}\")\n",
    "    print(f\"   - Learning rate: {learning_rate} (reduced for fine-tuning)\")\n",
    "    print(f\"   - Perceptual loss: {'‚úÖ ENABLED' if use_perceptual_loss_continuation else '‚ùå DISABLED'}\")\n",
    "    print(f\"   - Model weights: ‚úÖ PRESERVED from checkpoint\")\n",
    "    \n",
    "    # Option 1: Continue training using the trainer (recommended)\n",
    "    # The trainer will handle optimizer state and callbacks properly\n",
    "    print(\"\\nüöÄ Method 1: Using FLRTrainer (Recommended)\")\n",
    "    \n",
    "    # Set the loaded model in the trainer\n",
    "    trainer.vae_model = vae_model\n",
    "    \n",
    "    # Continue training with reduced learning rate\n",
    "    continued_vae = trainer.continue_vae_training(\n",
    "        train_dataset=vae_train_dataset,\n",
    "        val_dataset=vae_test_dataset,\n",
    "        epochs=additional_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        patience=config['patience'],\n",
    "        reduce_lr_patience=config['reduce_lr_patience'],\n",
    "        use_perceptual_loss=use_perceptual_loss_continuation  # New option for perceptual loss\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ VAE training continuation completed!\")\n",
    "    \n",
    "elif continue_vae_training and vae_model is None:\n",
    "    print(\"‚ùå Cannot continue VAE training: No model loaded from checkpoint\")\n",
    "    print(\"   First load a model using the checkpoint loading cell above\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  VAE training continuation is disabled\")\n",
    "    print(\"   Set continue_vae_training = True to enable\")\n",
    "    if not continue_vae_training:\n",
    "        print(f\"   Current perceptual loss setting: {'‚úÖ ENABLED' if use_perceptual_loss_continuation else '‚ùå DISABLED'}\")\n",
    "        print(\"   (can be changed via use_perceptual_loss_continuation variable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Post-Continuation Weight Verification\n",
    "print(\"üîç === Post-Continuation Weight Verification ===\")\n",
    "\n",
    "if 'baseline_mse' in locals() and baseline_mse is not None and trainer.vae_model is not None:\n",
    "    print(\"Verifying that model weights were properly preserved during training continuation...\")\n",
    "    \n",
    "    # Get the same test sample used for baseline\n",
    "    test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "    test_inputs, test_targets = test_batch\n",
    "    \n",
    "    if isinstance(test_inputs, tuple):\n",
    "        test_field, test_coord = test_inputs\n",
    "        test_input = [test_field[:1], test_coord[:1]]\n",
    "    else:\n",
    "        test_input = test_inputs[:1]\n",
    "    \n",
    "    # Test prediction AFTER continuation\n",
    "    print(\"\\nüìä AFTER continuation:\")\n",
    "    prediction_after = trainer.vae_model.predict(test_input, verbose=0)\n",
    "    mse_after = np.mean((test_targets[:1].numpy() - prediction_after) ** 2)\n",
    "    prediction_range_after = [prediction_after.min(), prediction_after.max()]\n",
    "    \n",
    "    print(f\"   MSE: {mse_after:.6f}\")\n",
    "    print(f\"   Prediction range: [{prediction_range_after[0]:.4f}, {prediction_range_after[1]:.4f}]\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    print(f\"\\nüìà Comparison with baseline:\")\n",
    "    print(f\"   Baseline MSE: {baseline_mse:.6f}\")\n",
    "    print(f\"   After MSE: {mse_after:.6f}\")\n",
    "    print(f\"   MSE change: {mse_after - baseline_mse:.6f}\")\n",
    "    \n",
    "    # Analyze results\n",
    "    if mse_after < baseline_mse:\n",
    "        improvement = ((baseline_mse - mse_after) / baseline_mse) * 100\n",
    "        print(f\"‚úÖ EXCELLENT: Model improved by {improvement:.2f}%!\")\n",
    "        print(\"   Training continuation was successful and preserved weights correctly\")\n",
    "    elif abs(mse_after - baseline_mse) < 0.01:\n",
    "        print(\"‚úÖ GOOD: Model performance maintained (minimal change)\")\n",
    "        print(\"   Weights were preserved correctly during continuation\")\n",
    "    elif mse_after > baseline_mse * 2:\n",
    "        print(\"‚ùå WARNING: Model performance degraded significantly!\")\n",
    "        print(\"   This may indicate:\")\n",
    "        print(\"   - Learning rate too high\")\n",
    "        print(\"   - Training instability\")\n",
    "        print(\"   - Possible weight corruption\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  Model performance changed moderately\")\n",
    "        print(\"   This is normal for continued training\")\n",
    "    \n",
    "    # Check for weight consistency if we stored baseline weights\n",
    "    if 'baseline_weights' in locals() and baseline_weights is not None:\n",
    "        try:\n",
    "            print(f\"\\nüîç Direct weight comparison:\")\n",
    "            current_weights = []\n",
    "            for i, layer in enumerate(trainer.vae_model.layers):\n",
    "                if hasattr(layer, 'get_weights') and layer.get_weights():\n",
    "                    weights = layer.get_weights()\n",
    "                    if len(weights) > 0 and weights[0].size > 0:\n",
    "                        current_weights.append(weights[0].copy())\n",
    "                        if len(current_weights) >= len(baseline_weights):\n",
    "                            break\n",
    "            \n",
    "            weights_changed = False\n",
    "            for i, (baseline_w, current_w) in enumerate(zip(baseline_weights, current_weights)):\n",
    "                if baseline_w.shape == current_w.shape:\n",
    "                    weight_diff = np.mean(np.abs(baseline_w - current_w))\n",
    "                    print(f\"   Layer {i} weight change: {weight_diff:.6f}\")\n",
    "                    if weight_diff > 0.001:  # Weights should change during training\n",
    "                        weights_changed = True\n",
    "            \n",
    "            if weights_changed:\n",
    "                print(\"‚úÖ CONFIRMED: Weights changed during training (expected)\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  WARNING: Weights didn't change much - training may not be effective\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not compare weights directly: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéØ FINAL ASSESSMENT:\")\n",
    "    if mse_after <= baseline_mse + 0.01:\n",
    "        print(\"‚úÖ Training continuation was SUCCESSFUL\")\n",
    "        print(\"   - Weights were preserved from checkpoint\")\n",
    "        print(\"   - Training improved or maintained model performance\")\n",
    "        print(\"   - Model is ready for inference or further training\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Training continuation had MIXED RESULTS\")\n",
    "        print(\"   - Weights were preserved, but performance may have degraded\")\n",
    "        print(\"   - Consider adjusting learning rate or training parameters\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot perform verification - baseline not available\")\n",
    "    print(\"Run the weight preservation check cell before training continuation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6048e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue FLRNet Training from Checkpoint\n",
    "# Set this to True if you want to continue FLRNet training from checkpoint\n",
    "continue_flrnet_training = False  # Change to True to enable\n",
    "\n",
    "if continue_flrnet_training:\n",
    "    print(\"üîÑ Continuing FLRNet training from checkpoint...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # First, ensure we have a VAE model (required for FLRNet)\n",
    "    if vae_model is None:\n",
    "        print(\"üîÑ Loading VAE model first (required for FLRNet)...\")\n",
    "        vae_model = trainer.load_vae_from_checkpoint(\n",
    "            checkpoint_dir=checkpoint_directory,\n",
    "            latent_dims=config['latent_dims'],\n",
    "            n_base_features=config['n_base_features'],\n",
    "            use_perceptual_loss=config['use_perceptual_loss'],\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    if vae_model is not None:\n",
    "        # Try to load existing FLRNet model\n",
    "        print(\"üîÑ Loading FLRNet model from checkpoint...\")\n",
    "        flrnet_model = trainer.load_flrnet_from_checkpoint(\n",
    "            n_sensors=config['n_sensors'],\n",
    "            checkpoint_dir=checkpoint_directory,\n",
    "            pretrained_vae=vae_model,\n",
    "            latent_dims=config['latent_dims'],\n",
    "            n_base_features=config['n_base_features'],\n",
    "            use_perceptual_loss=config['use_perceptual_loss'],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        if flrnet_model is not None:\n",
    "            # Configure training parameters for continuation\n",
    "            additional_epochs = 100  # How many more epochs to train\n",
    "            learning_rate = 1e-6     # Very low learning rate for fine-tuning\n",
    "            \n",
    "            print(f\"\\nüìã FLRNet Training Configuration:\")\n",
    "            print(f\"   - Additional epochs: {additional_epochs}\")\n",
    "            print(f\"   - Learning rate: {learning_rate}\")\n",
    "            print(f\"   - VAE model: ‚úÖ Loaded\")\n",
    "            print(f\"   - FLRNet model: ‚úÖ Loaded\")\n",
    "            \n",
    "            # Set the loaded model in the trainer\n",
    "            trainer.flr_model = flrnet_model\n",
    "            trainer.vae_model = vae_model\n",
    "            \n",
    "            # Continue training with reduced learning rate\n",
    "            continued_flrnet = trainer.train_flr_net(\n",
    "                train_dataset=flrnet_train_dataset,\n",
    "                val_dataset=flrnet_test_dataset,\n",
    "                n_sensors=config['n_sensors'],\n",
    "                epochs=additional_epochs,\n",
    "                learning_rate=learning_rate,\n",
    "                pretrained_vae=vae_model,\n",
    "                latent_dims=config['latent_dims'],\n",
    "                n_base_features=config['n_base_features'],\n",
    "                use_perceptual_loss=config['use_perceptual_loss'],\n",
    "                patience=config['patience'],\n",
    "                reduce_lr_patience=config['reduce_lr_patience']\n",
    "            )\n",
    "            \n",
    "            print(\"‚úÖ FLRNet training continuation completed!\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå Could not load FLRNet model from checkpoint\")\n",
    "            print(\"You may need to train FLRNet from scratch first\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not load VAE model - required for FLRNet training\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  FLRNet training continuation is disabled\")\n",
    "    print(\"Set continue_flrnet_training = True to enable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2328cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Manual Checkpoint Restoration with Optimizer State\n",
    "# This method provides more control over the restoration process\n",
    "use_advanced_restoration = False  # Change to True to enable\n",
    "\n",
    "if use_advanced_restoration:\n",
    "    print(\"üîß Advanced Checkpoint Restoration...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Configuration for restoration\n",
    "    restore_epoch = 50  # The epoch to restore from (if known)\n",
    "    target_total_epochs = 150  # Total epochs you want to reach\n",
    "    \n",
    "    print(f\"üìã Advanced Restoration Configuration:\")\n",
    "    print(f\"   - Restore from epoch: {restore_epoch}\")\n",
    "    print(f\"   - Target total epochs: {target_total_epochs}\")\n",
    "    print(f\"   - Additional epochs: {target_total_epochs - restore_epoch}\")\n",
    "    \n",
    "    # Method 1: TensorFlow's built-in checkpoint manager\n",
    "    def setup_checkpoint_manager(model, optimizer, checkpoint_dir):\n",
    "        \"\"\"Set up TensorFlow checkpoint manager for proper state restoration.\"\"\"\n",
    "        \n",
    "        # Create checkpoint object\n",
    "        checkpoint = tf.train.Checkpoint(\n",
    "            optimizer=optimizer,\n",
    "            model=model,\n",
    "            epoch=tf.Variable(0, dtype=tf.int64)\n",
    "        )\n",
    "        \n",
    "        # Create checkpoint manager\n",
    "        manager = tf.train.CheckpointManager(\n",
    "            checkpoint,\n",
    "            directory=str(checkpoint_dir / \"tf_checkpoints\"),\n",
    "            max_to_keep=3\n",
    "        )\n",
    "        \n",
    "        return checkpoint, manager\n",
    "    \n",
    "    # Example for VAE restoration\n",
    "    if vae_model is not None:\n",
    "        print(\"\\nüîß Setting up VAE checkpoint restoration...\")\n",
    "        \n",
    "        # Create optimizer (same as training)\n",
    "        vae_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=1e-5,  # Reduced for fine-tuning\n",
    "            beta_1=0.9, \n",
    "            beta_2=0.999\n",
    "        )\n",
    "        \n",
    "        # Compile model with optimizer\n",
    "        vae_model.compile(optimizer=vae_optimizer)\n",
    "        \n",
    "        # Set up checkpoint manager\n",
    "        vae_checkpoint, vae_manager = setup_checkpoint_manager(\n",
    "            vae_model, vae_optimizer, checkpoint_directory\n",
    "        )\n",
    "        \n",
    "        # Try to restore latest checkpoint\n",
    "        if vae_manager.latest_checkpoint:\n",
    "            vae_checkpoint.restore(vae_manager.latest_checkpoint)\n",
    "            restored_epoch = int(vae_checkpoint.epoch.numpy())\n",
    "            print(f\"‚úÖ Restored VAE from epoch {restored_epoch}\")\n",
    "            print(f\"üéØ Will continue training from epoch {restored_epoch + 1}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No TensorFlow checkpoint found, using loaded weights\")\n",
    "            restored_epoch = 0\n",
    "        \n",
    "        # Calculate remaining epochs\n",
    "        remaining_epochs = max(0, target_total_epochs - restored_epoch)\n",
    "        \n",
    "        if remaining_epochs > 0:\n",
    "            print(f\"\\nüöÄ Continuing VAE training for {remaining_epochs} more epochs...\")\n",
    "            \n",
    "            # Custom training loop with checkpoint saving\n",
    "            for epoch in range(remaining_epochs):\n",
    "                current_epoch = restored_epoch + epoch + 1\n",
    "                print(f\"\\nEpoch {current_epoch}/{target_total_epochs}\")\n",
    "                \n",
    "                # Train for one epoch\n",
    "                history = vae_model.fit(\n",
    "                    vae_train_dataset,\n",
    "                    validation_data=vae_test_dataset,\n",
    "                    epochs=1,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Update epoch counter\n",
    "                vae_checkpoint.epoch.assign(current_epoch)\n",
    "                \n",
    "                # Save checkpoint every 10 epochs\n",
    "                if current_epoch % 10 == 0:\n",
    "                    save_path = vae_manager.save()\n",
    "                    print(f\"üíæ Saved checkpoint: {save_path}\")\n",
    "                \n",
    "                # Early stopping logic (optional)\n",
    "                val_loss = history.history.get('val_loss', [0])[-1]\n",
    "                if val_loss < 0.001:  # Example threshold\n",
    "                    print(f\"üéØ Early stopping - validation loss {val_loss:.6f} is below threshold\")\n",
    "                    break\n",
    "            \n",
    "            print(\"‚úÖ Advanced VAE training continuation completed!\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è  Target epochs already reached\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No VAE model loaded for advanced restoration\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Advanced checkpoint restoration is disabled\")\n",
    "    print(\"Set use_advanced_restoration = True to enable\")\n",
    "    print(\"This method provides:\")\n",
    "    print(\"   ‚Ä¢ True optimizer state restoration\")\n",
    "    print(\"   ‚Ä¢ Exact epoch continuation\")\n",
    "    print(\"   ‚Ä¢ Custom training loop control\")\n",
    "    print(\"   ‚Ä¢ Proper checkpoint management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8935323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Load both VAE and FLRNet models at once using trainer\n",
    "# Uncomment this section if you want to load both models together\n",
    "\n",
    "# print(\"\\nüîÑ Alternative: Loading both VAE and FLRNet models...\")\n",
    "# vae_model, flrnet_model = trainer.load_models_from_checkpoint(\n",
    "#     n_sensors=config['n_sensors'],\n",
    "#     checkpoint_dir=checkpoint_directory,\n",
    "#     latent_dims=config['latent_dims'],\n",
    "#     n_base_features=config['n_base_features'],\n",
    "#     use_perceptual_loss=config['use_perceptual_loss'],\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# if vae_model is not None and flrnet_model is not None:\n",
    "#     print(\"üéâ Both models loaded successfully!\")\n",
    "#     print(\"üìà Models are ready for inference and visualization\")\n",
    "# elif vae_model is not None:\n",
    "#     print(\"‚ö†Ô∏è  Only VAE model loaded successfully\")\n",
    "# elif flrnet_model is not None:\n",
    "#     print(\"‚ö†Ô∏è  Only FLRNet model loaded successfully\") \n",
    "# else:\n",
    "#     print(\"‚ùå Failed to load both models\")\n",
    "\n",
    "# Example: Load FLRNet separately (uncomment if needed)\n",
    "# print(\"\\nüîÑ Loading FLRNet model separately...\")\n",
    "# flrnet_model = trainer.load_flrnet_from_checkpoint(\n",
    "#     n_sensors=config['n_sensors'],\n",
    "#     checkpoint_dir=checkpoint_directory,\n",
    "#     pretrained_vae=vae_model,  # Use the VAE we just loaded\n",
    "#     latent_dims=config['latent_dims'],\n",
    "#     n_base_features=config['n_base_features'],\n",
    "#     use_perceptual_loss=config['use_perceptual_loss'],\n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16712b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced VAE Model Inference and Visualization\n",
    "if vae_model is not None:\n",
    "    print(\"üé® Performing enhanced VAE inference and visualization...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get multiple test samples for comprehensive evaluation\n",
    "    test_samples = 4\n",
    "    test_batch = next(iter(vae_test_dataset.batch(test_samples).take(1)))\n",
    "    test_inputs, test_targets = test_batch\n",
    "    \n",
    "    # Handle different input formats (Fourier vs Standard VAE)\n",
    "    if isinstance(test_inputs, (list, tuple)) and len(test_inputs) == 2:\n",
    "        # Fourier VAE expects [field, coordinates]\n",
    "        test_fields, test_coords = test_inputs\n",
    "        predictions = vae_model([test_fields, test_coords])\n",
    "        print(f\"üåä Using Fourier VAE with coordinate inputs\")\n",
    "        print(f\"   - Field input shape: {test_fields.shape}\")\n",
    "        print(f\"   - Coordinate input shape: {test_coords.shape}\")\n",
    "    else:\n",
    "        # Standard VAE expects just field data\n",
    "        predictions = vae_model(test_inputs)\n",
    "        test_fields = test_inputs\n",
    "        print(f\"üîÑ Using Standard VAE\")\n",
    "        print(f\"   - Input shape: {test_inputs.shape}\")\n",
    "    \n",
    "    print(f\"   - Output shape: {predictions.shape}\")\n",
    "    \n",
    "    # Convert to numpy for analysis\n",
    "    test_targets_np = test_targets.numpy()\n",
    "    predictions_np = predictions.numpy()\n",
    "    test_fields_np = test_fields.numpy()\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    mse_per_sample = np.mean((test_targets_np - predictions_np)**2, axis=(1,2,3))\n",
    "    mae_per_sample = np.mean(np.abs(test_targets_np - predictions_np), axis=(1,2,3))\n",
    "    \n",
    "    overall_mse = np.mean(mse_per_sample)\n",
    "    overall_mae = np.mean(mae_per_sample)\n",
    "    \n",
    "    print(f\"üìä Model Performance Metrics:\")\n",
    "    print(f\"   - Overall MSE: {overall_mse:.6f}\")\n",
    "    print(f\"   - Overall MAE: {overall_mae:.6f}\")\n",
    "    print(f\"   - Best sample MSE: {np.min(mse_per_sample):.6f}\")\n",
    "    print(f\"   - Worst sample MSE: {np.max(mse_per_sample):.6f}\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(3, test_samples, figsize=(4*test_samples, 12))\n",
    "    \n",
    "    for i in range(test_samples):\n",
    "        # Original field\n",
    "        im1 = axes[0, i].imshow(test_targets_np[i, :, :, 0], cmap='RdBu_r', aspect='equal', origin='lower')\n",
    "        axes[0, i].set_title(f'Original Field {i+1}', fontsize=12)\n",
    "        axes[0, i].axis('off')\n",
    "        plt.colorbar(im1, ax=axes[0, i], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Reconstructed field\n",
    "        im2 = axes[1, i].imshow(predictions_np[i, :, :, 0], cmap='RdBu_r', aspect='equal', origin='lower')\n",
    "        axes[1, i].set_title(f'Reconstructed Field {i+1}', fontsize=12)\n",
    "        axes[1, i].axis('off')\n",
    "        plt.colorbar(im2, ax=axes[1, i], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Error field\n",
    "        error = np.abs(test_targets_np[i, :, :, 0] - predictions_np[i, :, :, 0])\n",
    "        im3 = axes[2, i].imshow(error, cmap='hot', aspect='equal', origin='lower')\n",
    "        axes[2, i].set_title(f'Error Field {i+1}\\nMSE: {mse_per_sample[i]:.4f}', fontsize=12)\n",
    "        axes[2, i].axis('off')\n",
    "        plt.colorbar(im3, ax=axes[2, i], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Add row labels\n",
    "    axes[0, 0].text(-0.2, 0.5, 'Original', transform=axes[0, 0].transAxes, \n",
    "                    rotation=90, va='center', ha='center', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].text(-0.2, 0.5, 'Reconstructed', transform=axes[1, 0].transAxes, \n",
    "                    rotation=90, va='center', ha='center', fontsize=14, fontweight='bold')\n",
    "    axes[2, 0].text(-0.2, 0.5, 'Error', transform=axes[2, 0].transAxes, \n",
    "                    rotation=90, va='center', ha='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'VAE Model Performance - {test_samples} Test Samples\\nOverall MSE: {overall_mse:.6f}, MAE: {overall_mae:.6f}', \n",
    "                 fontsize=16, y=0.98)\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(f\"\\nüìà Statistical Analysis:\")\n",
    "    print(f\"   - Target field range: [{test_targets_np.min():.4f}, {test_targets_np.max():.4f}]\")\n",
    "    print(f\"   - Prediction range: [{predictions_np.min():.4f}, {predictions_np.max():.4f}]\")\n",
    "    print(f\"   - Prediction std: {predictions_np.std():.4f}\")\n",
    "    print(f\"   - Target std: {test_targets_np.std():.4f}\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    correlation = np.corrcoef(test_targets_np.flatten(), predictions_np.flatten())[0, 1]\n",
    "    print(f\"   - Correlation coefficient: {correlation:.4f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ VAE model evaluation complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot perform inference - VAE model not loaded\")\n",
    "    print(\"Please run the model loading cell above first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05ad50",
   "metadata": {},
   "source": [
    "## Workflow Summary\n",
    "\n",
    "This notebook has successfully completed a comprehensive flow field reconstruction training and validation workflow:\n",
    "\n",
    "### ‚úÖ Completed Tasks:\n",
    "\n",
    "1. **Configuration Management**: Used `ConfigManager` for robust configuration handling\n",
    "2. **Dataset Loading**: Created coordinate-aware datasets for Fourier features\n",
    "3. **VAE Training**: Trained VAE with proper coordinate input for Fourier features\n",
    "4. **VAE Validation**: Comprehensive testing and visualization of VAE performance\n",
    "5. **FLRNet Training**: Trained sensor-to-field reconstruction model\n",
    "6. **FLRNet Validation**: Evaluated FLRNet performance with sensor position overlays\n",
    "7. **Model Saving**: Saved trained models for future use\n",
    "\n",
    "### üîß Key Fixes Applied:\n",
    "\n",
    "- **Fourier Bug Fix**: Resolved coordinate input issues for Fourier-aware VAE\n",
    "- **Dataset Coordinate Integration**: Added proper coordinate grid generation\n",
    "- **Robust Error Handling**: Added validation and error checking throughout\n",
    "- **Comprehensive Visualization**: Created detailed analysis plots and metrics\n",
    "\n",
    "### üìä Results:\n",
    "\n",
    "The notebook now provides a complete, debugged workflow for training and validating both VAE and FLRNet models with optional Fourier features. All models are properly coordinate-aware and ready for physics-informed machine learning applications.\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "- Models are saved and ready for inference\n",
    "- Configuration can be easily changed for different sensor layouts\n",
    "- Fourier features are properly integrated and tested\n",
    "- Framework is extensible for additional physics-informed constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
