{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89d8f71",
   "metadata": {},
   "source": [
    "## üîß Windows Checkpoint File Lock Troubleshooting\n",
    "\n",
    "If you encounter \"Access is denied\" errors during checkpoint saving, this is a known Windows-specific issue with TensorFlow. Here are the solutions implemented:\n",
    "\n",
    "### ‚úÖ Current Solutions Applied:\n",
    "\n",
    "1. **Delayed Checkpoint Saving**: Checkpoint saving is now disabled for the first 30 epochs and only starts from epoch 31 onward\n",
    "2. **Unique Checkpoint Directories**: Using UUID-based unique directories to avoid conflicts\n",
    "3. **Error Handling**: Added try-catch blocks around checkpoint operations\n",
    "4. **Alternative Backup**: Models are saved after training completes\n",
    "\n",
    "### üõ†Ô∏è If Issues Persist:\n",
    "\n",
    "1. **Completely Disable Checkpoints**: Set `disable_checkpoints_entirely = True` in the cell above\n",
    "2. **Manual Cleanup**: Delete the entire checkpoint directory before training\n",
    "3. **Use Different Drive**: Try running on a different drive (e.g., C: instead of E:)\n",
    "4. **Run as Administrator**: Run Jupyter/VS Code as administrator\n",
    "\n",
    "### üìã Understanding the Error:\n",
    "\n",
    "The \"Access is denied\" error occurs when:\n",
    "- TensorFlow tries to rename temporary checkpoint files\n",
    "- Another process has the file locked\n",
    "- Windows file system permissions prevent the operation\n",
    "- Previous training runs left orphaned file handles\n",
    "\n",
    "The conditional checkpoint approach bypasses this issue by avoiding file operations during the sensitive early training phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957488d1",
   "metadata": {},
   "source": [
    "# Flow Field Reconstruction with 8 Edge Sensors\n",
    "\n",
    "This notebook implements a physics-informed machine learning approach to reconstruct flow fields using data from 8 edge sensors. The implementation uses:\n",
    "- Variational Autoencoder (VAE) for dimensionality reduction and flow field reconstruction\n",
    "- Fourier feature embeddings for coordinate information\n",
    "- FLRNet architecture to predict flow fields from sparse sensor measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ded4a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll import the necessary libraries for data manipulation, visualization, and deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e48aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow Field Dataset Creation Package v1.0.0 loaded successfully!\n",
      "\n",
      "Quick Start:\n",
      "  from dataset_creation import FlowFieldDatasetCreator\n",
      "  creator = FlowFieldDatasetCreator()\n",
      "  creator.create_all_datasets()\n",
      "\n",
      "For more info: print_package_info()\n",
      "TensorFlow version: 2.8.0\n",
      "Found 1 GPU(s): [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Suppress TensorFlow warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load local modules\n",
    "import model_fourier\n",
    "import models_improved\n",
    "import layer as flr_layer\n",
    "import config_manager\n",
    "from data.flow_field_dataset import FlowFieldDatasetCreator\n",
    "\n",
    "# Check available GPUs and set memory growth\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth for all GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Found {len(gpus)} GPU(s): {gpus}\")\n",
    "        # Use first GPU\n",
    "        gpu = gpus[0]\n",
    "        tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "        print(f\"Using GPU: {gpu}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error configuring GPUs: {e}\")\n",
    "else:\n",
    "    print(\"No GPUs found. Using CPU.\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22807b8f",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup\n",
    "\n",
    "Let's set up the configuration for our model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994c56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Configuration Summary\n",
      "==================================================\n",
      "üèóÔ∏è  Model Architecture:\n",
      "   - Fourier Enhancement: False\n",
      "   - Perceptual Loss: True\n",
      "   - Input Shape: [128, 256, 1]\n",
      "   - Latent Dimensions: 8\n",
      "   - Base Features: 64\n",
      "\n",
      "üì° Sensor Configuration:\n",
      "   - Layout: random\n",
      "   - Number of Sensors: 8\n",
      "   - Dataset: data/datasets\\dataset_random_8.npz\n",
      "\n",
      "üöÄ Training Parameters:\n",
      "   - VAE Epochs: 250\n",
      "   - FLRNet Epochs: 150\n",
      "   - VAE Learning Rate: 0.0001\n",
      "   - FLRNet Learning Rate: 0.0001\n",
      "   - Batch Size: 8\n",
      "   - Test Split: 0.2\n",
      "\n",
      "üíæ Output Configuration:\n",
      "   - Model Name: fourierFalse_percepTrue_random_8\n",
      "   - Checkpoints: ./checkpoints\\fourierFalse_percepTrue_random_8\n",
      "   - Logs: ./logs\\fourierFalse_percepTrue_random_8\n",
      "   - Save Best Model: True\n",
      "   - Save Last Model: True\n",
      "\n",
      "\n",
      "üîß Final Configuration (lowercase keys):\n",
      "   Model name: fourierFalse_percepTrue_random_8\n",
      "   Use Fourier: False\n",
      "   Use perceptual loss: True\n",
      "   Input shape: (128, 256, 1)\n",
      "   Number of sensors: 8\n",
      "   Latent dims: 8\n",
      "   Base features: 64\n",
      "   Batch size: 8\n",
      "   VAE epochs: 250\n",
      "   FLRNet epochs: 150\n",
      "   VAE learning rate: 0.0001\n",
      "   FLRNet learning rate: 0.0001\n",
      "   Dataset path: data/datasets\\dataset_random_8.npz\n",
      "   Checkpoint dir: ./checkpoints\\fourierFalse_percepTrue_random_8\n",
      "   Logs dir: ./logs\\fourierFalse_percepTrue_random_8\n",
      "‚úÖ All required configuration keys are present\n"
     ]
    }
   ],
   "source": [
    "# Load configuration using ConfigManager\n",
    "config_name = \"random_8_no_fourier\"\n",
    "config_mgr = config_manager.ConfigManager()\n",
    "hierarchical_config = config_mgr.load_config(config_name)\n",
    "\n",
    "# Print configuration summary\n",
    "print(config_mgr.create_config_summary(hierarchical_config))\n",
    "\n",
    "# Flatten the config for training (with lowercase keys for Python style)\n",
    "flattened_config = config_manager.flatten_config_for_training(hierarchical_config)\n",
    "\n",
    "# Convert to lowercase keys for consistent Python style\n",
    "config = {}\n",
    "for key, value in flattened_config.items():\n",
    "    config[key.lower()] = value\n",
    "\n",
    "print(\"\\nüîß Final Configuration (lowercase keys):\")\n",
    "print(f\"   Model name: {config['model_name']}\")\n",
    "print(f\"   Use Fourier: {config['use_fourier']}\")\n",
    "print(f\"   Use perceptual loss: {config['use_perceptual_loss']}\")\n",
    "print(f\"   Input shape: {config['input_shape']}\")\n",
    "print(f\"   Number of sensors: {config['n_sensors']}\")\n",
    "print(f\"   Latent dims: {config['latent_dims']}\")\n",
    "print(f\"   Base features: {config['n_base_features']}\")\n",
    "print(f\"   Batch size: {config['batch_size']}\")\n",
    "print(f\"   VAE epochs: {config['vae_epochs']}\")\n",
    "print(f\"   FLRNet epochs: {config['flr_epochs']}\")\n",
    "print(f\"   VAE learning rate: {config['vae_learning_rate']}\")\n",
    "print(f\"   FLRNet learning rate: {config['flr_learning_rate']}\")\n",
    "print(f\"   Dataset path: {config['dataset_path']}\")\n",
    "print(f\"   Checkpoint dir: {config['checkpoint_dir']}\")\n",
    "print(f\"   Logs dir: {config['logs_dir']}\")\n",
    "\n",
    "# Verify essential config keys exist\n",
    "required_keys = ['model_name', 'use_fourier', 'use_perceptual_loss', 'input_shape', \n",
    "                 'n_sensors', 'latent_dims', 'n_base_features', 'batch_size', \n",
    "                 'vae_epochs', 'flr_epochs', 'vae_learning_rate', 'flr_learning_rate',\n",
    "                 'dataset_path', 'checkpoint_dir', 'logs_dir']\n",
    "missing_keys = [key for key in required_keys if key not in config]\n",
    "if missing_keys:\n",
    "    print(f\"‚ö†Ô∏è  Missing required config keys: {missing_keys}\")\n",
    "else:\n",
    "    print(\"‚úÖ All required configuration keys are present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c2fdc",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset\n",
    "\n",
    "We'll load the flow field dataset and the sensor layout for 8 edge sensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f51841e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from: data/datasets\\dataset_random_8.npz\n",
      "üìä Dataset parameters:\n",
      "   Layout type: random\n",
      "   Number of sensors: 8\n",
      "üìÅ Loading dataset file: data/datasets\\dataset_random_8.npz\n",
      "üìã Available keys in dataset: ['sensor_data', 'field_data', 'sensor_positions', 'reynolds_numbers', 'layout_type', 'n_sensors']\n",
      "üìä Dataset loaded successfully:\n",
      "   sensor_data: (28, 8, 39) (float64)\n",
      "   field_data: (28, 128, 256, 39) (float64)\n",
      "   sensor_positions: (8, 2) (float64)\n",
      "   reynolds_numbers: (28,) (int32)\n",
      "   layout_type: () (<U6)\n",
      "   n_sensors: () (int32)\n",
      "üìç Sensor positions shape: (8, 2)\n",
      "Dataset reshaped:\n",
      "  Original sensor data: (28, 8, 39)\n",
      "  Reshaped sensor data: (1092, 8)\n",
      "  Original field data: (28, 128, 256, 39)\n",
      "  Reshaped field data: (1092, 128, 256, 1)\n",
      "  Total samples: 1092\n",
      "TensorFlow datasets created:\n",
      "  Train samples: 873\n",
      "  Test samples: 219\n",
      "\n",
      "üìä TensorFlow datasets created:\n",
      "   Train dataset: <ShuffleDataset element_spec={'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None)}>\n",
      "   Test dataset: <BatchDataset element_spec={'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None)}>\n",
      "\n",
      "üìä Coordinate-aware datasets created:\n",
      "   Train dataset with coordinates: <MapDataset element_spec={'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), 'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'coordinates': TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)}>\n",
      "   Test dataset with coordinates: <MapDataset element_spec={'field_data': TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), 'sensor_data': TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), 'coordinates': TensorSpec(shape=(None, 128, 256, 2), dtype=tf.float32, name=None)}>\n",
      "\n",
      "üîÑ Creating standard VAE datasets...\n",
      "‚úÖ Standard VAE datasets created\n",
      "\n",
      "üìä Specialized datasets:\n",
      "   VAE train dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   VAE test dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   FLRNet train dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "   FLRNet test dataset: <MapDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 256, 1), dtype=tf.float32, name=None))>\n",
      "\n",
      "üìä Data shape verification:\n",
      "   Sensor data shape: (8, 8)\n",
      "   Field data shape: (8, 128, 256, 1)\n",
      "   Coordinates shape: (8, 128, 256, 2)\n",
      "\n",
      "üìä VAE dataset structure verification:\n",
      "   VAE input shape: (8, 128, 256, 1)\n",
      "   VAE target shape: (8, 128, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare dataset created from data_creation_and_viz.ipynb\n",
    "print(f\"üìÇ Loading dataset from: {config['dataset_path']}\")\n",
    "\n",
    "# Parse the dataset filename to get layout and n_sensors\n",
    "dataset_filename = Path(config['dataset_path']).name\n",
    "# Expected format: dataset_edge_8.npz\n",
    "parts = dataset_filename.split('_')\n",
    "layout_type = parts[1]  # 'edge'\n",
    "n_sensors = int(parts[2].split('.')[0])  # 8\n",
    "\n",
    "print(f\"üìä Dataset parameters:\")\n",
    "print(f\"   Layout type: {layout_type}\")\n",
    "print(f\"   Number of sensors: {n_sensors}\")\n",
    "\n",
    "# Load the dataset directly from the NPZ file\n",
    "print(f\"üìÅ Loading dataset file: {config['dataset_path']}\")\n",
    "data = np.load(config['dataset_path'])\n",
    "\n",
    "# Check what keys are available in the dataset\n",
    "print(f\"üìã Available keys in dataset: {list(data.keys())}\")\n",
    "\n",
    "# Create dataset dictionary\n",
    "dataset = {key: data[key] for key in data.keys()}\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"üìä Dataset loaded successfully:\")\n",
    "for key, value in dataset.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"   {key}: {value.shape} ({value.dtype})\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "# Extract sensor positions\n",
    "sensor_positions = dataset['sensor_positions']\n",
    "print(f\"üìç Sensor positions shape: {sensor_positions.shape}\")\n",
    "\n",
    "# Create dataset creator instance for TensorFlow dataset creation\n",
    "creator = FlowFieldDatasetCreator(\n",
    "    output_path=\"./data/\",\n",
    "    domain_shape=config['input_shape'][:2],  # (height, width)\n",
    "    use_synthetic_data=False  # Don't create synthetic data, just use for TF dataset creation\n",
    ")\n",
    "\n",
    "# Create TensorFlow datasets using the creator's method\n",
    "train_dataset, test_dataset = creator.create_tensorflow_dataset(\n",
    "    dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    test_split=config['test_split']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä TensorFlow datasets created:\")\n",
    "print(f\"   Train dataset: {train_dataset}\")\n",
    "print(f\"   Test dataset: {test_dataset}\")\n",
    "\n",
    "# Function to add coordinate grids to field data for Fourier-aware VAE training\n",
    "def add_coordinate_grid(batch):\n",
    "    \"\"\"Add coordinate grid to field data for Fourier VAE training\"\"\"\n",
    "    field_data = batch['field_data']\n",
    "    \n",
    "    # Get dimensions\n",
    "    batch_size = tf.shape(field_data)[0]\n",
    "    height = tf.shape(field_data)[1]\n",
    "    width = tf.shape(field_data)[2]\n",
    "    \n",
    "    # Create normalized coordinate grids [0, 1] - match field dimensions\n",
    "    x_coords = tf.linspace(0.0, 1.0, width)   # Width corresponds to x\n",
    "    y_coords = tf.linspace(0.0, 1.0, height)  # Height corresponds to y\n",
    "    \n",
    "    # Create meshgrid to match image indexing: [height, width, 2]\n",
    "    y_grid, x_grid = tf.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    # Stack to create coordinate grid (height, width, 2)\n",
    "    coord_grid = tf.stack([x_grid, y_grid], axis=-1)\n",
    "    \n",
    "    # Expand to batch size (batch_size, height, width, 2)\n",
    "    coord_batch = tf.tile(tf.expand_dims(coord_grid, 0), [batch_size, 1, 1, 1])\n",
    "    \n",
    "    # Update batch to include coordinates\n",
    "    return {\n",
    "        'field_data': field_data,\n",
    "        'sensor_data': batch['sensor_data'],\n",
    "        'coordinates': coord_batch\n",
    "    }\n",
    "\n",
    "# Add coordinate grids to datasets\n",
    "coord_train_dataset = train_dataset.map(add_coordinate_grid)\n",
    "coord_test_dataset = test_dataset.map(add_coordinate_grid)\n",
    "\n",
    "print(f\"\\nüìä Coordinate-aware datasets created:\")\n",
    "print(f\"   Train dataset with coordinates: {coord_train_dataset}\")\n",
    "print(f\"   Test dataset with coordinates: {coord_test_dataset}\")\n",
    "\n",
    "# Create specialized datasets for VAE and FLRNet training\n",
    "# VAE trains on field reconstruction WITH coordinates for Fourier features\n",
    "if config['use_fourier']:\n",
    "    print(\"\\nüåä Creating Fourier-aware VAE datasets...\")\n",
    "    # For Fourier VAE: input = (field, coordinates), output = field\n",
    "    vae_train_dataset = coord_train_dataset.map(\n",
    "        lambda batch: ((batch['field_data'], batch['coordinates']), batch['field_data'])\n",
    "    )\n",
    "    vae_test_dataset = coord_test_dataset.map(\n",
    "        lambda batch: ((batch['field_data'], batch['coordinates']), batch['field_data'])\n",
    "    )\n",
    "    print(\"‚úÖ Fourier-aware VAE datasets created\")\n",
    "else:\n",
    "    print(\"\\nüîÑ Creating standard VAE datasets...\")\n",
    "    # Standard VAE: input = field, output = field\n",
    "    vae_train_dataset = coord_train_dataset.map(\n",
    "        lambda batch: (batch['field_data'], batch['field_data'])\n",
    "    )\n",
    "    vae_test_dataset = coord_test_dataset.map(\n",
    "        lambda batch: (batch['field_data'], batch['field_data'])\n",
    "    )\n",
    "    print(\"‚úÖ Standard VAE datasets created\")\n",
    "\n",
    "# FLRNet trains on sensor-to-field reconstruction (sensor -> field)\n",
    "flrnet_train_dataset = coord_train_dataset.map(\n",
    "    lambda batch: (batch['sensor_data'], batch['field_data'])\n",
    ")\n",
    "flrnet_test_dataset = coord_test_dataset.map(\n",
    "    lambda batch: (batch['sensor_data'], batch['field_data'])\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Specialized datasets:\")\n",
    "print(f\"   VAE train dataset: {vae_train_dataset}\")\n",
    "print(f\"   VAE test dataset: {vae_test_dataset}\")\n",
    "print(f\"   FLRNet train dataset: {flrnet_train_dataset}\")\n",
    "print(f\"   FLRNet test dataset: {flrnet_test_dataset}\")\n",
    "\n",
    "# Get a sample to verify data shapes\n",
    "print(f\"\\nüìä Data shape verification:\")\n",
    "for batch in coord_train_dataset.take(1):\n",
    "    print(f\"   Sensor data shape: {batch['sensor_data'].shape}\")\n",
    "    print(f\"   Field data shape: {batch['field_data'].shape}\")\n",
    "    print(f\"   Coordinates shape: {batch['coordinates'].shape}\")\n",
    "    break\n",
    "\n",
    "# Verify VAE dataset structure\n",
    "print(f\"\\nüìä VAE dataset structure verification:\")\n",
    "for vae_batch in vae_train_dataset.take(1):\n",
    "    if config['use_fourier']:\n",
    "        inputs, targets = vae_batch\n",
    "        if isinstance(inputs, tuple):\n",
    "            field_input, coord_input = inputs\n",
    "            print(f\"   VAE field input shape: {field_input.shape}\")\n",
    "            print(f\"   VAE coordinate input shape: {coord_input.shape}\")\n",
    "            print(f\"   VAE target shape: {targets.shape}\")\n",
    "        else:\n",
    "            print(f\"   Unexpected VAE input format: {type(inputs)}\")\n",
    "    else:\n",
    "        inputs, targets = vae_batch\n",
    "        print(f\"   VAE input shape: {inputs.shape}\")\n",
    "        print(f\"   VAE target shape: {targets.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61349511",
   "metadata": {},
   "source": [
    "## 5. Train VAE and FLRNet Models\n",
    "\n",
    "Now we'll train our models using the FLRTrainer:\n",
    "1. First, the Variational Autoencoder (VAE) to learn a compressed representation of flow fields\n",
    "2. Then, the FLRNet to reconstruct flow fields from sparse sensor measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "566a6830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Clearing TensorFlow session...\n",
      "üöÄ Initializing FLRTrainer...\n",
      "‚úÖ FLRTrainer initialized:\n",
      "   Input shape: (128, 256, 1)\n",
      "   Use Fourier: False\n",
      "   Model name: fourierFalse_percepTrue_random_8\n",
      "   Gradient clipping: 2.0\n",
      "   Checkpoints: ./checkpoints\\fourierFalse_percepTrue_random_8\n",
      "   Logs: ./logs\\fourierFalse_percepTrue_random_8\n",
      "   Save best model: True\n",
      "   Save last model: True\n",
      "\n",
      "üîß Training configuration:\n",
      "   Train VAE: True\n",
      "   Train FLRNet: True\n",
      "\n",
      "üîç Verifying dataset shapes...\n",
      "   Field data shape: (8, 128, 256, 1)\n",
      "   Coordinates shape: (8, 128, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# Clear TensorFlow session to fix any shape mismatches\n",
    "print(\"üîÑ Clearing TensorFlow session...\")\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Initialize FLRTrainer from models_improved.py\n",
    "print(\"üöÄ Initializing FLRTrainer...\")\n",
    "\n",
    "# Create the FLRTrainer instance\n",
    "trainer = models_improved.FLRTrainer(\n",
    "    input_shape=config['input_shape'],\n",
    "    use_fourier=config['use_fourier'],\n",
    "    checkpoint_dir=config['checkpoint_dir'],\n",
    "    logs_dir=config['logs_dir'],\n",
    "    model_name=config['model_name'],\n",
    "    save_best_model=config['save_best_model'],\n",
    "    save_last_model=config['save_last_model'],\n",
    "    gradient_clip_norm=config['gradient_clip_norm']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ FLRTrainer initialized:\")\n",
    "print(f\"   Input shape: {config['input_shape']}\")\n",
    "print(f\"   Use Fourier: {config['use_fourier']}\")\n",
    "print(f\"   Model name: {config['model_name']}\")\n",
    "print(f\"   Gradient clipping: {config['gradient_clip_norm']}\")\n",
    "print(f\"   Checkpoints: {config['checkpoint_dir']}\")\n",
    "print(f\"   Logs: {config['logs_dir']}\")\n",
    "print(f\"   Save best model: {config['save_best_model']}\")\n",
    "print(f\"   Save last model: {config['save_last_model']}\")\n",
    "\n",
    "# Set training flags\n",
    "train_vae_model = True  # Set to False if you want to skip VAE training\n",
    "train_flrnet_model = True  # Set to True to test FLRNet with proper Fourier features\n",
    "\n",
    "print(f\"\\nüîß Training configuration:\")\n",
    "print(f\"   Train VAE: {train_vae_model}\")\n",
    "print(f\"   Train FLRNet: {train_flrnet_model}\")\n",
    "\n",
    "# Quick test to verify dataset shapes are correct\n",
    "print(f\"\\nüîç Verifying dataset shapes...\")\n",
    "for batch in coord_train_dataset.take(1):\n",
    "    print(f\"   Field data shape: {batch['field_data'].shape}\")\n",
    "    print(f\"   Coordinates shape: {batch['coordinates'].shape}\")\n",
    "    break\n",
    "\n",
    "# Test VAE dataset format\n",
    "for vae_batch in vae_train_dataset.take(1):\n",
    "    inputs, targets = vae_batch\n",
    "    if isinstance(inputs, tuple):\n",
    "        field_input, coord_input = inputs\n",
    "        print(f\"   VAE field input shape: {field_input.shape}\")\n",
    "        print(f\"   VAE coordinate input shape: {coord_input.shape}\")\n",
    "        print(f\"   VAE target shape: {targets.shape}\")\n",
    "        \n",
    "        # Check if shapes match now\n",
    "        if field_input.shape[1:3] == coord_input.shape[1:3]:\n",
    "            print(f\"   ‚úÖ Coordinate shapes now match field shapes!\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Shape mismatch: field {field_input.shape[1:3]} vs coord {coord_input.shape[1:3]}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1d0dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Starting VAE Training with Coordinate-Aware Data...\n",
      "============================================================\n",
      "\n",
      "üåä Training VAE with Fourier features: False\n",
      "   VAE will be trained with standard field data\n",
      "\n",
      "üõ°Ô∏è Note: Checkpoint saving is disabled for the first 30 epochs to avoid Windows file lock issues.\n",
      "   Saving will automatically start from epoch 31 onward.\n",
      "   This ensures robust training without checkpoint file access conflicts.\n",
      "üöÄ Training VAE Model...\n",
      "üõ°Ô∏è Checkpoint saving will be disabled for epochs 1-30, enabled from epoch 31\n",
      "üõ°Ô∏è Checkpoint saving will be disabled for epochs 1-30, enabled from epoch 31\n",
      "Epoch 1/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 158829.7239 - reconstruction_loss: 107232.2578 - kl_loss: 626.4015 - perceptual_loss: 71.0264Epoch 1: val_reconstruction_loss improved from inf to 22982.37891 (saving disabled for epochs 1-30)\n",
      "Epoch 1: val_reconstruction_loss improved from inf to 22982.37891 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 31s 176ms/step - loss: 158371.1650 - reconstruction_loss: 107232.2578 - kl_loss: 626.4015 - perceptual_loss: 71.0264 - val_loss: 23425.7188 - val_reconstruction_loss: 22982.3789 - val_kl_loss: 418.0415 - val_perceptual_loss: 25.2981 - lr: 1.0000e-04\n",
      "Epoch 2/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 59824.0709 - reconstruction_loss: 54466.0234 - kl_loss: 1649.7542 - perceptual_loss: 62.7437Epoch 2: val_reconstruction_loss improved from 22982.37891 to 15885.23047 (saving disabled for epochs 1-30)\n",
      "Epoch 2: val_reconstruction_loss improved from 22982.37891 to 15885.23047 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 150ms/step - loss: 59791.2280 - reconstruction_loss: 54466.0234 - kl_loss: 1649.7542 - perceptual_loss: 62.7437 - val_loss: 16646.5293 - val_reconstruction_loss: 15885.2305 - val_kl_loss: 741.3151 - val_perceptual_loss: 19.9848 - lr: 1.0000e-04\n",
      "Epoch 3/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 46548.8577 - reconstruction_loss: 42774.5586 - kl_loss: 2181.8250 - perceptual_loss: 50.4667Epoch 3: val_reconstruction_loss improved from 15885.23047 to 14173.91602 (saving disabled for epochs 1-30)\n",
      "Epoch 3: val_reconstruction_loss improved from 15885.23047 to 14173.91602 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 151ms/step - loss: 46534.9656 - reconstruction_loss: 42774.5586 - kl_loss: 2181.8250 - perceptual_loss: 50.4667 - val_loss: 14925.8838 - val_reconstruction_loss: 14173.9160 - val_kl_loss: 736.8427 - val_perceptual_loss: 15.1247 - lr: 1.0000e-04\n",
      "Epoch 4/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 39554.5459 - reconstruction_loss: 36954.7266 - kl_loss: 2281.0205 - perceptual_loss: 34.9856Epoch 4: val_reconstruction_loss improved from 14173.91602 to 12810.75879 (saving disabled for epochs 1-30)\n",
      "Epoch 4: val_reconstruction_loss improved from 14173.91602 to 12810.75879 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 150ms/step - loss: 39551.9891 - reconstruction_loss: 36954.7266 - kl_loss: 2281.0205 - perceptual_loss: 34.9856 - val_loss: 13603.9863 - val_reconstruction_loss: 12810.7588 - val_kl_loss: 781.8983 - val_perceptual_loss: 11.3294 - lr: 1.0000e-04\n",
      "Epoch 5/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 36842.9658 - reconstruction_loss: 33739.5664 - kl_loss: 2346.4626 - perceptual_loss: 28.4357Epoch 5: val_reconstruction_loss improved from 12810.75879 to 12581.68945 (saving disabled for epochs 1-30)\n",
      "Epoch 5: val_reconstruction_loss improved from 12810.75879 to 12581.68945 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 150ms/step - loss: 36836.4027 - reconstruction_loss: 33739.5664 - kl_loss: 2346.4626 - perceptual_loss: 28.4357 - val_loss: 13452.3447 - val_reconstruction_loss: 12581.6895 - val_kl_loss: 859.4414 - val_perceptual_loss: 11.2138 - lr: 1.0000e-04\n",
      "Epoch 6/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 33937.2876 - reconstruction_loss: 31629.0098 - kl_loss: 2370.7905 - perceptual_loss: 25.1640Epoch 6: val_reconstruction_loss improved from 12581.68945 to 12300.27441 (saving disabled for epochs 1-30)\n",
      "Epoch 6: val_reconstruction_loss improved from 12581.68945 to 12300.27441 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 151ms/step - loss: 33938.0774 - reconstruction_loss: 31629.0098 - kl_loss: 2370.7905 - perceptual_loss: 25.1640 - val_loss: 13115.2852 - val_reconstruction_loss: 12300.2744 - val_kl_loss: 805.3820 - val_perceptual_loss: 9.6288 - lr: 1.0000e-04\n",
      "Epoch 7/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 32084.7762 - reconstruction_loss: 29829.7207 - kl_loss: 2377.9775 - perceptual_loss: 24.2516Epoch 7: val_reconstruction_loss improved from 12300.27441 to 10767.60156 (saving disabled for epochs 1-30)\n",
      "Epoch 7: val_reconstruction_loss improved from 12300.27441 to 10767.60156 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 150ms/step - loss: 32086.1021 - reconstruction_loss: 29829.7207 - kl_loss: 2377.9775 - perceptual_loss: 24.2516 - val_loss: 11652.0781 - val_reconstruction_loss: 10767.6016 - val_kl_loss: 875.1398 - val_perceptual_loss: 9.3368 - lr: 1.0000e-04\n",
      "Epoch 8/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 30950.1765 - reconstruction_loss: 28493.0469 - kl_loss: 2385.1487 - perceptual_loss: 22.6534Epoch 8: val_reconstruction_loss did not improve from 10767.60156 (saving disabled)\n",
      "Epoch 8: val_reconstruction_loss did not improve from 10767.60156 (saving disabled)\n",
      "110/110 [==============================] - 17s 149ms/step - loss: 30949.7321 - reconstruction_loss: 28493.0469 - kl_loss: 2385.1487 - perceptual_loss: 22.6534 - val_loss: 11852.5938 - val_reconstruction_loss: 10982.1113 - val_kl_loss: 861.3590 - val_perceptual_loss: 9.1233 - lr: 1.0000e-04\n",
      "Epoch 9/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 27933.0301 - reconstruction_loss: 24591.1426 - kl_loss: 2591.0032 - perceptual_loss: 21.6248Epoch 9: val_reconstruction_loss improved from 10767.60156 to 9528.88965 (saving disabled for epochs 1-30)\n",
      "Epoch 9: val_reconstruction_loss improved from 10767.60156 to 9528.88965 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 150ms/step - loss: 27926.4602 - reconstruction_loss: 24591.1426 - kl_loss: 2591.0032 - perceptual_loss: 21.6248 - val_loss: 10479.2471 - val_reconstruction_loss: 9528.8896 - val_kl_loss: 941.8278 - val_perceptual_loss: 8.5290 - lr: 1.0000e-04\n",
      "Epoch 10/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 28892.4407 - reconstruction_loss: 26284.5957 - kl_loss: 2395.8076 - perceptual_loss: 20.5179Epoch 10: val_reconstruction_loss improved from 9528.88965 to 8538.38281 (saving disabled for epochs 1-30)\n",
      "Epoch 10: val_reconstruction_loss improved from 9528.88965 to 8538.38281 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 150ms/step - loss: 28890.7152 - reconstruction_loss: 26284.5957 - kl_loss: 2395.8076 - perceptual_loss: 20.5179 - val_loss: 9421.2061 - val_reconstruction_loss: 8538.3828 - val_kl_loss: 874.6161 - val_perceptual_loss: 8.2066 - lr: 1.0000e-04\n",
      "Epoch 11/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 27639.5296 - reconstruction_loss: 25262.0449 - kl_loss: 2416.1514 - perceptual_loss: 19.7191Epoch 11: val_reconstruction_loss did not improve from 8538.38281 (saving disabled)\n",
      "Epoch 11: val_reconstruction_loss did not improve from 8538.38281 (saving disabled)\n",
      "110/110 [==============================] - 17s 149ms/step - loss: 27640.0557 - reconstruction_loss: 25262.0449 - kl_loss: 2416.1514 - perceptual_loss: 19.7191 - val_loss: 10472.5117 - val_reconstruction_loss: 9639.8330 - val_kl_loss: 825.0918 - val_perceptual_loss: 7.5865 - lr: 1.0000e-04\n",
      "Epoch 12/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 27388.5568 - reconstruction_loss: 24526.7090 - kl_loss: 2395.6069 - perceptual_loss: 19.0855Epoch 12: val_reconstruction_loss improved from 8538.38281 to 8525.41016 (saving disabled for epochs 1-30)\n",
      "Epoch 12: val_reconstruction_loss improved from 8538.38281 to 8525.41016 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 152ms/step - loss: 27384.5283 - reconstruction_loss: 24526.7090 - kl_loss: 2395.6069 - perceptual_loss: 19.0855 - val_loss: 9401.8555 - val_reconstruction_loss: 8525.4102 - val_kl_loss: 868.9254 - val_perceptual_loss: 7.5194 - lr: 1.0000e-04\n",
      "Epoch 13/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 26007.2403 - reconstruction_loss: 23897.2188 - kl_loss: 2446.3640 - perceptual_loss: 18.2334Epoch 13: val_reconstruction_loss improved from 8525.41016 to 8357.59766 (saving disabled for epochs 1-30)\n",
      "Epoch 13: val_reconstruction_loss improved from 8525.41016 to 8357.59766 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 150ms/step - loss: 26010.4347 - reconstruction_loss: 23897.2188 - kl_loss: 2446.3640 - perceptual_loss: 18.2334 - val_loss: 9281.5039 - val_reconstruction_loss: 8357.5977 - val_kl_loss: 916.3826 - val_perceptual_loss: 7.5235 - lr: 1.0000e-04\n",
      "Epoch 14/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 25659.3855 - reconstruction_loss: 23301.2930 - kl_loss: 2472.7581 - perceptual_loss: 17.7637Epoch 14: val_reconstruction_loss did not improve from 8357.59766 (saving disabled)\n",
      "Epoch 14: val_reconstruction_loss did not improve from 8357.59766 (saving disabled)\n",
      "110/110 [==============================] - 17s 150ms/step - loss: 25660.5786 - reconstruction_loss: 23301.2930 - kl_loss: 2472.7581 - perceptual_loss: 17.7637 - val_loss: 11002.2383 - val_reconstruction_loss: 10141.0498 - val_kl_loss: 853.4874 - val_perceptual_loss: 7.7011 - lr: 1.0000e-04\n",
      "Epoch 15/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 25425.3060 - reconstruction_loss: 22528.5332 - kl_loss: 2448.9924 - perceptual_loss: 17.0433Epoch 15: val_reconstruction_loss did not improve from 8357.59766 (saving disabled)\n",
      "Epoch 15: val_reconstruction_loss did not improve from 8357.59766 (saving disabled)\n",
      "110/110 [==============================] - 17s 150ms/step - loss: 25421.4254 - reconstruction_loss: 22528.5332 - kl_loss: 2448.9924 - perceptual_loss: 17.0433 - val_loss: 9294.6162 - val_reconstruction_loss: 8370.5371 - val_kl_loss: 917.0311 - val_perceptual_loss: 7.0478 - lr: 1.0000e-04\n",
      "Epoch 16/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 24494.5462 - reconstruction_loss: 21929.6934 - kl_loss: 2451.4187 - perceptual_loss: 16.3592Epoch 16: val_reconstruction_loss improved from 8357.59766 to 7496.79150 (saving disabled for epochs 1-30)\n",
      "Epoch 16: val_reconstruction_loss improved from 8357.59766 to 7496.79150 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 151ms/step - loss: 24493.6716 - reconstruction_loss: 21929.6934 - kl_loss: 2451.4187 - perceptual_loss: 16.3592 - val_loss: 8396.5488 - val_reconstruction_loss: 7496.7915 - val_kl_loss: 892.8252 - val_perceptual_loss: 6.9319 - lr: 1.0000e-04\n",
      "Epoch 17/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 23644.3584 - reconstruction_loss: 21234.1836 - kl_loss: 2449.1133 - perceptual_loss: 15.8631Epoch 17: val_reconstruction_loss did not improve from 7496.79150 (saving disabled)\n",
      "Epoch 17: val_reconstruction_loss did not improve from 7496.79150 (saving disabled)\n",
      "110/110 [==============================] - 17s 150ms/step - loss: 23644.8520 - reconstruction_loss: 21234.1836 - kl_loss: 2449.1133 - perceptual_loss: 15.8631 - val_loss: 10963.0703 - val_reconstruction_loss: 10121.3818 - val_kl_loss: 834.5741 - val_perceptual_loss: 7.1147 - lr: 1.0000e-04\n",
      "Epoch 18/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 23574.8931 - reconstruction_loss: 20738.2676 - kl_loss: 2413.0186 - perceptual_loss: 15.4242Epoch 18: val_reconstruction_loss improved from 7496.79150 to 6874.58594 (saving disabled for epochs 1-30)\n",
      "Epoch 18: val_reconstruction_loss improved from 7496.79150 to 6874.58594 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 152ms/step - loss: 23571.2159 - reconstruction_loss: 20738.2676 - kl_loss: 2413.0186 - perceptual_loss: 15.4242 - val_loss: 7775.4404 - val_reconstruction_loss: 6874.5859 - val_kl_loss: 894.3568 - val_perceptual_loss: 6.4976 - lr: 1.0000e-04\n",
      "Epoch 19/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 23088.5114 - reconstruction_loss: 20486.0391 - kl_loss: 2442.7971 - perceptual_loss: 14.8508Epoch 19: val_reconstruction_loss did not improve from 6874.58594 (saving disabled)\n",
      "Epoch 19: val_reconstruction_loss did not improve from 6874.58594 (saving disabled)\n",
      "110/110 [==============================] - 17s 149ms/step - loss: 23087.2068 - reconstruction_loss: 20486.0391 - kl_loss: 2442.7971 - perceptual_loss: 14.8508 - val_loss: 9143.1807 - val_reconstruction_loss: 8253.2012 - val_kl_loss: 883.3396 - val_perceptual_loss: 6.6395 - lr: 1.0000e-04\n",
      "Epoch 20/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 22731.9522 - reconstruction_loss: 20228.2617 - kl_loss: 2326.1238 - perceptual_loss: 14.6387Epoch 20: val_reconstruction_loss did not improve from 6874.58594 (saving disabled)\n",
      "Epoch 20: val_reconstruction_loss did not improve from 6874.58594 (saving disabled)\n",
      "110/110 [==============================] - 17s 151ms/step - loss: 22730.4845 - reconstruction_loss: 20228.2617 - kl_loss: 2326.1238 - perceptual_loss: 14.6387 - val_loss: 8845.4170 - val_reconstruction_loss: 8013.9116 - val_kl_loss: 825.2116 - val_perceptual_loss: 6.2933 - lr: 1.0000e-04\n",
      "Epoch 21/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 22340.1773 - reconstruction_loss: 20104.5840 - kl_loss: 2340.4294 - perceptual_loss: 14.3291Epoch 21: val_reconstruction_loss did not improve from 6874.58594 (saving disabled)\n",
      "Epoch 21: val_reconstruction_loss did not improve from 6874.58594 (saving disabled)\n",
      "110/110 [==============================] - 17s 149ms/step - loss: 22341.2509 - reconstruction_loss: 20104.5840 - kl_loss: 2340.4294 - perceptual_loss: 14.3291 - val_loss: 8534.9043 - val_reconstruction_loss: 7662.7988 - val_kl_loss: 865.9945 - val_perceptual_loss: 6.1113 - lr: 1.0000e-04\n",
      "Epoch 22/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 21920.6039 - reconstruction_loss: 19024.7617 - kl_loss: 2360.6807 - perceptual_loss: 13.8266Epoch 22: val_reconstruction_loss improved from 6874.58594 to 6564.00000 (saving disabled for epochs 1-30)\n",
      "Epoch 22: val_reconstruction_loss improved from 6874.58594 to 6564.00000 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 151ms/step - loss: 21915.9071 - reconstruction_loss: 19024.7617 - kl_loss: 2360.6807 - perceptual_loss: 13.8266 - val_loss: 7444.7686 - val_reconstruction_loss: 6564.0000 - val_kl_loss: 874.7776 - val_perceptual_loss: 5.9910 - lr: 1.0000e-04\n",
      "Epoch 23/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 21451.7050 - reconstruction_loss: 18850.5664 - kl_loss: 2318.6497 - perceptual_loss: 13.3017Epoch 23: val_reconstruction_loss improved from 6564.00000 to 6318.86182 (saving disabled for epochs 1-30)\n",
      "Epoch 23: val_reconstruction_loss improved from 6564.00000 to 6318.86182 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 151ms/step - loss: 21449.2799 - reconstruction_loss: 18850.5664 - kl_loss: 2318.6497 - perceptual_loss: 13.3017 - val_loss: 7204.6313 - val_reconstruction_loss: 6318.8618 - val_kl_loss: 880.2040 - val_perceptual_loss: 5.5655 - lr: 1.0000e-04\n",
      "Epoch 24/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 21046.0976 - reconstruction_loss: 18452.0000 - kl_loss: 2353.6094 - perceptual_loss: 12.8753Epoch 24: val_reconstruction_loss did not improve from 6318.86182 (saving disabled)\n",
      "Epoch 24: val_reconstruction_loss did not improve from 6318.86182 (saving disabled)\n",
      "110/110 [==============================] - 17s 150ms/step - loss: 21044.0470 - reconstruction_loss: 18452.0000 - kl_loss: 2353.6094 - perceptual_loss: 12.8753 - val_loss: 8448.1045 - val_reconstruction_loss: 7620.5532 - val_kl_loss: 822.1080 - val_perceptual_loss: 5.4427 - lr: 1.0000e-04\n",
      "Epoch 25/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 20827.3651 - reconstruction_loss: 18272.4551 - kl_loss: 2300.4927 - perceptual_loss: 12.5819Epoch 25: val_reconstruction_loss did not improve from 6318.86182 (saving disabled)\n",
      "Epoch 25: val_reconstruction_loss did not improve from 6318.86182 (saving disabled)\n",
      "110/110 [==============================] - 17s 150ms/step - loss: 20825.1864 - reconstruction_loss: 18272.4551 - kl_loss: 2300.4927 - perceptual_loss: 12.5819 - val_loss: 8710.9355 - val_reconstruction_loss: 7857.3779 - val_kl_loss: 848.2015 - val_perceptual_loss: 5.3563 - lr: 1.0000e-04\n",
      "Epoch 26/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 21491.1121 - reconstruction_loss: 18288.3066 - kl_loss: 2254.8826 - perceptual_loss: 12.4676Epoch 26: val_reconstruction_loss improved from 6318.86182 to 5995.75537 (saving disabled for epochs 1-30)\n",
      "Epoch 26: val_reconstruction_loss improved from 6318.86182 to 5995.75537 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 153ms/step - loss: 21482.6845 - reconstruction_loss: 18288.3066 - kl_loss: 2254.8826 - perceptual_loss: 12.4676 - val_loss: 6842.6699 - val_reconstruction_loss: 5995.7554 - val_kl_loss: 841.8048 - val_perceptual_loss: 5.1100 - lr: 1.0000e-04\n",
      "Epoch 27/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 18789.6304 - reconstruction_loss: 16671.4121 - kl_loss: 2382.5295 - perceptual_loss: 12.3988Epoch 27: val_reconstruction_loss did not improve from 5995.75537 (saving disabled)\n",
      "Epoch 27: val_reconstruction_loss did not improve from 5995.75537 (saving disabled)\n",
      "110/110 [==============================] - 17s 151ms/step - loss: 18792.1232 - reconstruction_loss: 16671.4121 - kl_loss: 2382.5295 - perceptual_loss: 12.3988 - val_loss: 7039.7207 - val_reconstruction_loss: 6193.0386 - val_kl_loss: 841.3560 - val_perceptual_loss: 5.3260 - lr: 1.0000e-04\n",
      "Epoch 28/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 20063.0514 - reconstruction_loss: 17570.4180 - kl_loss: 2344.1245 - perceptual_loss: 11.8483Epoch 28: val_reconstruction_loss improved from 5995.75537 to 4826.80859 (saving disabled for epochs 1-30)\n",
      "Epoch 28: val_reconstruction_loss improved from 5995.75537 to 4826.80859 (saving disabled for epochs 1-30)\n",
      "110/110 [==============================] - 17s 153ms/step - loss: 20061.8202 - reconstruction_loss: 17570.4180 - kl_loss: 2344.1245 - perceptual_loss: 11.8483 - val_loss: 5674.1309 - val_reconstruction_loss: 4826.8086 - val_kl_loss: 842.8541 - val_perceptual_loss: 4.4682 - lr: 1.0000e-04\n",
      "Epoch 29/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 19061.5516 - reconstruction_loss: 16703.6289 - kl_loss: 2339.5098 - perceptual_loss: 11.4928Epoch 29: val_reconstruction_loss did not improve from 4826.80859 (saving disabled)\n",
      "Epoch 29: val_reconstruction_loss did not improve from 4826.80859 (saving disabled)\n",
      "110/110 [==============================] - 17s 151ms/step - loss: 19061.4892 - reconstruction_loss: 16703.6289 - kl_loss: 2339.5098 - perceptual_loss: 11.4928 - val_loss: 6581.9697 - val_reconstruction_loss: 5740.2285 - val_kl_loss: 836.9385 - val_perceptual_loss: 4.8026 - lr: 1.0000e-04\n",
      "Epoch 30/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 19351.4084 - reconstruction_loss: 17030.9629 - kl_loss: 2335.3831 - perceptual_loss: 11.3645Epoch 30: val_reconstruction_loss did not improve from 4826.80859 (saving disabled)\n",
      "Epoch 30: val_reconstruction_loss did not improve from 4826.80859 (saving disabled)\n",
      "110/110 [==============================] - 17s 156ms/step - loss: 19351.6453 - reconstruction_loss: 17030.9629 - kl_loss: 2335.3831 - perceptual_loss: 11.3645 - val_loss: 8308.9395 - val_reconstruction_loss: 7429.9873 - val_kl_loss: 874.3397 - val_perceptual_loss: 4.6124 - lr: 1.0000e-04\n",
      "Epoch 31/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 19191.2346 - reconstruction_loss: 16455.6641 - kl_loss: 2266.4756 - perceptual_loss: 11.1503\n",
      "Epoch 31: val_reconstruction_loss did not improve from 4826.80859\n",
      "110/110 [==============================] - 19s 167ms/step - loss: 19187.1090 - reconstruction_loss: 16455.6641 - kl_loss: 2266.4756 - perceptual_loss: 11.1503 - val_loss: 7562.4121 - val_reconstruction_loss: 6734.3086 - val_kl_loss: 823.6654 - val_perceptual_loss: 4.4380 - lr: 1.0000e-04\n",
      "Epoch 32/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 18085.2201 - reconstruction_loss: 16565.7969 - kl_loss: 2260.5913 - perceptual_loss: 10.9695\n",
      "Epoch 32: val_reconstruction_loss did not improve from 4826.80859\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 18091.9962 - reconstruction_loss: 16565.7969 - kl_loss: 2260.5913 - perceptual_loss: 10.9695 - val_loss: 6461.3857 - val_reconstruction_loss: 5635.6255 - val_kl_loss: 821.2498 - val_perceptual_loss: 4.5108 - lr: 1.0000e-04\n",
      "Epoch 33/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 18421.8067 - reconstruction_loss: 15469.1855 - kl_loss: 2359.9595 - perceptual_loss: 11.1139\n",
      "Epoch 33: val_reconstruction_loss did not improve from 4826.80859\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 18416.5675 - reconstruction_loss: 15469.1855 - kl_loss: 2359.9595 - perceptual_loss: 11.1139 - val_loss: 8085.0542 - val_reconstruction_loss: 7209.8301 - val_kl_loss: 870.7043 - val_perceptual_loss: 4.5202 - lr: 1.0000e-04\n",
      "Epoch 34/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 17968.2904 - reconstruction_loss: 15351.6191 - kl_loss: 2375.9951 - perceptual_loss: 10.8916\n",
      "Epoch 34: val_reconstruction_loss did not improve from 4826.80859\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 17966.2203 - reconstruction_loss: 15351.6191 - kl_loss: 2375.9951 - perceptual_loss: 10.8916 - val_loss: 6805.8179 - val_reconstruction_loss: 5851.1348 - val_kl_loss: 949.8740 - val_perceptual_loss: 4.8090 - lr: 1.0000e-04\n",
      "Epoch 35/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 15920.6356 - reconstruction_loss: 12981.1543 - kl_loss: 2457.1047 - perceptual_loss: 10.5086\n",
      "Epoch 35: val_reconstruction_loss improved from 4826.80859 to 4440.37012, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 172ms/step - loss: 15916.3845 - reconstruction_loss: 12981.1543 - kl_loss: 2457.1047 - perceptual_loss: 10.5086 - val_loss: 5340.6729 - val_reconstruction_loss: 4440.3701 - val_kl_loss: 896.4351 - val_perceptual_loss: 3.8676 - lr: 1.0000e-04\n",
      "Epoch 36/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 15311.5846 - reconstruction_loss: 14429.8076 - kl_loss: 2432.8945 - perceptual_loss: 10.0184\n",
      "Epoch 36: val_reconstruction_loss did not improve from 4440.37012\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 15325.6489 - reconstruction_loss: 14429.8076 - kl_loss: 2432.8945 - perceptual_loss: 10.0184 - val_loss: 6218.4688 - val_reconstruction_loss: 5365.2451 - val_kl_loss: 849.3506 - val_perceptual_loss: 3.8730 - lr: 1.0000e-04\n",
      "Epoch 37/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 17760.8353 - reconstruction_loss: 15177.0537 - kl_loss: 2289.1951 - perceptual_loss: 10.0612\n",
      "Epoch 37: val_reconstruction_loss did not improve from 4440.37012\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 17758.2719 - reconstruction_loss: 15177.0537 - kl_loss: 2289.1951 - perceptual_loss: 10.0612 - val_loss: 5942.5747 - val_reconstruction_loss: 5120.1699 - val_kl_loss: 818.5547 - val_perceptual_loss: 3.8502 - lr: 1.0000e-04\n",
      "Epoch 38/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 16895.0001 - reconstruction_loss: 15355.6484 - kl_loss: 2276.2151 - perceptual_loss: 10.0431\n",
      "Epoch 38: val_reconstruction_loss did not improve from 4440.37012\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 16901.7290 - reconstruction_loss: 15355.6484 - kl_loss: 2276.2151 - perceptual_loss: 10.0431 - val_loss: 7062.7495 - val_reconstruction_loss: 6212.2041 - val_kl_loss: 846.6284 - val_perceptual_loss: 3.9168 - lr: 1.0000e-04\n",
      "Epoch 39/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 16925.9164 - reconstruction_loss: 14816.1045 - kl_loss: 2239.8586 - perceptual_loss: 9.7712\n",
      "Epoch 39: val_reconstruction_loss did not improve from 4440.37012\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 16927.1760 - reconstruction_loss: 14816.1045 - kl_loss: 2239.8586 - perceptual_loss: 9.7712 - val_loss: 5922.5010 - val_reconstruction_loss: 5088.0991 - val_kl_loss: 830.6758 - val_perceptual_loss: 3.7262 - lr: 1.0000e-04\n",
      "Epoch 40/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 17880.7167 - reconstruction_loss: 15665.3330 - kl_loss: 2327.8157 - perceptual_loss: 10.7012\n",
      "Epoch 40: val_reconstruction_loss improved from 4440.37012 to 4357.00195, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 172ms/step - loss: 17881.8260 - reconstruction_loss: 15665.3330 - kl_loss: 2327.8157 - perceptual_loss: 10.7012 - val_loss: 5246.9673 - val_reconstruction_loss: 4357.0020 - val_kl_loss: 886.2266 - val_perceptual_loss: 3.7387 - lr: 1.0000e-04\n",
      "Epoch 41/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 17299.8344 - reconstruction_loss: 15071.5488 - kl_loss: 2314.3882 - perceptual_loss: 9.7834\n",
      "Epoch 41: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 17300.6982 - reconstruction_loss: 15071.5488 - kl_loss: 2314.3882 - perceptual_loss: 9.7834 - val_loss: 5483.6812 - val_reconstruction_loss: 4628.3750 - val_kl_loss: 851.8916 - val_perceptual_loss: 3.4144 - lr: 1.0000e-04\n",
      "Epoch 42/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 16657.1003 - reconstruction_loss: 14278.3877 - kl_loss: 2279.6843 - perceptual_loss: 9.3830\n",
      "Epoch 42: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 16656.2927 - reconstruction_loss: 14278.3877 - kl_loss: 2279.6843 - perceptual_loss: 9.3830 - val_loss: 6950.4028 - val_reconstruction_loss: 6110.1699 - val_kl_loss: 836.4258 - val_perceptual_loss: 3.8070 - lr: 1.0000e-04\n",
      "Epoch 43/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 17279.1885 - reconstruction_loss: 14840.1846 - kl_loss: 2332.9871 - perceptual_loss: 9.4579\n",
      "Epoch 43: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 17278.3186 - reconstruction_loss: 14840.1846 - kl_loss: 2332.9871 - perceptual_loss: 9.4579 - val_loss: 5693.4351 - val_reconstruction_loss: 4822.2305 - val_kl_loss: 867.5410 - val_perceptual_loss: 3.6638 - lr: 1.0000e-04\n",
      "Epoch 44/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 17964.8209 - reconstruction_loss: 15052.6348 - kl_loss: 2396.8943 - perceptual_loss: 9.5938\n",
      "Epoch 44: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 17960.2651 - reconstruction_loss: 15052.6348 - kl_loss: 2396.8943 - perceptual_loss: 9.5938 - val_loss: 7616.8677 - val_reconstruction_loss: 6675.8447 - val_kl_loss: 937.5287 - val_perceptual_loss: 3.4943 - lr: 1.0000e-04\n",
      "Epoch 45/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 16417.2359 - reconstruction_loss: 13880.1934 - kl_loss: 2368.8770 - perceptual_loss: 9.0809\n",
      "Epoch 45: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 16415.8027 - reconstruction_loss: 13880.1934 - kl_loss: 2368.8770 - perceptual_loss: 9.0809 - val_loss: 6318.7466 - val_reconstruction_loss: 5455.3301 - val_kl_loss: 859.9294 - val_perceptual_loss: 3.4871 - lr: 1.0000e-04\n",
      "Epoch 46/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 16131.0398 - reconstruction_loss: 13717.3115 - kl_loss: 2330.2429 - perceptual_loss: 8.9571\n",
      "Epoch 46: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 16130.3684 - reconstruction_loss: 13717.3115 - kl_loss: 2330.2429 - perceptual_loss: 8.9571 - val_loss: 6377.9458 - val_reconstruction_loss: 5532.8320 - val_kl_loss: 841.4468 - val_perceptual_loss: 3.6670 - lr: 1.0000e-04\n",
      "Epoch 47/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 16266.6016 - reconstruction_loss: 13936.9639 - kl_loss: 2233.4270 - perceptual_loss: 9.0047\n",
      "Epoch 47: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 16265.8159 - reconstruction_loss: 13936.9639 - kl_loss: 2233.4270 - perceptual_loss: 9.0047 - val_loss: 7349.6138 - val_reconstruction_loss: 6483.2070 - val_kl_loss: 862.9298 - val_perceptual_loss: 3.4772 - lr: 1.0000e-04\n",
      "Epoch 48/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 16143.7805 - reconstruction_loss: 13807.5684 - kl_loss: 2235.8215 - perceptual_loss: 8.8884\n",
      "Epoch 48: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 16142.9561 - reconstruction_loss: 13807.5684 - kl_loss: 2235.8215 - perceptual_loss: 8.8884 - val_loss: 5473.1943 - val_reconstruction_loss: 4667.7666 - val_kl_loss: 802.1363 - val_perceptual_loss: 3.2913 - lr: 1.0000e-04\n",
      "Epoch 49/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 15879.6129 - reconstruction_loss: 14232.6582 - kl_loss: 2233.4050 - perceptual_loss: 8.9794\n",
      "Epoch 49: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 15884.9771 - reconstruction_loss: 14232.6582 - kl_loss: 2233.4050 - perceptual_loss: 8.9794 - val_loss: 5389.9966 - val_reconstruction_loss: 4573.3203 - val_kl_loss: 813.2316 - val_perceptual_loss: 3.4449 - lr: 1.0000e-04\n",
      "Epoch 50/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 16096.3186 - reconstruction_loss: 13816.7207 - kl_loss: 2207.8772 - perceptual_loss: 8.7297\n",
      "Epoch 50: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 16095.7511 - reconstruction_loss: 13816.7207 - kl_loss: 2207.8772 - perceptual_loss: 8.7297 - val_loss: 7232.6587 - val_reconstruction_loss: 6378.2207 - val_kl_loss: 851.1172 - val_perceptual_loss: 3.3208 - lr: 1.0000e-04\n",
      "Epoch 51/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 15821.7807 - reconstruction_loss: 13773.8428 - kl_loss: 2248.0671 - perceptual_loss: 8.8216\n",
      "Epoch 51: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 15823.6631 - reconstruction_loss: 13773.8428 - kl_loss: 2248.0671 - perceptual_loss: 8.8216 - val_loss: 6356.6572 - val_reconstruction_loss: 5567.7871 - val_kl_loss: 785.2671 - val_perceptual_loss: 3.6032 - lr: 1.0000e-04\n",
      "Epoch 52/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 15336.1265 - reconstruction_loss: 13395.7402 - kl_loss: 2248.9658 - perceptual_loss: 9.0844\n",
      "Epoch 52: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 15338.9883 - reconstruction_loss: 13395.7402 - kl_loss: 2248.9658 - perceptual_loss: 9.0844 - val_loss: 5318.1235 - val_reconstruction_loss: 4455.1084 - val_kl_loss: 859.6224 - val_perceptual_loss: 3.3928 - lr: 1.0000e-04\n",
      "Epoch 53/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 14376.2391 - reconstruction_loss: 11836.6777 - kl_loss: 2391.8416 - perceptual_loss: 8.9227\n",
      "Epoch 53: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 14374.9887 - reconstruction_loss: 11836.6777 - kl_loss: 2391.8416 - perceptual_loss: 8.9227 - val_loss: 6100.4634 - val_reconstruction_loss: 5221.7944 - val_kl_loss: 875.3002 - val_perceptual_loss: 3.3689 - lr: 1.0000e-04\n",
      "Epoch 54/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 16152.5745 - reconstruction_loss: 13696.4150 - kl_loss: 2235.0042 - perceptual_loss: 8.7039\n",
      "Epoch 54: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 16150.6606 - reconstruction_loss: 13696.4150 - kl_loss: 2235.0042 - perceptual_loss: 8.7039 - val_loss: 5676.6875 - val_reconstruction_loss: 4838.6992 - val_kl_loss: 834.3336 - val_perceptual_loss: 3.6548 - lr: 1.0000e-04\n",
      "Epoch 55/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 15100.9572 - reconstruction_loss: 12871.9121 - kl_loss: 2270.5039 - perceptual_loss: 8.3654\n",
      "Epoch 55: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 15101.4061 - reconstruction_loss: 12871.9121 - kl_loss: 2270.5039 - perceptual_loss: 8.3654 - val_loss: 6443.0332 - val_reconstruction_loss: 5587.2593 - val_kl_loss: 852.3025 - val_perceptual_loss: 3.4715 - lr: 1.0000e-04\n",
      "Epoch 56/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 15960.0595 - reconstruction_loss: 15199.6260 - kl_loss: 2174.4910 - perceptual_loss: 8.7093\n",
      "Epoch 56: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 15972.8772 - reconstruction_loss: 15199.6260 - kl_loss: 2174.4910 - perceptual_loss: 8.7093 - val_loss: 8594.2666 - val_reconstruction_loss: 7815.6543 - val_kl_loss: 774.9504 - val_perceptual_loss: 3.6621 - lr: 1.0000e-04\n",
      "Epoch 57/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 19517.1157 - reconstruction_loss: 17180.9141 - kl_loss: 1987.2101 - perceptual_loss: 8.6829\n",
      "Epoch 57: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 19514.0498 - reconstruction_loss: 17180.9141 - kl_loss: 1987.2101 - perceptual_loss: 8.6829 - val_loss: 5829.1792 - val_reconstruction_loss: 5093.1016 - val_kl_loss: 732.3538 - val_perceptual_loss: 3.7239 - lr: 1.0000e-04\n",
      "Epoch 58/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 18989.3509 - reconstruction_loss: 16828.7871 - kl_loss: 2046.4325 - perceptual_loss: 8.5363\n",
      "Epoch 58: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 18988.3995 - reconstruction_loss: 16828.7871 - kl_loss: 2046.4325 - perceptual_loss: 8.5363 - val_loss: 9347.2646 - val_reconstruction_loss: 8610.6953 - val_kl_loss: 733.1033 - val_perceptual_loss: 3.4656 - lr: 1.0000e-04\n",
      "Epoch 59/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 17075.4042 - reconstruction_loss: 12950.3105 - kl_loss: 2239.9346 - perceptual_loss: 8.5949\n",
      "Epoch 59: val_reconstruction_loss did not improve from 4357.00195\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 17058.4982 - reconstruction_loss: 12950.3105 - kl_loss: 2239.9346 - perceptual_loss: 8.5949 - val_loss: 5889.1709 - val_reconstruction_loss: 5033.9922 - val_kl_loss: 850.7662 - val_perceptual_loss: 4.4127 - lr: 1.0000e-04\n",
      "Epoch 60/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 14399.2360 - reconstruction_loss: 11991.6348 - kl_loss: 2357.9397 - perceptual_loss: 8.8099\n",
      "Epoch 60: val_reconstruction_loss did not improve from 4357.00195\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 14398.8679 - reconstruction_loss: 11991.6348 - kl_loss: 2357.9397 - perceptual_loss: 8.8099 - val_loss: 6085.8081 - val_reconstruction_loss: 5202.9102 - val_kl_loss: 879.6055 - val_perceptual_loss: 3.2926 - lr: 1.0000e-04\n",
      "Epoch 61/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 12090.2106 - reconstruction_loss: 9492.9395 - kl_loss: 2460.6597 - perceptual_loss: 7.9518\n",
      "Epoch 61: val_reconstruction_loss improved from 4357.00195 to 3782.99487, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 174ms/step - loss: 12089.0515 - reconstruction_loss: 9492.9395 - kl_loss: 2460.6597 - perceptual_loss: 7.9518 - val_loss: 4722.5332 - val_reconstruction_loss: 3782.9949 - val_kl_loss: 936.3059 - val_perceptual_loss: 3.2321 - lr: 5.0000e-05\n",
      "Epoch 62/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 12065.9995 - reconstruction_loss: 9431.5283 - kl_loss: 2468.4202 - perceptual_loss: 7.8628\n",
      "Epoch 62: val_reconstruction_loss improved from 3782.99487 to 3529.32129, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 171ms/step - loss: 12064.5744 - reconstruction_loss: 9431.5283 - kl_loss: 2468.4202 - perceptual_loss: 7.8628 - val_loss: 4451.3008 - val_reconstruction_loss: 3529.3213 - val_kl_loss: 918.9296 - val_perceptual_loss: 3.0502 - lr: 5.0000e-05\n",
      "Epoch 63/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11595.9510 - reconstruction_loss: 8925.4453 - kl_loss: 2528.8279 - perceptual_loss: 7.7996\n",
      "Epoch 63: val_reconstruction_loss did not improve from 3529.32129\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 11594.7449 - reconstruction_loss: 8925.4453 - kl_loss: 2528.8279 - perceptual_loss: 7.7996 - val_loss: 6466.8818 - val_reconstruction_loss: 5521.3076 - val_kl_loss: 942.4568 - val_perceptual_loss: 3.1174 - lr: 5.0000e-05\n",
      "Epoch 64/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11647.4831 - reconstruction_loss: 9463.4023 - kl_loss: 2578.1047 - perceptual_loss: 7.7693\n",
      "Epoch 64: val_reconstruction_loss did not improve from 3529.32129\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 11651.1028 - reconstruction_loss: 9463.4023 - kl_loss: 2578.1047 - perceptual_loss: 7.7693 - val_loss: 6009.5723 - val_reconstruction_loss: 5048.7822 - val_kl_loss: 957.8356 - val_perceptual_loss: 2.9545 - lr: 5.0000e-05\n",
      "Epoch 65/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 13112.9820 - reconstruction_loss: 10739.2969 - kl_loss: 2449.4792 - perceptual_loss: 7.5932\n",
      "Epoch 65: val_reconstruction_loss did not improve from 3529.32129\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 13113.7332 - reconstruction_loss: 10739.2969 - kl_loss: 2449.4792 - perceptual_loss: 7.5932 - val_loss: 4564.9302 - val_reconstruction_loss: 3661.4397 - val_kl_loss: 900.3651 - val_perceptual_loss: 3.1251 - lr: 5.0000e-05\n",
      "Epoch 66/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 13020.2076 - reconstruction_loss: 10322.8818 - kl_loss: 2449.2571 - perceptual_loss: 7.6237\n",
      "Epoch 66: val_reconstruction_loss did not improve from 3529.32129\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 13018.0414 - reconstruction_loss: 10322.8818 - kl_loss: 2449.2571 - perceptual_loss: 7.6237 - val_loss: 4615.0752 - val_reconstruction_loss: 3678.5493 - val_kl_loss: 933.4416 - val_perceptual_loss: 3.0844 - lr: 5.0000e-05\n",
      "Epoch 67/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11285.3725 - reconstruction_loss: 8776.0889 - kl_loss: 2555.7319 - perceptual_loss: 7.5377\n",
      "Epoch 67: val_reconstruction_loss did not improve from 3529.32129\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 11285.8589 - reconstruction_loss: 8776.0889 - kl_loss: 2555.7319 - perceptual_loss: 7.5377 - val_loss: 4745.5830 - val_reconstruction_loss: 3780.8340 - val_kl_loss: 961.5151 - val_perceptual_loss: 3.2341 - lr: 5.0000e-05\n",
      "Epoch 68/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11604.8485 - reconstruction_loss: 8879.1240 - kl_loss: 2560.3462 - perceptual_loss: 7.5168\n",
      "Epoch 68: val_reconstruction_loss did not improve from 3529.32129\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 11603.4263 - reconstruction_loss: 8879.1240 - kl_loss: 2560.3462 - perceptual_loss: 7.5168 - val_loss: 4984.1855 - val_reconstruction_loss: 4042.6609 - val_kl_loss: 938.6649 - val_perceptual_loss: 2.8596 - lr: 5.0000e-05\n",
      "Epoch 69/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11546.3737 - reconstruction_loss: 8814.1436 - kl_loss: 2517.7461 - perceptual_loss: 7.3664\n",
      "Epoch 69: val_reconstruction_loss did not improve from 3529.32129\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 11544.5078 - reconstruction_loss: 8814.1436 - kl_loss: 2517.7461 - perceptual_loss: 7.3664 - val_loss: 5087.1797 - val_reconstruction_loss: 4111.8682 - val_kl_loss: 972.2909 - val_perceptual_loss: 3.0206 - lr: 5.0000e-05\n",
      "Epoch 70/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 12135.6795 - reconstruction_loss: 9315.7666 - kl_loss: 2509.5266 - perceptual_loss: 7.4494\n",
      "Epoch 70: val_reconstruction_loss improved from 3529.32129 to 3201.70679, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 171ms/step - loss: 12132.9504 - reconstruction_loss: 9315.7666 - kl_loss: 2509.5266 - perceptual_loss: 7.4494 - val_loss: 4132.5420 - val_reconstruction_loss: 3201.7068 - val_kl_loss: 927.8011 - val_perceptual_loss: 3.0342 - lr: 5.0000e-05\n",
      "Epoch 71/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10858.4176 - reconstruction_loss: 8674.8936 - kl_loss: 2495.9756 - perceptual_loss: 7.3159\n",
      "Epoch 71: val_reconstruction_loss did not improve from 3201.70679\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 10861.2984 - reconstruction_loss: 8674.8936 - kl_loss: 2495.9756 - perceptual_loss: 7.3159 - val_loss: 5136.5405 - val_reconstruction_loss: 4181.2051 - val_kl_loss: 952.4218 - val_perceptual_loss: 2.9134 - lr: 5.0000e-05\n",
      "Epoch 72/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11055.6747 - reconstruction_loss: 8688.7197 - kl_loss: 2487.3462 - perceptual_loss: 7.2488\n",
      "Epoch 72: val_reconstruction_loss did not improve from 3201.70679\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 11056.8246 - reconstruction_loss: 8688.7197 - kl_loss: 2487.3462 - perceptual_loss: 7.2488 - val_loss: 4580.8984 - val_reconstruction_loss: 3654.0000 - val_kl_loss: 923.7624 - val_perceptual_loss: 3.1359 - lr: 5.0000e-05\n",
      "Epoch 73/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10905.2837 - reconstruction_loss: 8531.0840 - kl_loss: 2564.6802 - perceptual_loss: 7.2536\n",
      "Epoch 73: val_reconstruction_loss did not improve from 3201.70679\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 10907.0650 - reconstruction_loss: 8531.0840 - kl_loss: 2564.6802 - perceptual_loss: 7.2536 - val_loss: 4293.4424 - val_reconstruction_loss: 3333.0269 - val_kl_loss: 957.3965 - val_perceptual_loss: 3.0190 - lr: 5.0000e-05\n",
      "Epoch 74/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11579.7315 - reconstruction_loss: 8865.3662 - kl_loss: 2531.4658 - perceptual_loss: 7.1918\n",
      "Epoch 74: val_reconstruction_loss did not improve from 3201.70679\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 11578.1486 - reconstruction_loss: 8865.3662 - kl_loss: 2531.4658 - perceptual_loss: 7.1918 - val_loss: 5108.0020 - val_reconstruction_loss: 4128.3584 - val_kl_loss: 976.4610 - val_perceptual_loss: 3.1827 - lr: 5.0000e-05\n",
      "Epoch 75/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11634.7853 - reconstruction_loss: 8852.7129 - kl_loss: 2494.2600 - perceptual_loss: 7.1372\n",
      "Epoch 75: val_reconstruction_loss did not improve from 3201.70679\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 11632.2567 - reconstruction_loss: 8852.7129 - kl_loss: 2494.2600 - perceptual_loss: 7.1372 - val_loss: 5302.3638 - val_reconstruction_loss: 4361.0991 - val_kl_loss: 938.3729 - val_perceptual_loss: 2.8916 - lr: 5.0000e-05\n",
      "Epoch 76/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11065.9415 - reconstruction_loss: 8530.0713 - kl_loss: 2474.1680 - perceptual_loss: 7.0091\n",
      "Epoch 76: val_reconstruction_loss did not improve from 3201.70679\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 11065.4488 - reconstruction_loss: 8530.0713 - kl_loss: 2474.1680 - perceptual_loss: 7.0091 - val_loss: 4527.7158 - val_reconstruction_loss: 3612.0933 - val_kl_loss: 912.5687 - val_perceptual_loss: 3.0537 - lr: 5.0000e-05\n",
      "Epoch 77/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11049.4792 - reconstruction_loss: 8460.5371 - kl_loss: 2539.3369 - perceptual_loss: 7.0031\n",
      "Epoch 77: val_reconstruction_loss did not improve from 3201.70679\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 11049.0954 - reconstruction_loss: 8460.5371 - kl_loss: 2539.3369 - perceptual_loss: 7.0031 - val_loss: 4232.9131 - val_reconstruction_loss: 3295.9099 - val_kl_loss: 933.9678 - val_perceptual_loss: 3.0354 - lr: 5.0000e-05\n",
      "Epoch 78/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10928.5951 - reconstruction_loss: 8430.5430 - kl_loss: 2506.8745 - perceptual_loss: 6.9172\n",
      "Epoch 78: val_reconstruction_loss did not improve from 3201.70679\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 10928.7369 - reconstruction_loss: 8430.5430 - kl_loss: 2506.8745 - perceptual_loss: 6.9172 - val_loss: 4971.1899 - val_reconstruction_loss: 4008.1785 - val_kl_loss: 959.7376 - val_perceptual_loss: 3.2740 - lr: 5.0000e-05\n",
      "Epoch 79/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10894.3195 - reconstruction_loss: 8350.3096 - kl_loss: 2486.5178 - perceptual_loss: 6.9107\n",
      "Epoch 79: val_reconstruction_loss did not improve from 3201.70679\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 10893.8638 - reconstruction_loss: 8350.3096 - kl_loss: 2486.5178 - perceptual_loss: 6.9107 - val_loss: 4543.9990 - val_reconstruction_loss: 3632.3706 - val_kl_loss: 908.5576 - val_perceptual_loss: 3.0705 - lr: 5.0000e-05\n",
      "Epoch 80/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10493.7839 - reconstruction_loss: 8365.0186 - kl_loss: 2444.9846 - perceptual_loss: 6.9294\n",
      "Epoch 80: val_reconstruction_loss improved from 3201.70679 to 3179.98975, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 20s 177ms/step - loss: 10496.6952 - reconstruction_loss: 8365.0186 - kl_loss: 2444.9846 - perceptual_loss: 6.9294 - val_loss: 4085.9165 - val_reconstruction_loss: 3179.9897 - val_kl_loss: 903.0545 - val_perceptual_loss: 2.8723 - lr: 5.0000e-05\n",
      "Epoch 81/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10805.1569 - reconstruction_loss: 8443.5635 - kl_loss: 2453.3994 - perceptual_loss: 6.9045\n",
      "Epoch 81: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 10806.0462 - reconstruction_loss: 8443.5635 - kl_loss: 2453.3994 - perceptual_loss: 6.9045 - val_loss: 4293.4800 - val_reconstruction_loss: 3369.3320 - val_kl_loss: 921.0001 - val_perceptual_loss: 3.1479 - lr: 5.0000e-05\n",
      "Epoch 82/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10858.7486 - reconstruction_loss: 8344.5830 - kl_loss: 2445.3547 - perceptual_loss: 6.8426\n",
      "Epoch 82: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 10858.1903 - reconstruction_loss: 8344.5830 - kl_loss: 2445.3547 - perceptual_loss: 6.8426 - val_loss: 4513.7139 - val_reconstruction_loss: 3609.0713 - val_kl_loss: 901.4623 - val_perceptual_loss: 3.1802 - lr: 5.0000e-05\n",
      "Epoch 83/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10983.6705 - reconstruction_loss: 8445.7705 - kl_loss: 2434.2063 - perceptual_loss: 6.8259\n",
      "Epoch 83: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 10982.7978 - reconstruction_loss: 8445.7705 - kl_loss: 2434.2063 - perceptual_loss: 6.8259 - val_loss: 4365.4199 - val_reconstruction_loss: 3462.8828 - val_kl_loss: 899.5820 - val_perceptual_loss: 2.9550 - lr: 5.0000e-05\n",
      "Epoch 84/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10608.3760 - reconstruction_loss: 8533.8467 - kl_loss: 2431.2754 - perceptual_loss: 6.9178\n",
      "Epoch 84: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 10611.6523 - reconstruction_loss: 8533.8467 - kl_loss: 2431.2754 - perceptual_loss: 6.9178 - val_loss: 4663.1440 - val_reconstruction_loss: 3762.2051 - val_kl_loss: 898.0238 - val_perceptual_loss: 2.9153 - lr: 5.0000e-05\n",
      "Epoch 85/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11020.1945 - reconstruction_loss: 8791.1201 - kl_loss: 2432.5039 - perceptual_loss: 6.8031\n",
      "Epoch 85: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 11022.0885 - reconstruction_loss: 8791.1201 - kl_loss: 2432.5039 - perceptual_loss: 6.8031 - val_loss: 4529.3770 - val_reconstruction_loss: 3611.5837 - val_kl_loss: 914.6754 - val_perceptual_loss: 3.1179 - lr: 5.0000e-05\n",
      "Epoch 86/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 12715.0101 - reconstruction_loss: 10212.7803 - kl_loss: 2369.2336 - perceptual_loss: 6.7709\n",
      "Epoch 86: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 12713.8730 - reconstruction_loss: 10212.7803 - kl_loss: 2369.2336 - perceptual_loss: 6.7709 - val_loss: 5613.3584 - val_reconstruction_loss: 4756.1074 - val_kl_loss: 854.0123 - val_perceptual_loss: 3.2386 - lr: 5.0000e-05\n",
      "Epoch 87/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 12104.2376 - reconstruction_loss: 8986.9980 - kl_loss: 2356.7605 - perceptual_loss: 6.8401\n",
      "Epoch 87: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 12097.4480 - reconstruction_loss: 8986.9980 - kl_loss: 2356.7605 - perceptual_loss: 6.8401 - val_loss: 4938.1514 - val_reconstruction_loss: 4052.6333 - val_kl_loss: 882.5740 - val_perceptual_loss: 2.9441 - lr: 5.0000e-05\n",
      "Epoch 88/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11461.1796 - reconstruction_loss: 8845.4453 - kl_loss: 2360.3735 - perceptual_loss: 6.9394\n",
      "Epoch 88: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 11458.9415 - reconstruction_loss: 8845.4453 - kl_loss: 2360.3735 - perceptual_loss: 6.9394 - val_loss: 4399.9600 - val_reconstruction_loss: 3510.9741 - val_kl_loss: 885.8684 - val_perceptual_loss: 3.1176 - lr: 5.0000e-05\n",
      "Epoch 89/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10724.5417 - reconstruction_loss: 8233.3711 - kl_loss: 2354.2209 - perceptual_loss: 6.7882\n",
      "Epoch 89: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 10723.3691 - reconstruction_loss: 8233.3711 - kl_loss: 2354.2209 - perceptual_loss: 6.7882 - val_loss: 4600.0825 - val_reconstruction_loss: 3720.2090 - val_kl_loss: 876.8119 - val_perceptual_loss: 3.0617 - lr: 5.0000e-05\n",
      "Epoch 90/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10597.0204 - reconstruction_loss: 8353.3555 - kl_loss: 2373.7498 - perceptual_loss: 6.7592\n",
      "Epoch 90: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 10598.2533 - reconstruction_loss: 8353.3555 - kl_loss: 2373.7498 - perceptual_loss: 6.7592 - val_loss: 4344.4097 - val_reconstruction_loss: 3442.1792 - val_kl_loss: 898.9346 - val_perceptual_loss: 3.2959 - lr: 5.0000e-05\n",
      "Epoch 91/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11161.5602 - reconstruction_loss: 8786.9834 - kl_loss: 2384.8799 - perceptual_loss: 6.8692\n",
      "Epoch 91: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 11161.7149 - reconstruction_loss: 8786.9834 - kl_loss: 2384.8799 - perceptual_loss: 6.8692 - val_loss: 4556.8276 - val_reconstruction_loss: 3684.1362 - val_kl_loss: 869.9091 - val_perceptual_loss: 2.7825 - lr: 5.0000e-05\n",
      "Epoch 92/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11037.2697 - reconstruction_loss: 8458.7090 - kl_loss: 2344.7229 - perceptual_loss: 6.6986\n",
      "Epoch 92: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 11035.2234 - reconstruction_loss: 8458.7090 - kl_loss: 2344.7229 - perceptual_loss: 6.6986 - val_loss: 4580.7393 - val_reconstruction_loss: 3705.6902 - val_kl_loss: 871.9636 - val_perceptual_loss: 3.0855 - lr: 5.0000e-05\n",
      "Epoch 93/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10448.1722 - reconstruction_loss: 8124.5186 - kl_loss: 2354.9666 - perceptual_loss: 6.6164\n",
      "Epoch 93: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 166ms/step - loss: 10448.5140 - reconstruction_loss: 8124.5186 - kl_loss: 2354.9666 - perceptual_loss: 6.6164 - val_loss: 4559.3560 - val_reconstruction_loss: 3655.3567 - val_kl_loss: 900.9014 - val_perceptual_loss: 3.0979 - lr: 5.0000e-05\n",
      "Epoch 94/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10540.3401 - reconstruction_loss: 8195.5010 - kl_loss: 2367.1738 - perceptual_loss: 6.5863\n",
      "Epoch 94: val_reconstruction_loss did not improve from 3179.98975\n",
      "110/110 [==============================] - 18s 166ms/step - loss: 10540.6006 - reconstruction_loss: 8195.5010 - kl_loss: 2367.1738 - perceptual_loss: 6.5863 - val_loss: 4799.2988 - val_reconstruction_loss: 3910.4409 - val_kl_loss: 885.8606 - val_perceptual_loss: 2.9977 - lr: 5.0000e-05\n",
      "Epoch 95/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10337.1419 - reconstruction_loss: 8229.6963 - kl_loss: 2350.6211 - perceptual_loss: 6.5632\n",
      "Epoch 95: val_reconstruction_loss improved from 3179.98975 to 3113.76587, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 20s 176ms/step - loss: 10339.3918 - reconstruction_loss: 8229.6963 - kl_loss: 2350.6211 - perceptual_loss: 6.5632 - val_loss: 3987.3794 - val_reconstruction_loss: 3113.7659 - val_kl_loss: 870.5454 - val_perceptual_loss: 3.0682 - lr: 5.0000e-05\n",
      "Epoch 96/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10337.0131 - reconstruction_loss: 8120.8828 - kl_loss: 2349.2358 - perceptual_loss: 6.5666\n",
      "Epoch 96: val_reconstruction_loss did not improve from 3113.76587\n",
      "110/110 [==============================] - 18s 165ms/step - loss: 10338.2714 - reconstruction_loss: 8120.8828 - kl_loss: 2349.2358 - perceptual_loss: 6.5666 - val_loss: 4924.8398 - val_reconstruction_loss: 4032.0732 - val_kl_loss: 889.7802 - val_perceptual_loss: 2.9864 - lr: 5.0000e-05\n",
      "Epoch 97/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10556.6788 - reconstruction_loss: 8207.1650 - kl_loss: 2352.0881 - perceptual_loss: 6.5194\n",
      "Epoch 97: val_reconstruction_loss did not improve from 3113.76587\n",
      "110/110 [==============================] - 18s 166ms/step - loss: 10556.7608 - reconstruction_loss: 8207.1650 - kl_loss: 2352.0881 - perceptual_loss: 6.5194 - val_loss: 4718.8496 - val_reconstruction_loss: 3837.8247 - val_kl_loss: 878.1472 - val_perceptual_loss: 2.8774 - lr: 5.0000e-05\n",
      "Epoch 98/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10410.2785 - reconstruction_loss: 8163.9131 - kl_loss: 2356.7258 - perceptual_loss: 6.4971\n",
      "Epoch 98: val_reconstruction_loss did not improve from 3113.76587\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 10411.3313 - reconstruction_loss: 8163.9131 - kl_loss: 2356.7258 - perceptual_loss: 6.4971 - val_loss: 4542.3027 - val_reconstruction_loss: 3660.6008 - val_kl_loss: 878.8048 - val_perceptual_loss: 2.8973 - lr: 5.0000e-05\n",
      "Epoch 99/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 11154.7319 - reconstruction_loss: 8693.5293 - kl_loss: 2323.1890 - perceptual_loss: 6.5920\n",
      "Epoch 99: val_reconstruction_loss did not improve from 3113.76587\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 11153.5478 - reconstruction_loss: 8693.5293 - kl_loss: 2323.1890 - perceptual_loss: 6.5920 - val_loss: 4139.2988 - val_reconstruction_loss: 3263.7646 - val_kl_loss: 872.4973 - val_perceptual_loss: 3.0367 - lr: 5.0000e-05\n",
      "Epoch 100/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10464.3762 - reconstruction_loss: 7893.1279 - kl_loss: 2352.0029 - perceptual_loss: 6.5156\n",
      "Epoch 100: val_reconstruction_loss improved from 3113.76587 to 3108.63843, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 173ms/step - loss: 10462.4597 - reconstruction_loss: 7893.1279 - kl_loss: 2352.0029 - perceptual_loss: 6.5156 - val_loss: 4049.8928 - val_reconstruction_loss: 3108.6384 - val_kl_loss: 938.3837 - val_perceptual_loss: 2.8705 - lr: 5.0000e-05\n",
      "Epoch 101/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10274.3593 - reconstruction_loss: 8087.9561 - kl_loss: 2413.3137 - perceptual_loss: 6.4997\n",
      "Epoch 101: val_reconstruction_loss did not improve from 3108.63843\n",
      "110/110 [==============================] - 18s 163ms/step - loss: 10276.4621 - reconstruction_loss: 8087.9561 - kl_loss: 2413.3137 - perceptual_loss: 6.4997 - val_loss: 4737.4927 - val_reconstruction_loss: 3865.2812 - val_kl_loss: 869.2462 - val_perceptual_loss: 2.9653 - lr: 5.0000e-05\n",
      "Epoch 102/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10477.9796 - reconstruction_loss: 8049.7603 - kl_loss: 2370.2981 - perceptual_loss: 6.5210\n",
      "Epoch 102: val_reconstruction_loss did not improve from 3108.63843\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 10477.5165 - reconstruction_loss: 8049.7603 - kl_loss: 2370.2981 - perceptual_loss: 6.5210 - val_loss: 4321.4536 - val_reconstruction_loss: 3430.5439 - val_kl_loss: 887.5966 - val_perceptual_loss: 3.3131 - lr: 5.0000e-05\n",
      "Epoch 103/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10520.5108 - reconstruction_loss: 8115.2979 - kl_loss: 2360.1538 - perceptual_loss: 6.3479\n",
      "Epoch 103: val_reconstruction_loss did not improve from 3108.63843\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 10520.1621 - reconstruction_loss: 8115.2979 - kl_loss: 2360.1538 - perceptual_loss: 6.3479 - val_loss: 4114.9224 - val_reconstruction_loss: 3229.2202 - val_kl_loss: 882.8141 - val_perceptual_loss: 2.8878 - lr: 5.0000e-05\n",
      "Epoch 104/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10296.1684 - reconstruction_loss: 8214.5498 - kl_loss: 2357.5020 - perceptual_loss: 6.3539\n",
      "Epoch 104: val_reconstruction_loss did not improve from 3108.63843\n",
      "110/110 [==============================] - 18s 157ms/step - loss: 10298.7111 - reconstruction_loss: 8214.5498 - kl_loss: 2357.5020 - perceptual_loss: 6.3539 - val_loss: 4702.4966 - val_reconstruction_loss: 3823.1025 - val_kl_loss: 876.3929 - val_perceptual_loss: 3.0012 - lr: 5.0000e-05\n",
      "Epoch 105/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10373.6905 - reconstruction_loss: 7935.5591 - kl_loss: 2358.3640 - perceptual_loss: 6.2461\n",
      "Epoch 105: val_reconstruction_loss did not improve from 3108.63843\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 10373.0281 - reconstruction_loss: 7935.5591 - kl_loss: 2358.3640 - perceptual_loss: 6.2461 - val_loss: 4114.2642 - val_reconstruction_loss: 3231.2344 - val_kl_loss: 879.9330 - val_perceptual_loss: 3.0967 - lr: 5.0000e-05\n",
      "Epoch 106/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10105.6767 - reconstruction_loss: 7966.3574 - kl_loss: 2332.3989 - perceptual_loss: 6.2434\n",
      "Epoch 106: val_reconstruction_loss improved from 3108.63843 to 3081.54028, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 168ms/step - loss: 10107.4724 - reconstruction_loss: 7966.3574 - kl_loss: 2332.3989 - perceptual_loss: 6.2434 - val_loss: 3959.0762 - val_reconstruction_loss: 3081.5403 - val_kl_loss: 874.5275 - val_perceptual_loss: 3.0085 - lr: 5.0000e-05\n",
      "Epoch 107/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10310.4073 - reconstruction_loss: 7884.4102 - kl_loss: 2331.1838 - perceptual_loss: 6.2385\n",
      "Epoch 107: val_reconstruction_loss did not improve from 3081.54028\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 10309.6094 - reconstruction_loss: 7884.4102 - kl_loss: 2331.1838 - perceptual_loss: 6.2385 - val_loss: 4358.5986 - val_reconstruction_loss: 3463.5811 - val_kl_loss: 892.0584 - val_perceptual_loss: 2.9591 - lr: 5.0000e-05\n",
      "Epoch 108/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9899.5260 - reconstruction_loss: 7862.1357 - kl_loss: 2378.3901 - perceptual_loss: 6.4947\n",
      "Epoch 108: val_reconstruction_loss did not improve from 3081.54028\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 9902.6565 - reconstruction_loss: 7862.1357 - kl_loss: 2378.3901 - perceptual_loss: 6.4947 - val_loss: 4279.4014 - val_reconstruction_loss: 3392.6089 - val_kl_loss: 883.9617 - val_perceptual_loss: 2.8309 - lr: 5.0000e-05\n",
      "Epoch 109/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9987.0410 - reconstruction_loss: 7855.4272 - kl_loss: 2321.8147 - perceptual_loss: 6.2511\n",
      "Epoch 109: val_reconstruction_loss did not improve from 3081.54028\n",
      "110/110 [==============================] - 18s 164ms/step - loss: 9988.8109 - reconstruction_loss: 7855.4272 - kl_loss: 2321.8147 - perceptual_loss: 6.2511 - val_loss: 4284.6836 - val_reconstruction_loss: 3417.7715 - val_kl_loss: 863.9974 - val_perceptual_loss: 2.9147 - lr: 5.0000e-05\n",
      "Epoch 110/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9967.6796 - reconstruction_loss: 7758.7388 - kl_loss: 2373.0054 - perceptual_loss: 6.2683\n",
      "Epoch 110: val_reconstruction_loss did not improve from 3081.54028\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 9969.2141 - reconstruction_loss: 7758.7388 - kl_loss: 2373.0054 - perceptual_loss: 6.2683 - val_loss: 4331.7798 - val_reconstruction_loss: 3463.7617 - val_kl_loss: 865.2447 - val_perceptual_loss: 2.7733 - lr: 5.0000e-05\n",
      "Epoch 111/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10498.4381 - reconstruction_loss: 8053.4937 - kl_loss: 2378.5044 - perceptual_loss: 6.2572\n",
      "Epoch 111: val_reconstruction_loss improved from 3081.54028 to 3056.54736, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 168ms/step - loss: 10497.8959 - reconstruction_loss: 8053.4937 - kl_loss: 2378.5044 - perceptual_loss: 6.2572 - val_loss: 3948.4194 - val_reconstruction_loss: 3056.5474 - val_kl_loss: 888.8557 - val_perceptual_loss: 3.0162 - lr: 5.0000e-05\n",
      "Epoch 112/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10262.4298 - reconstruction_loss: 7717.3911 - kl_loss: 2344.4578 - perceptual_loss: 6.0959\n",
      "Epoch 112: val_reconstruction_loss did not improve from 3056.54736\n",
      "110/110 [==============================] - 20s 179ms/step - loss: 10260.6777 - reconstruction_loss: 7717.3911 - kl_loss: 2344.4578 - perceptual_loss: 6.0959 - val_loss: 4288.8530 - val_reconstruction_loss: 3398.6309 - val_kl_loss: 887.3596 - val_perceptual_loss: 2.8629 - lr: 5.0000e-05\n",
      "Epoch 113/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10314.3691 - reconstruction_loss: 7770.8545 - kl_loss: 2358.1340 - perceptual_loss: 6.1048\n",
      "Epoch 113: val_reconstruction_loss did not improve from 3056.54736\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 10312.7540 - reconstruction_loss: 7770.8545 - kl_loss: 2358.1340 - perceptual_loss: 6.1048 - val_loss: 4317.3052 - val_reconstruction_loss: 3446.8477 - val_kl_loss: 867.3571 - val_perceptual_loss: 3.1006 - lr: 5.0000e-05\n",
      "Epoch 114/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10174.8792 - reconstruction_loss: 7609.3472 - kl_loss: 2340.6694 - perceptual_loss: 6.0801\n",
      "Epoch 114: val_reconstruction_loss improved from 3056.54736 to 2942.71802, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 168ms/step - loss: 10172.9082 - reconstruction_loss: 7609.3472 - kl_loss: 2340.6694 - perceptual_loss: 6.0801 - val_loss: 3849.6821 - val_reconstruction_loss: 2942.7180 - val_kl_loss: 904.1801 - val_perceptual_loss: 2.7842 - lr: 5.0000e-05\n",
      "Epoch 115/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9364.7440 - reconstruction_loss: 7466.6250 - kl_loss: 2468.9004 - perceptual_loss: 6.1729\n",
      "Epoch 115: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 9369.9418 - reconstruction_loss: 7466.6250 - kl_loss: 2468.9004 - perceptual_loss: 6.1729 - val_loss: 4491.0645 - val_reconstruction_loss: 3590.3352 - val_kl_loss: 897.6917 - val_perceptual_loss: 3.0376 - lr: 5.0000e-05\n",
      "Epoch 116/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10715.7365 - reconstruction_loss: 7739.4688 - kl_loss: 2377.9675 - perceptual_loss: 6.1253\n",
      "Epoch 116: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 10710.4016 - reconstruction_loss: 7739.4688 - kl_loss: 2377.9675 - perceptual_loss: 6.1253 - val_loss: 3955.3142 - val_reconstruction_loss: 3053.0679 - val_kl_loss: 899.3301 - val_perceptual_loss: 2.9163 - lr: 5.0000e-05\n",
      "Epoch 117/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9998.4004 - reconstruction_loss: 7815.5312 - kl_loss: 2349.5681 - perceptual_loss: 6.0497\n",
      "Epoch 117: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 9999.9567 - reconstruction_loss: 7815.5312 - kl_loss: 2349.5681 - perceptual_loss: 6.0497 - val_loss: 4943.5469 - val_reconstruction_loss: 4079.6978 - val_kl_loss: 860.9777 - val_perceptual_loss: 2.8715 - lr: 5.0000e-05\n",
      "Epoch 118/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9875.7617 - reconstruction_loss: 7629.0840 - kl_loss: 2325.4204 - perceptual_loss: 5.9944\n",
      "Epoch 118: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 9876.5251 - reconstruction_loss: 7629.0840 - kl_loss: 2325.4204 - perceptual_loss: 5.9944 - val_loss: 4816.3379 - val_reconstruction_loss: 3912.6494 - val_kl_loss: 900.5227 - val_perceptual_loss: 3.1658 - lr: 5.0000e-05\n",
      "Epoch 119/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9971.2479 - reconstruction_loss: 7623.5459 - kl_loss: 2320.7942 - perceptual_loss: 5.9651\n",
      "Epoch 119: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 9971.0592 - reconstruction_loss: 7623.5459 - kl_loss: 2320.7942 - perceptual_loss: 5.9651 - val_loss: 4150.4907 - val_reconstruction_loss: 3280.8479 - val_kl_loss: 866.8300 - val_perceptual_loss: 2.8130 - lr: 5.0000e-05\n",
      "Epoch 120/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9789.8590 - reconstruction_loss: 7626.3032 - kl_loss: 2306.0732 - perceptual_loss: 5.9086\n",
      "Epoch 120: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 9791.1961 - reconstruction_loss: 7626.3032 - kl_loss: 2306.0732 - perceptual_loss: 5.9086 - val_loss: 4507.7261 - val_reconstruction_loss: 3639.7778 - val_kl_loss: 865.2097 - val_perceptual_loss: 2.7385 - lr: 5.0000e-05\n",
      "Epoch 121/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9905.7954 - reconstruction_loss: 7698.2002 - kl_loss: 2301.8745 - perceptual_loss: 5.9467\n",
      "Epoch 121: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 9906.6983 - reconstruction_loss: 7698.2002 - kl_loss: 2301.8745 - perceptual_loss: 5.9467 - val_loss: 4537.9692 - val_reconstruction_loss: 3656.7041 - val_kl_loss: 878.3783 - val_perceptual_loss: 2.8870 - lr: 5.0000e-05\n",
      "Epoch 122/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9943.5270 - reconstruction_loss: 7579.9673 - kl_loss: 2285.7253 - perceptual_loss: 5.9036\n",
      "Epoch 122: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 9942.8790 - reconstruction_loss: 7579.9673 - kl_loss: 2285.7253 - perceptual_loss: 5.9036 - val_loss: 3897.0032 - val_reconstruction_loss: 3035.1841 - val_kl_loss: 858.8868 - val_perceptual_loss: 2.9324 - lr: 5.0000e-05\n",
      "Epoch 123/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9989.2760 - reconstruction_loss: 7576.0508 - kl_loss: 2304.4751 - perceptual_loss: 5.8543\n",
      "Epoch 123: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 9988.3490 - reconstruction_loss: 7576.0508 - kl_loss: 2304.4751 - perceptual_loss: 5.8543 - val_loss: 4056.6890 - val_reconstruction_loss: 3201.7173 - val_kl_loss: 851.9636 - val_perceptual_loss: 3.0079 - lr: 5.0000e-05\n",
      "Epoch 124/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10195.1848 - reconstruction_loss: 7703.6558 - kl_loss: 2285.7659 - perceptual_loss: 5.8998\n",
      "Epoch 124: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 10193.3842 - reconstruction_loss: 7703.6558 - kl_loss: 2285.7659 - perceptual_loss: 5.8998 - val_loss: 4347.7036 - val_reconstruction_loss: 3473.8828 - val_kl_loss: 870.8130 - val_perceptual_loss: 3.0077 - lr: 5.0000e-05\n",
      "Epoch 125/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9711.3051 - reconstruction_loss: 7566.5757 - kl_loss: 2280.4836 - perceptual_loss: 5.8518\n",
      "Epoch 125: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 9712.5808 - reconstruction_loss: 7566.5757 - kl_loss: 2280.4836 - perceptual_loss: 5.8518 - val_loss: 4389.8599 - val_reconstruction_loss: 3555.9016 - val_kl_loss: 830.9802 - val_perceptual_loss: 2.9780 - lr: 5.0000e-05\n",
      "Epoch 126/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9839.8383 - reconstruction_loss: 7573.3516 - kl_loss: 2255.9016 - perceptual_loss: 5.8289\n",
      "Epoch 126: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 9839.7955 - reconstruction_loss: 7573.3516 - kl_loss: 2255.9016 - perceptual_loss: 5.8289 - val_loss: 4160.0054 - val_reconstruction_loss: 3322.7656 - val_kl_loss: 834.1967 - val_perceptual_loss: 3.0430 - lr: 5.0000e-05\n",
      "Epoch 127/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9926.0613 - reconstruction_loss: 7571.9692 - kl_loss: 2248.3579 - perceptual_loss: 5.8796\n",
      "Epoch 127: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 9925.1617 - reconstruction_loss: 7571.9692 - kl_loss: 2248.3579 - perceptual_loss: 5.8796 - val_loss: 3963.4241 - val_reconstruction_loss: 3107.5278 - val_kl_loss: 852.8975 - val_perceptual_loss: 2.9987 - lr: 5.0000e-05\n",
      "Epoch 128/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9698.1289 - reconstruction_loss: 7439.2231 - kl_loss: 2276.1733 - perceptual_loss: 5.9335\n",
      "Epoch 128: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 9698.3379 - reconstruction_loss: 7439.2231 - kl_loss: 2276.1733 - perceptual_loss: 5.9335 - val_loss: 3912.1084 - val_reconstruction_loss: 3058.2219 - val_kl_loss: 850.9652 - val_perceptual_loss: 2.9214 - lr: 5.0000e-05\n",
      "Epoch 129/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10309.2383 - reconstruction_loss: 7705.4727 - kl_loss: 2262.6360 - perceptual_loss: 5.9782\n",
      "Epoch 129: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 10306.2189 - reconstruction_loss: 7705.4727 - kl_loss: 2262.6360 - perceptual_loss: 5.9782 - val_loss: 4198.4590 - val_reconstruction_loss: 3324.9634 - val_kl_loss: 870.7195 - val_perceptual_loss: 2.7759 - lr: 5.0000e-05\n",
      "Epoch 130/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 10345.9703 - reconstruction_loss: 7758.7290 - kl_loss: 2224.5383 - perceptual_loss: 5.9033\n",
      "Epoch 130: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 10342.7559 - reconstruction_loss: 7758.7290 - kl_loss: 2224.5383 - perceptual_loss: 5.9033 - val_loss: 4190.0776 - val_reconstruction_loss: 3358.9248 - val_kl_loss: 828.0587 - val_perceptual_loss: 3.0939 - lr: 5.0000e-05\n",
      "Epoch 131/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9550.5255 - reconstruction_loss: 7529.1626 - kl_loss: 2230.7834 - perceptual_loss: 5.7764\n",
      "Epoch 131: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 17s 158ms/step - loss: 9552.4642 - reconstruction_loss: 7529.1626 - kl_loss: 2230.7834 - perceptual_loss: 5.7764 - val_loss: 4503.8931 - val_reconstruction_loss: 3651.9426 - val_kl_loss: 849.0801 - val_perceptual_loss: 2.8703 - lr: 5.0000e-05\n",
      "Epoch 132/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9631.0619 - reconstruction_loss: 7522.1870 - kl_loss: 2235.4458 - perceptual_loss: 5.7369\n",
      "Epoch 132: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 9632.2538 - reconstruction_loss: 7522.1870 - kl_loss: 2235.4458 - perceptual_loss: 5.7369 - val_loss: 4188.2124 - val_reconstruction_loss: 3356.0615 - val_kl_loss: 829.3224 - val_perceptual_loss: 2.8284 - lr: 5.0000e-05\n",
      "Epoch 133/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9746.3313 - reconstruction_loss: 7556.2305 - kl_loss: 2218.0723 - perceptual_loss: 5.7124\n",
      "Epoch 133: val_reconstruction_loss did not improve from 2942.71802\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 9746.6347 - reconstruction_loss: 7556.2305 - kl_loss: 2218.0723 - perceptual_loss: 5.7124 - val_loss: 3939.9675 - val_reconstruction_loss: 3116.5933 - val_kl_loss: 820.4263 - val_perceptual_loss: 2.9480 - lr: 5.0000e-05\n",
      "Epoch 134/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9515.2035 - reconstruction_loss: 7419.6226 - kl_loss: 2228.8379 - perceptual_loss: 5.6780\n",
      "Epoch 134: val_reconstruction_loss did not improve from 2942.71802\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 9516.4551 - reconstruction_loss: 7419.6226 - kl_loss: 2228.8379 - perceptual_loss: 5.6780 - val_loss: 4287.0269 - val_reconstruction_loss: 3433.0515 - val_kl_loss: 851.2103 - val_perceptual_loss: 2.7649 - lr: 5.0000e-05\n",
      "Epoch 135/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9398.8304 - reconstruction_loss: 6731.9590 - kl_loss: 2262.8665 - perceptual_loss: 5.7294\n",
      "Epoch 135: val_reconstruction_loss improved from 2942.71802 to 2823.37695, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 169ms/step - loss: 9395.2423 - reconstruction_loss: 6731.9590 - kl_loss: 2262.8665 - perceptual_loss: 5.7294 - val_loss: 3695.3506 - val_reconstruction_loss: 2823.3770 - val_kl_loss: 869.0677 - val_perceptual_loss: 2.9060 - lr: 2.5000e-05\n",
      "Epoch 136/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8994.3646 - reconstruction_loss: 6487.0972 - kl_loss: 2338.3521 - perceptual_loss: 5.6619\n",
      "Epoch 136: val_reconstruction_loss did not improve from 2823.37695\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 8992.8938 - reconstruction_loss: 6487.0972 - kl_loss: 2338.3521 - perceptual_loss: 5.6619 - val_loss: 3866.9907 - val_reconstruction_loss: 2977.3926 - val_kl_loss: 886.6299 - val_perceptual_loss: 2.9683 - lr: 2.5000e-05\n",
      "Epoch 137/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8487.6091 - reconstruction_loss: 6114.1001 - kl_loss: 2319.2046 - perceptual_loss: 5.6103\n",
      "Epoch 137: val_reconstruction_loss improved from 2823.37695 to 2684.62427, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 169ms/step - loss: 8487.1704 - reconstruction_loss: 6114.1001 - kl_loss: 2319.2046 - perceptual_loss: 5.6103 - val_loss: 3564.0547 - val_reconstruction_loss: 2684.6243 - val_kl_loss: 876.5381 - val_perceptual_loss: 2.8922 - lr: 2.5000e-05\n",
      "Epoch 138/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8810.8548 - reconstruction_loss: 6500.6479 - kl_loss: 2344.8181 - perceptual_loss: 5.6172\n",
      "Epoch 138: val_reconstruction_loss did not improve from 2684.62427\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 8811.2172 - reconstruction_loss: 6500.6479 - kl_loss: 2344.8181 - perceptual_loss: 5.6172 - val_loss: 3624.4941 - val_reconstruction_loss: 2731.2856 - val_kl_loss: 890.2963 - val_perceptual_loss: 2.9121 - lr: 2.5000e-05\n",
      "Epoch 139/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8447.3154 - reconstruction_loss: 6073.3413 - kl_loss: 2376.0518 - perceptual_loss: 5.5861\n",
      "Epoch 139: val_reconstruction_loss did not improve from 2684.62427\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 8447.3844 - reconstruction_loss: 6073.3413 - kl_loss: 2376.0518 - perceptual_loss: 5.5861 - val_loss: 3637.3567 - val_reconstruction_loss: 2745.7810 - val_kl_loss: 888.8185 - val_perceptual_loss: 2.7571 - lr: 2.5000e-05\n",
      "Epoch 140/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8525.4515 - reconstruction_loss: 6063.3130 - kl_loss: 2376.1411 - perceptual_loss: 5.5405\n",
      "Epoch 140: val_reconstruction_loss did not improve from 2684.62427\n",
      "110/110 [==============================] - 17s 157ms/step - loss: 8524.7267 - reconstruction_loss: 6063.3130 - kl_loss: 2376.1411 - perceptual_loss: 5.5405 - val_loss: 4126.5859 - val_reconstruction_loss: 3256.3208 - val_kl_loss: 867.1791 - val_perceptual_loss: 3.0860 - lr: 2.5000e-05\n",
      "Epoch 141/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 9021.3707 - reconstruction_loss: 6509.0771 - kl_loss: 2340.9819 - perceptual_loss: 5.5484\n",
      "Epoch 141: val_reconstruction_loss did not improve from 2684.62427\n",
      "110/110 [==============================] - 17s 156ms/step - loss: 9019.8774 - reconstruction_loss: 6509.0771 - kl_loss: 2340.9819 - perceptual_loss: 5.5484 - val_loss: 4139.8799 - val_reconstruction_loss: 3244.1304 - val_kl_loss: 893.0906 - val_perceptual_loss: 2.6592 - lr: 2.5000e-05\n",
      "Epoch 142/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8483.7270 - reconstruction_loss: 5998.3647 - kl_loss: 2408.6064 - perceptual_loss: 5.4579\n",
      "Epoch 142: val_reconstruction_loss did not improve from 2684.62427\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 8483.0847 - reconstruction_loss: 5998.3647 - kl_loss: 2408.6064 - perceptual_loss: 5.4579 - val_loss: 3835.1252 - val_reconstruction_loss: 2942.7783 - val_kl_loss: 889.6237 - val_perceptual_loss: 2.7232 - lr: 2.5000e-05\n",
      "Epoch 143/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8468.9172 - reconstruction_loss: 6046.8184 - kl_loss: 2388.8579 - perceptual_loss: 5.4695\n",
      "Epoch 143: val_reconstruction_loss did not improve from 2684.62427\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 8468.6670 - reconstruction_loss: 6046.8184 - kl_loss: 2388.8579 - perceptual_loss: 5.4695 - val_loss: 3729.8057 - val_reconstruction_loss: 2834.1326 - val_kl_loss: 892.7560 - val_perceptual_loss: 2.9171 - lr: 2.5000e-05\n",
      "Epoch 144/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8700.3844 - reconstruction_loss: 6067.1919 - kl_loss: 2373.0637 - perceptual_loss: 5.4450\n",
      "Epoch 144: val_reconstruction_loss improved from 2684.62427 to 2595.85742, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 169ms/step - loss: 8698.0899 - reconstruction_loss: 6067.1919 - kl_loss: 2373.0637 - perceptual_loss: 5.4450 - val_loss: 3487.4102 - val_reconstruction_loss: 2595.8574 - val_kl_loss: 888.5276 - val_perceptual_loss: 3.0253 - lr: 2.5000e-05\n",
      "Epoch 145/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8101.7595 - reconstruction_loss: 6086.6431 - kl_loss: 2396.4368 - perceptual_loss: 5.4354\n",
      "Epoch 145: val_reconstruction_loss did not improve from 2595.85742\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 8105.2438 - reconstruction_loss: 6086.6431 - kl_loss: 2396.4368 - perceptual_loss: 5.4354 - val_loss: 3523.3701 - val_reconstruction_loss: 2615.0542 - val_kl_loss: 905.5463 - val_perceptual_loss: 2.7696 - lr: 2.5000e-05\n",
      "Epoch 146/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8330.6080 - reconstruction_loss: 5706.9780 - kl_loss: 2425.2468 - perceptual_loss: 5.4024\n",
      "Epoch 146: val_reconstruction_loss did not improve from 2595.85742\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 8328.8694 - reconstruction_loss: 5706.9780 - kl_loss: 2425.2468 - perceptual_loss: 5.4024 - val_loss: 3635.1685 - val_reconstruction_loss: 2737.3179 - val_kl_loss: 894.9532 - val_perceptual_loss: 2.8975 - lr: 2.5000e-05\n",
      "Epoch 147/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8046.7774 - reconstruction_loss: 5725.5464 - kl_loss: 2403.1182 - perceptual_loss: 5.3877\n",
      "Epoch 147: val_reconstruction_loss improved from 2595.85742 to 2539.08252, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 169ms/step - loss: 8047.5636 - reconstruction_loss: 5725.5464 - kl_loss: 2403.1182 - perceptual_loss: 5.3877 - val_loss: 3439.1233 - val_reconstruction_loss: 2539.0825 - val_kl_loss: 897.1555 - val_perceptual_loss: 2.8853 - lr: 2.5000e-05\n",
      "Epoch 148/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8200.7486 - reconstruction_loss: 5813.4072 - kl_loss: 2379.0947 - perceptual_loss: 5.3309\n",
      "Epoch 148: val_reconstruction_loss did not improve from 2539.08252\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 8200.7223 - reconstruction_loss: 5813.4072 - kl_loss: 2379.0947 - perceptual_loss: 5.3309 - val_loss: 3901.3643 - val_reconstruction_loss: 3012.5818 - val_kl_loss: 885.8041 - val_perceptual_loss: 2.9782 - lr: 2.5000e-05\n",
      "Epoch 149/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7818.3097 - reconstruction_loss: 5731.6992 - kl_loss: 2366.7302 - perceptual_loss: 5.3460\n",
      "Epoch 149: val_reconstruction_loss did not improve from 2539.08252\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7820.8815 - reconstruction_loss: 5731.6992 - kl_loss: 2366.7302 - perceptual_loss: 5.3460 - val_loss: 3748.0876 - val_reconstruction_loss: 2858.7390 - val_kl_loss: 886.5129 - val_perceptual_loss: 2.8357 - lr: 2.5000e-05\n",
      "Epoch 150/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8245.3937 - reconstruction_loss: 5739.3887 - kl_loss: 2372.8320 - perceptual_loss: 5.3772\n",
      "Epoch 150: val_reconstruction_loss did not improve from 2539.08252\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 8244.2424 - reconstruction_loss: 5739.3887 - kl_loss: 2372.8320 - perceptual_loss: 5.3772 - val_loss: 4031.6912 - val_reconstruction_loss: 3146.1802 - val_kl_loss: 882.5897 - val_perceptual_loss: 2.9215 - lr: 2.5000e-05\n",
      "Epoch 151/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8167.6980 - reconstruction_loss: 5713.6841 - kl_loss: 2395.7961 - perceptual_loss: 5.3399\n",
      "Epoch 151: val_reconstruction_loss did not improve from 2539.08252\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 8167.2216 - reconstruction_loss: 5713.6841 - kl_loss: 2395.7961 - perceptual_loss: 5.3399 - val_loss: 3827.3901 - val_reconstruction_loss: 2932.3965 - val_kl_loss: 892.0821 - val_perceptual_loss: 2.9117 - lr: 2.5000e-05\n",
      "Epoch 152/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8090.2767 - reconstruction_loss: 5731.1240 - kl_loss: 2359.9927 - perceptual_loss: 5.2903\n",
      "Epoch 152: val_reconstruction_loss did not improve from 2539.08252\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 8090.3319 - reconstruction_loss: 5731.1240 - kl_loss: 2359.9927 - perceptual_loss: 5.2903 - val_loss: 3429.8677 - val_reconstruction_loss: 2548.9397 - val_kl_loss: 878.1392 - val_perceptual_loss: 2.7889 - lr: 2.5000e-05\n",
      "Epoch 153/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8322.4874 - reconstruction_loss: 6111.0098 - kl_loss: 2343.1147 - perceptual_loss: 5.3564\n",
      "Epoch 153: val_reconstruction_loss did not improve from 2539.08252\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 8323.7216 - reconstruction_loss: 6111.0098 - kl_loss: 2343.1147 - perceptual_loss: 5.3564 - val_loss: 3645.0791 - val_reconstruction_loss: 2768.6057 - val_kl_loss: 873.7432 - val_perceptual_loss: 2.7302 - lr: 2.5000e-05\n",
      "Epoch 154/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8065.6979 - reconstruction_loss: 5782.4204 - kl_loss: 2357.2393 - perceptual_loss: 5.3227\n",
      "Epoch 154: val_reconstruction_loss did not improve from 2539.08252\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 8066.4121 - reconstruction_loss: 5782.4204 - kl_loss: 2357.2393 - perceptual_loss: 5.3227 - val_loss: 3650.1567 - val_reconstruction_loss: 2777.5249 - val_kl_loss: 869.6603 - val_perceptual_loss: 2.9715 - lr: 2.5000e-05\n",
      "Epoch 155/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8051.2786 - reconstruction_loss: 5758.9966 - kl_loss: 2378.0579 - perceptual_loss: 5.3383\n",
      "Epoch 155: val_reconstruction_loss did not improve from 2539.08252\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 8052.0994 - reconstruction_loss: 5758.9966 - kl_loss: 2378.0579 - perceptual_loss: 5.3383 - val_loss: 3552.3357 - val_reconstruction_loss: 2644.1733 - val_kl_loss: 905.5090 - val_perceptual_loss: 2.6534 - lr: 2.5000e-05\n",
      "Epoch 156/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8254.7075 - reconstruction_loss: 5695.8638 - kl_loss: 2388.0171 - perceptual_loss: 5.3087\n",
      "Epoch 156: val_reconstruction_loss did not improve from 2539.08252\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 8253.2164 - reconstruction_loss: 5695.8638 - kl_loss: 2388.0171 - perceptual_loss: 5.3087 - val_loss: 3626.5251 - val_reconstruction_loss: 2733.6223 - val_kl_loss: 890.1303 - val_perceptual_loss: 2.7726 - lr: 2.5000e-05\n",
      "Epoch 157/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7739.5904 - reconstruction_loss: 5536.9233 - kl_loss: 2407.2080 - perceptual_loss: 5.1972\n",
      "Epoch 157: val_reconstruction_loss improved from 2539.08252 to 2484.25293, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 167ms/step - loss: 7741.4799 - reconstruction_loss: 5536.9233 - kl_loss: 2407.2080 - perceptual_loss: 5.1972 - val_loss: 3382.5264 - val_reconstruction_loss: 2484.2529 - val_kl_loss: 895.4910 - val_perceptual_loss: 2.7824 - lr: 2.5000e-05\n",
      "Epoch 158/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8102.9288 - reconstruction_loss: 5664.3618 - kl_loss: 2368.6707 - perceptual_loss: 5.2274\n",
      "Epoch 158: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 8102.3462 - reconstruction_loss: 5664.3618 - kl_loss: 2368.6707 - perceptual_loss: 5.2274 - val_loss: 3869.7576 - val_reconstruction_loss: 2995.9966 - val_kl_loss: 870.9201 - val_perceptual_loss: 2.8407 - lr: 2.5000e-05\n",
      "Epoch 159/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8308.2184 - reconstruction_loss: 6051.0898 - kl_loss: 2323.8337 - perceptual_loss: 5.2365\n",
      "Epoch 159: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 8308.8666 - reconstruction_loss: 6051.0898 - kl_loss: 2323.8337 - perceptual_loss: 5.2365 - val_loss: 3582.5420 - val_reconstruction_loss: 2700.7158 - val_kl_loss: 879.0907 - val_perceptual_loss: 2.7355 - lr: 2.5000e-05\n",
      "Epoch 160/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7981.7780 - reconstruction_loss: 5610.2603 - kl_loss: 2345.1003 - perceptual_loss: 5.1840\n",
      "Epoch 160: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7981.5866 - reconstruction_loss: 5610.2603 - kl_loss: 2345.1003 - perceptual_loss: 5.1840 - val_loss: 3858.5422 - val_reconstruction_loss: 2983.1528 - val_kl_loss: 872.5757 - val_perceptual_loss: 2.8137 - lr: 2.5000e-05\n",
      "Epoch 161/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7924.3997 - reconstruction_loss: 5697.7422 - kl_loss: 2330.9116 - perceptual_loss: 5.2244\n",
      "Epoch 161: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7925.3860 - reconstruction_loss: 5697.7422 - kl_loss: 2330.9116 - perceptual_loss: 5.2244 - val_loss: 3586.2725 - val_reconstruction_loss: 2714.2388 - val_kl_loss: 869.0765 - val_perceptual_loss: 2.9573 - lr: 2.5000e-05\n",
      "Epoch 162/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7919.3526 - reconstruction_loss: 5665.2637 - kl_loss: 2338.0010 - perceptual_loss: 5.1468\n",
      "Epoch 162: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7920.1549 - reconstruction_loss: 5665.2637 - kl_loss: 2338.0010 - perceptual_loss: 5.1468 - val_loss: 4313.0464 - val_reconstruction_loss: 3445.8608 - val_kl_loss: 864.3679 - val_perceptual_loss: 2.8176 - lr: 2.5000e-05\n",
      "Epoch 163/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8905.2095 - reconstruction_loss: 6407.3081 - kl_loss: 2336.6567 - perceptual_loss: 5.1601\n",
      "Epoch 163: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 8903.8033 - reconstruction_loss: 6407.3081 - kl_loss: 2336.6567 - perceptual_loss: 5.1601 - val_loss: 3592.2666 - val_reconstruction_loss: 2715.7898 - val_kl_loss: 873.6273 - val_perceptual_loss: 2.8495 - lr: 2.5000e-05\n",
      "Epoch 164/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8493.4125 - reconstruction_loss: 6447.7803 - kl_loss: 2300.1414 - perceptual_loss: 5.1446\n",
      "Epoch 164: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 8495.7517 - reconstruction_loss: 6447.7803 - kl_loss: 2300.1414 - perceptual_loss: 5.1446 - val_loss: 3965.8677 - val_reconstruction_loss: 3101.1875 - val_kl_loss: 861.7459 - val_perceptual_loss: 2.9344 - lr: 2.5000e-05\n",
      "Epoch 165/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8394.5411 - reconstruction_loss: 5794.2612 - kl_loss: 2310.0535 - perceptual_loss: 5.1552\n",
      "Epoch 165: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 8391.9729 - reconstruction_loss: 5794.2612 - kl_loss: 2310.0535 - perceptual_loss: 5.1552 - val_loss: 3549.3521 - val_reconstruction_loss: 2688.1785 - val_kl_loss: 858.3444 - val_perceptual_loss: 2.8290 - lr: 2.5000e-05\n",
      "Epoch 166/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8312.5163 - reconstruction_loss: 5704.0981 - kl_loss: 2326.8843 - perceptual_loss: 5.1747\n",
      "Epoch 166: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 8310.0266 - reconstruction_loss: 5704.0981 - kl_loss: 2326.8843 - perceptual_loss: 5.1747 - val_loss: 3445.8206 - val_reconstruction_loss: 2573.4004 - val_kl_loss: 869.4890 - val_perceptual_loss: 2.9312 - lr: 2.5000e-05\n",
      "Epoch 167/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7624.5540 - reconstruction_loss: 5601.7808 - kl_loss: 2331.8779 - perceptual_loss: 5.0948\n",
      "Epoch 167: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7627.3846 - reconstruction_loss: 5601.7808 - kl_loss: 2331.8779 - perceptual_loss: 5.0948 - val_loss: 3930.2302 - val_reconstruction_loss: 3057.2710 - val_kl_loss: 870.1046 - val_perceptual_loss: 2.8547 - lr: 2.5000e-05\n",
      "Epoch 168/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8095.3995 - reconstruction_loss: 5652.2646 - kl_loss: 2318.4255 - perceptual_loss: 5.0996\n",
      "Epoch 168: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 8094.3219 - reconstruction_loss: 5652.2646 - kl_loss: 2318.4255 - perceptual_loss: 5.0996 - val_loss: 3630.2761 - val_reconstruction_loss: 2768.1064 - val_kl_loss: 859.3811 - val_perceptual_loss: 2.7885 - lr: 2.5000e-05\n",
      "Epoch 169/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8068.1336 - reconstruction_loss: 5617.4995 - kl_loss: 2318.2715 - perceptual_loss: 5.1496\n",
      "Epoch 169: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 8066.9876 - reconstruction_loss: 5617.4995 - kl_loss: 2318.2715 - perceptual_loss: 5.1496 - val_loss: 3679.1592 - val_reconstruction_loss: 2798.1731 - val_kl_loss: 878.1874 - val_perceptual_loss: 2.7988 - lr: 2.5000e-05\n",
      "Epoch 170/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7826.3115 - reconstruction_loss: 5619.3203 - kl_loss: 2324.6992 - perceptual_loss: 5.1801\n",
      "Epoch 170: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7827.4186 - reconstruction_loss: 5619.3203 - kl_loss: 2324.6992 - perceptual_loss: 5.1801 - val_loss: 3659.0928 - val_reconstruction_loss: 2802.8291 - val_kl_loss: 853.5598 - val_perceptual_loss: 2.7039 - lr: 2.5000e-05\n",
      "Epoch 171/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 8312.0671 - reconstruction_loss: 5667.4092 - kl_loss: 2336.1582 - perceptual_loss: 5.1274\n",
      "Epoch 171: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 17s 157ms/step - loss: 8309.3340 - reconstruction_loss: 5667.4092 - kl_loss: 2336.1582 - perceptual_loss: 5.1274 - val_loss: 3745.2964 - val_reconstruction_loss: 2860.7998 - val_kl_loss: 881.6706 - val_perceptual_loss: 2.8259 - lr: 2.5000e-05\n",
      "Epoch 172/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7870.2864 - reconstruction_loss: 5633.1245 - kl_loss: 2317.3188 - perceptual_loss: 5.1307\n",
      "Epoch 172: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7871.0547 - reconstruction_loss: 5633.1245 - kl_loss: 2317.3188 - perceptual_loss: 5.1307 - val_loss: 3470.3281 - val_reconstruction_loss: 2605.4180 - val_kl_loss: 861.9109 - val_perceptual_loss: 2.9994 - lr: 2.5000e-05\n",
      "Epoch 173/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7893.1883 - reconstruction_loss: 5504.8867 - kl_loss: 2310.7966 - perceptual_loss: 5.1049\n",
      "Epoch 173: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7892.5361 - reconstruction_loss: 5504.8867 - kl_loss: 2310.7966 - perceptual_loss: 5.1049 - val_loss: 3466.4492 - val_reconstruction_loss: 2594.4673 - val_kl_loss: 869.0522 - val_perceptual_loss: 2.9297 - lr: 2.5000e-05\n",
      "Epoch 174/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7858.2707 - reconstruction_loss: 5541.1646 - kl_loss: 2318.5974 - perceptual_loss: 5.1031\n",
      "Epoch 174: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7858.3301 - reconstruction_loss: 5541.1646 - kl_loss: 2318.5974 - perceptual_loss: 5.1031 - val_loss: 3497.8550 - val_reconstruction_loss: 2620.3479 - val_kl_loss: 874.4470 - val_perceptual_loss: 3.0600 - lr: 2.5000e-05\n",
      "Epoch 175/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7615.6205 - reconstruction_loss: 5482.8848 - kl_loss: 2307.2280 - perceptual_loss: 5.0861\n",
      "Epoch 175: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7617.2383 - reconstruction_loss: 5482.8848 - kl_loss: 2307.2280 - perceptual_loss: 5.0861 - val_loss: 3524.4629 - val_reconstruction_loss: 2662.9688 - val_kl_loss: 858.6733 - val_perceptual_loss: 2.8209 - lr: 2.5000e-05\n",
      "Epoch 176/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7729.7201 - reconstruction_loss: 5582.0303 - kl_loss: 2297.4768 - perceptual_loss: 5.0533\n",
      "Epoch 176: val_reconstruction_loss did not improve from 2484.25293\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 7731.1150 - reconstruction_loss: 5582.0303 - kl_loss: 2297.4768 - perceptual_loss: 5.0533 - val_loss: 3414.9553 - val_reconstruction_loss: 2544.3787 - val_kl_loss: 867.7234 - val_perceptual_loss: 2.8532 - lr: 2.5000e-05\n",
      "Epoch 177/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7869.8807 - reconstruction_loss: 5567.4482 - kl_loss: 2314.4934 - perceptual_loss: 5.0890\n",
      "Epoch 177: val_reconstruction_loss did not improve from 2484.25293\n",
      "\n",
      "Epoch 177: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7870.0352 - reconstruction_loss: 5567.4482 - kl_loss: 2314.4934 - perceptual_loss: 5.0890 - val_loss: 3660.4382 - val_reconstruction_loss: 2792.1479 - val_kl_loss: 865.2154 - val_perceptual_loss: 3.0750 - lr: 2.5000e-05\n",
      "Epoch 178/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7741.4045 - reconstruction_loss: 5379.9097 - kl_loss: 2326.2104 - perceptual_loss: 5.0358\n",
      "Epoch 178: val_reconstruction_loss improved from 2484.25293 to 2477.19629, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 170ms/step - loss: 7741.1320 - reconstruction_loss: 5379.9097 - kl_loss: 2326.2104 - perceptual_loss: 5.0358 - val_loss: 3351.0669 - val_reconstruction_loss: 2477.1963 - val_kl_loss: 871.2522 - val_perceptual_loss: 2.6183 - lr: 1.2500e-05\n",
      "Epoch 179/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7307.1485 - reconstruction_loss: 5077.6333 - kl_loss: 2348.1274 - perceptual_loss: 5.0161\n",
      "Epoch 179: val_reconstruction_loss did not improve from 2477.19629\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 7308.2622 - reconstruction_loss: 5077.6333 - kl_loss: 2348.1274 - perceptual_loss: 5.0161 - val_loss: 3567.0227 - val_reconstruction_loss: 2693.1089 - val_kl_loss: 871.0366 - val_perceptual_loss: 2.8773 - lr: 1.2500e-05\n",
      "Epoch 180/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7489.4181 - reconstruction_loss: 5102.4502 - kl_loss: 2329.2219 - perceptual_loss: 4.9971\n",
      "Epoch 180: val_reconstruction_loss did not improve from 2477.19629\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 7488.9428 - reconstruction_loss: 5102.4502 - kl_loss: 2329.2219 - perceptual_loss: 4.9971 - val_loss: 3409.5361 - val_reconstruction_loss: 2537.4128 - val_kl_loss: 869.1122 - val_perceptual_loss: 3.0110 - lr: 1.2500e-05\n",
      "Epoch 181/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7562.3867 - reconstruction_loss: 5150.9824 - kl_loss: 2329.7034 - perceptual_loss: 5.0274\n",
      "Epoch 181: val_reconstruction_loss did not improve from 2477.19629\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7561.6959 - reconstruction_loss: 5150.9824 - kl_loss: 2329.7034 - perceptual_loss: 5.0274 - val_loss: 3362.7869 - val_reconstruction_loss: 2490.2449 - val_kl_loss: 869.7153 - val_perceptual_loss: 2.8268 - lr: 1.2500e-05\n",
      "Epoch 182/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7438.7120 - reconstruction_loss: 5152.6494 - kl_loss: 2338.1965 - perceptual_loss: 4.9970\n",
      "Epoch 182: val_reconstruction_loss improved from 2477.19629 to 2339.67529, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 169ms/step - loss: 7439.2266 - reconstruction_loss: 5152.6494 - kl_loss: 2338.1965 - perceptual_loss: 4.9970 - val_loss: 3221.4302 - val_reconstruction_loss: 2339.6753 - val_kl_loss: 878.8359 - val_perceptual_loss: 2.9188 - lr: 1.2500e-05\n",
      "Epoch 183/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7300.5511 - reconstruction_loss: 5137.9727 - kl_loss: 2327.9512 - perceptual_loss: 4.9654\n",
      "Epoch 183: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7302.0857 - reconstruction_loss: 5137.9727 - kl_loss: 2327.9512 - perceptual_loss: 4.9654 - val_loss: 3357.8994 - val_reconstruction_loss: 2473.9392 - val_kl_loss: 881.1776 - val_perceptual_loss: 2.7828 - lr: 1.2500e-05\n",
      "Epoch 184/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7437.7878 - reconstruction_loss: 5033.5112 - kl_loss: 2366.2234 - perceptual_loss: 4.9667\n",
      "Epoch 184: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7437.4897 - reconstruction_loss: 5033.5112 - kl_loss: 2366.2234 - perceptual_loss: 4.9667 - val_loss: 3339.9790 - val_reconstruction_loss: 2455.2378 - val_kl_loss: 881.9730 - val_perceptual_loss: 2.7684 - lr: 1.2500e-05\n",
      "Epoch 185/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7137.0993 - reconstruction_loss: 5012.2319 - kl_loss: 2361.4893 - perceptual_loss: 4.9569\n",
      "Epoch 185: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 157ms/step - loss: 7139.2757 - reconstruction_loss: 5012.2319 - kl_loss: 2361.4893 - perceptual_loss: 4.9569 - val_loss: 3408.4160 - val_reconstruction_loss: 2518.4336 - val_kl_loss: 887.2719 - val_perceptual_loss: 2.7104 - lr: 1.2500e-05\n",
      "Epoch 186/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7331.6466 - reconstruction_loss: 4939.4536 - kl_loss: 2345.3289 - perceptual_loss: 4.9621\n",
      "Epoch 186: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 17s 157ms/step - loss: 7331.2692 - reconstruction_loss: 4939.4536 - kl_loss: 2345.3289 - perceptual_loss: 4.9621 - val_loss: 3342.8870 - val_reconstruction_loss: 2464.6587 - val_kl_loss: 875.6412 - val_perceptual_loss: 2.5871 - lr: 1.2500e-05\n",
      "Epoch 187/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7224.2784 - reconstruction_loss: 5007.2363 - kl_loss: 2369.3689 - perceptual_loss: 4.9590\n",
      "Epoch 187: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7225.6955 - reconstruction_loss: 5007.2363 - kl_loss: 2369.3689 - perceptual_loss: 4.9590 - val_loss: 3330.0747 - val_reconstruction_loss: 2443.8979 - val_kl_loss: 883.3219 - val_perceptual_loss: 2.8549 - lr: 1.2500e-05\n",
      "Epoch 188/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7534.6002 - reconstruction_loss: 5030.6631 - kl_loss: 2362.4011 - perceptual_loss: 4.9469\n",
      "Epoch 188: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 7533.3697 - reconstruction_loss: 5030.6631 - kl_loss: 2362.4011 - perceptual_loss: 4.9469 - val_loss: 3270.4141 - val_reconstruction_loss: 2384.2205 - val_kl_loss: 883.5399 - val_perceptual_loss: 2.6536 - lr: 1.2500e-05\n",
      "Epoch 189/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7188.9116 - reconstruction_loss: 4941.1123 - kl_loss: 2370.5896 - perceptual_loss: 4.9146\n",
      "Epoch 189: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 7190.0621 - reconstruction_loss: 4941.1123 - kl_loss: 2370.5896 - perceptual_loss: 4.9146 - val_loss: 3349.6609 - val_reconstruction_loss: 2460.0942 - val_kl_loss: 886.6345 - val_perceptual_loss: 2.9321 - lr: 1.2500e-05\n",
      "Epoch 190/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7378.3153 - reconstruction_loss: 4949.9014 - kl_loss: 2353.5830 - perceptual_loss: 4.9196\n",
      "Epoch 190: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7377.6854 - reconstruction_loss: 4949.9014 - kl_loss: 2353.5830 - perceptual_loss: 4.9196 - val_loss: 3336.9346 - val_reconstruction_loss: 2450.9944 - val_kl_loss: 883.0451 - val_perceptual_loss: 2.8949 - lr: 1.2500e-05\n",
      "Epoch 191/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7283.9867 - reconstruction_loss: 5005.2642 - kl_loss: 2361.7832 - perceptual_loss: 4.9211\n",
      "Epoch 191: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 7284.7793 - reconstruction_loss: 5005.2642 - kl_loss: 2361.7832 - perceptual_loss: 4.9211 - val_loss: 3531.0088 - val_reconstruction_loss: 2640.5405 - val_kl_loss: 887.5630 - val_perceptual_loss: 2.9053 - lr: 1.2500e-05\n",
      "Epoch 192/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7260.9893 - reconstruction_loss: 4858.3623 - kl_loss: 2364.4590 - perceptual_loss: 4.9157\n",
      "Epoch 192: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7260.6897 - reconstruction_loss: 4858.3623 - kl_loss: 2364.4590 - perceptual_loss: 4.9157 - val_loss: 3287.4863 - val_reconstruction_loss: 2399.5281 - val_kl_loss: 885.3077 - val_perceptual_loss: 2.6507 - lr: 1.2500e-05\n",
      "Epoch 193/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7182.4720 - reconstruction_loss: 4853.8403 - kl_loss: 2352.2393 - perceptual_loss: 4.9406\n",
      "Epoch 193: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7182.7292 - reconstruction_loss: 4853.8403 - kl_loss: 2352.2393 - perceptual_loss: 4.9406 - val_loss: 3390.4165 - val_reconstruction_loss: 2503.1294 - val_kl_loss: 884.7032 - val_perceptual_loss: 2.5836 - lr: 1.2500e-05\n",
      "Epoch 194/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7245.4147 - reconstruction_loss: 4895.6299 - kl_loss: 2343.2637 - perceptual_loss: 4.9145\n",
      "Epoch 194: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 7245.4002 - reconstruction_loss: 4895.6299 - kl_loss: 2343.2637 - perceptual_loss: 4.9145 - val_loss: 3312.7207 - val_reconstruction_loss: 2441.6880 - val_kl_loss: 868.3352 - val_perceptual_loss: 2.6975 - lr: 1.2500e-05\n",
      "Epoch 195/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7339.5941 - reconstruction_loss: 4885.0962 - kl_loss: 2331.7615 - perceptual_loss: 4.8842\n",
      "Epoch 195: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7338.5323 - reconstruction_loss: 4885.0962 - kl_loss: 2331.7615 - perceptual_loss: 4.8842 - val_loss: 3405.1411 - val_reconstruction_loss: 2525.8145 - val_kl_loss: 876.4768 - val_perceptual_loss: 2.8500 - lr: 1.2500e-05\n",
      "Epoch 196/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7382.2215 - reconstruction_loss: 4931.6221 - kl_loss: 2350.2512 - perceptual_loss: 4.9147\n",
      "Epoch 196: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 7381.3618 - reconstruction_loss: 4931.6221 - kl_loss: 2350.2512 - perceptual_loss: 4.9147 - val_loss: 3282.1030 - val_reconstruction_loss: 2391.9316 - val_kl_loss: 887.5500 - val_perceptual_loss: 2.6213 - lr: 1.2500e-05\n",
      "Epoch 197/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7158.7373 - reconstruction_loss: 4846.5391 - kl_loss: 2360.8206 - perceptual_loss: 4.9234\n",
      "Epoch 197: val_reconstruction_loss did not improve from 2339.67529\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 7159.2197 - reconstruction_loss: 4846.5391 - kl_loss: 2360.8206 - perceptual_loss: 4.9234 - val_loss: 3295.1670 - val_reconstruction_loss: 2407.2490 - val_kl_loss: 885.0380 - val_perceptual_loss: 2.8801 - lr: 1.2500e-05\n",
      "Epoch 198/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7164.3406 - reconstruction_loss: 4815.6265 - kl_loss: 2364.0142 - perceptual_loss: 4.8852\n",
      "Epoch 198: val_reconstruction_loss improved from 2339.67529 to 2247.94434, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 170ms/step - loss: 7164.5225 - reconstruction_loss: 4815.6265 - kl_loss: 2364.0142 - perceptual_loss: 4.8852 - val_loss: 3132.7991 - val_reconstruction_loss: 2247.9443 - val_kl_loss: 882.1298 - val_perceptual_loss: 2.7249 - lr: 1.2500e-05\n",
      "Epoch 199/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7045.7125 - reconstruction_loss: 4998.9976 - kl_loss: 2327.6707 - perceptual_loss: 4.8622\n",
      "Epoch 199: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7048.2874 - reconstruction_loss: 4998.9976 - kl_loss: 2327.6707 - perceptual_loss: 4.8622 - val_loss: 3369.3696 - val_reconstruction_loss: 2497.5488 - val_kl_loss: 868.9422 - val_perceptual_loss: 2.8787 - lr: 1.2500e-05\n",
      "Epoch 200/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6850.1559 - reconstruction_loss: 4878.5010 - kl_loss: 2332.8743 - perceptual_loss: 4.8930\n",
      "Epoch 200: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 6853.4542 - reconstruction_loss: 4878.5010 - kl_loss: 2332.8743 - perceptual_loss: 4.8930 - val_loss: 3276.4438 - val_reconstruction_loss: 2396.2061 - val_kl_loss: 877.6298 - val_perceptual_loss: 2.6080 - lr: 1.2500e-05\n",
      "Epoch 201/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7334.7830 - reconstruction_loss: 4945.5420 - kl_loss: 2340.4807 - perceptual_loss: 4.9033\n",
      "Epoch 201: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7334.3879 - reconstruction_loss: 4945.5420 - kl_loss: 2340.4807 - perceptual_loss: 4.9033 - val_loss: 3430.7876 - val_reconstruction_loss: 2546.1494 - val_kl_loss: 882.0221 - val_perceptual_loss: 2.6163 - lr: 1.2500e-05\n",
      "Epoch 202/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7435.2907 - reconstruction_loss: 5039.3706 - kl_loss: 2325.5242 - perceptual_loss: 4.8834\n",
      "Epoch 202: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7434.7005 - reconstruction_loss: 5039.3706 - kl_loss: 2325.5242 - perceptual_loss: 4.8834 - val_loss: 3184.1277 - val_reconstruction_loss: 2311.1167 - val_kl_loss: 870.2322 - val_perceptual_loss: 2.7787 - lr: 1.2500e-05\n",
      "Epoch 203/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7170.0214 - reconstruction_loss: 4971.9326 - kl_loss: 2318.7317 - perceptual_loss: 4.8834\n",
      "Epoch 203: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 7171.1523 - reconstruction_loss: 4971.9326 - kl_loss: 2318.7317 - perceptual_loss: 4.8834 - val_loss: 3225.6743 - val_reconstruction_loss: 2341.7336 - val_kl_loss: 881.2159 - val_perceptual_loss: 2.7245 - lr: 1.2500e-05\n",
      "Epoch 204/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7187.7818 - reconstruction_loss: 4933.6411 - kl_loss: 2344.9690 - perceptual_loss: 4.8747\n",
      "Epoch 204: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7188.6439 - reconstruction_loss: 4933.6411 - kl_loss: 2344.9690 - perceptual_loss: 4.8747 - val_loss: 3275.0320 - val_reconstruction_loss: 2401.8115 - val_kl_loss: 870.5998 - val_perceptual_loss: 2.6206 - lr: 1.2500e-05\n",
      "Epoch 205/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7272.7439 - reconstruction_loss: 4884.2671 - kl_loss: 2333.8660 - perceptual_loss: 4.8575\n",
      "Epoch 205: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 7272.2957 - reconstruction_loss: 4884.2671 - kl_loss: 2333.8660 - perceptual_loss: 4.8575 - val_loss: 3369.0100 - val_reconstruction_loss: 2497.2900 - val_kl_loss: 868.8228 - val_perceptual_loss: 2.8972 - lr: 1.2500e-05\n",
      "Epoch 206/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7141.8853 - reconstruction_loss: 4865.3936 - kl_loss: 2328.9460 - perceptual_loss: 4.8686\n",
      "Epoch 206: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7142.4017 - reconstruction_loss: 4865.3936 - kl_loss: 2328.9460 - perceptual_loss: 4.8686 - val_loss: 3244.9336 - val_reconstruction_loss: 2367.3101 - val_kl_loss: 874.7792 - val_perceptual_loss: 2.8443 - lr: 1.2500e-05\n",
      "Epoch 207/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6873.1158 - reconstruction_loss: 4801.6396 - kl_loss: 2338.0005 - perceptual_loss: 4.8444\n",
      "Epoch 207: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 6875.5606 - reconstruction_loss: 4801.6396 - kl_loss: 2338.0005 - perceptual_loss: 4.8444 - val_loss: 3284.3264 - val_reconstruction_loss: 2403.3259 - val_kl_loss: 878.2685 - val_perceptual_loss: 2.7318 - lr: 1.2500e-05\n",
      "Epoch 208/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7108.9304 - reconstruction_loss: 4822.9971 - kl_loss: 2340.4456 - perceptual_loss: 4.8306\n",
      "Epoch 208: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7109.4650 - reconstruction_loss: 4822.9971 - kl_loss: 2340.4456 - perceptual_loss: 4.8306 - val_loss: 3346.4470 - val_reconstruction_loss: 2469.2935 - val_kl_loss: 874.3312 - val_perceptual_loss: 2.8223 - lr: 1.2500e-05\n",
      "Epoch 209/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7089.6119 - reconstruction_loss: 4801.3545 - kl_loss: 2322.0640 - perceptual_loss: 4.8397\n",
      "Epoch 209: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 7089.9600 - reconstruction_loss: 4801.3545 - kl_loss: 2322.0640 - perceptual_loss: 4.8397 - val_loss: 3326.8826 - val_reconstruction_loss: 2459.1797 - val_kl_loss: 864.8618 - val_perceptual_loss: 2.8410 - lr: 1.2500e-05\n",
      "Epoch 210/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7062.6782 - reconstruction_loss: 4849.2666 - kl_loss: 2311.1401 - perceptual_loss: 4.8626\n",
      "Epoch 210: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7063.6024 - reconstruction_loss: 4849.2666 - kl_loss: 2311.1401 - perceptual_loss: 4.8626 - val_loss: 3298.4033 - val_reconstruction_loss: 2437.9004 - val_kl_loss: 857.8267 - val_perceptual_loss: 2.6762 - lr: 1.2500e-05\n",
      "Epoch 211/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7101.3913 - reconstruction_loss: 4772.4155 - kl_loss: 2324.8340 - perceptual_loss: 4.8276\n",
      "Epoch 211: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7101.3975 - reconstruction_loss: 4772.4155 - kl_loss: 2324.8340 - perceptual_loss: 4.8276 - val_loss: 3342.9954 - val_reconstruction_loss: 2470.3315 - val_kl_loss: 869.6655 - val_perceptual_loss: 2.9982 - lr: 1.2500e-05\n",
      "Epoch 212/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7065.0890 - reconstruction_loss: 4789.7339 - kl_loss: 2335.6873 - perceptual_loss: 4.7853\n",
      "Epoch 212: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7065.6756 - reconstruction_loss: 4789.7339 - kl_loss: 2335.6873 - perceptual_loss: 4.7853 - val_loss: 3400.4573 - val_reconstruction_loss: 2530.2561 - val_kl_loss: 867.3734 - val_perceptual_loss: 2.8279 - lr: 1.2500e-05\n",
      "Epoch 213/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7284.8504 - reconstruction_loss: 4913.4517 - kl_loss: 2313.3684 - perceptual_loss: 4.7977\n",
      "Epoch 213: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 7284.3708 - reconstruction_loss: 4913.4517 - kl_loss: 2313.3684 - perceptual_loss: 4.7977 - val_loss: 3261.7690 - val_reconstruction_loss: 2393.6460 - val_kl_loss: 865.4933 - val_perceptual_loss: 2.6296 - lr: 1.2500e-05\n",
      "Epoch 214/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6903.9142 - reconstruction_loss: 4759.4800 - kl_loss: 2313.8354 - perceptual_loss: 4.8049\n",
      "Epoch 214: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 6905.4837 - reconstruction_loss: 4759.4800 - kl_loss: 2313.8354 - perceptual_loss: 4.8049 - val_loss: 3235.5176 - val_reconstruction_loss: 2363.9978 - val_kl_loss: 868.6566 - val_perceptual_loss: 2.8630 - lr: 1.2500e-05\n",
      "Epoch 215/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7031.6572 - reconstruction_loss: 4975.0664 - kl_loss: 2321.9287 - perceptual_loss: 4.7944\n",
      "Epoch 215: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7034.0908 - reconstruction_loss: 4975.0664 - kl_loss: 2321.9287 - perceptual_loss: 4.7944 - val_loss: 3541.0200 - val_reconstruction_loss: 2666.5393 - val_kl_loss: 871.7057 - val_perceptual_loss: 2.7750 - lr: 1.2500e-05\n",
      "Epoch 216/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7272.1650 - reconstruction_loss: 4916.1484 - kl_loss: 2311.9136 - perceptual_loss: 4.8056\n",
      "Epoch 216: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7271.8110 - reconstruction_loss: 4916.1484 - kl_loss: 2311.9136 - perceptual_loss: 4.8056 - val_loss: 3323.8159 - val_reconstruction_loss: 2455.7759 - val_kl_loss: 865.1582 - val_perceptual_loss: 2.8818 - lr: 1.2500e-05\n",
      "Epoch 217/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7157.0367 - reconstruction_loss: 4863.3359 - kl_loss: 2308.4614 - perceptual_loss: 4.8090\n",
      "Epoch 217: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 7157.2130 - reconstruction_loss: 4863.3359 - kl_loss: 2308.4614 - perceptual_loss: 4.8090 - val_loss: 3382.3467 - val_reconstruction_loss: 2524.4346 - val_kl_loss: 855.2360 - val_perceptual_loss: 2.6759 - lr: 1.2500e-05\n",
      "Epoch 218/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7460.0182 - reconstruction_loss: 4995.1001 - kl_loss: 2307.8875 - perceptual_loss: 4.8108\n",
      "Epoch 218: val_reconstruction_loss did not improve from 2247.94434\n",
      "\n",
      "Epoch 218: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 7458.6469 - reconstruction_loss: 4995.1001 - kl_loss: 2307.8875 - perceptual_loss: 4.8108 - val_loss: 3377.0312 - val_reconstruction_loss: 2509.7520 - val_kl_loss: 864.5519 - val_perceptual_loss: 2.7274 - lr: 1.2500e-05\n",
      "Epoch 219/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6877.2490 - reconstruction_loss: 4695.9180 - kl_loss: 2313.8101 - perceptual_loss: 4.7822\n",
      "Epoch 219: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6878.4856 - reconstruction_loss: 4695.9180 - kl_loss: 2313.8101 - perceptual_loss: 4.7822 - val_loss: 3335.1338 - val_reconstruction_loss: 2468.2234 - val_kl_loss: 864.0411 - val_perceptual_loss: 2.8691 - lr: 6.2500e-06\n",
      "Epoch 220/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7056.7333 - reconstruction_loss: 4640.6274 - kl_loss: 2313.8184 - perceptual_loss: 4.7569\n",
      "Epoch 220: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 162ms/step - loss: 7055.8546 - reconstruction_loss: 4640.6274 - kl_loss: 2313.8184 - perceptual_loss: 4.7569 - val_loss: 3127.9519 - val_reconstruction_loss: 2255.6343 - val_kl_loss: 869.5654 - val_perceptual_loss: 2.7523 - lr: 6.2500e-06\n",
      "Epoch 221/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7037.4752 - reconstruction_loss: 4559.3677 - kl_loss: 2328.5037 - perceptual_loss: 4.7393\n",
      "Epoch 221: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7036.1701 - reconstruction_loss: 4559.3677 - kl_loss: 2328.5037 - perceptual_loss: 4.7393 - val_loss: 3211.1064 - val_reconstruction_loss: 2338.9807 - val_kl_loss: 869.4186 - val_perceptual_loss: 2.7070 - lr: 6.2500e-06\n",
      "Epoch 222/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6781.2801 - reconstruction_loss: 4551.8950 - kl_loss: 2334.2542 - perceptual_loss: 4.7681\n",
      "Epoch 222: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 6782.2678 - reconstruction_loss: 4551.8950 - kl_loss: 2334.2542 - perceptual_loss: 4.7681 - val_loss: 3138.5723 - val_reconstruction_loss: 2263.1467 - val_kl_loss: 872.6217 - val_perceptual_loss: 2.8038 - lr: 6.2500e-06\n",
      "Epoch 223/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6900.0730 - reconstruction_loss: 4578.0059 - kl_loss: 2336.2549 - perceptual_loss: 4.7612\n",
      "Epoch 223: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 6900.2437 - reconstruction_loss: 4578.0059 - kl_loss: 2336.2549 - perceptual_loss: 4.7612 - val_loss: 3135.7119 - val_reconstruction_loss: 2255.7959 - val_kl_loss: 877.1260 - val_perceptual_loss: 2.7901 - lr: 6.2500e-06\n",
      "Epoch 224/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6794.4164 - reconstruction_loss: 4535.4893 - kl_loss: 2342.0959 - perceptual_loss: 4.7541\n",
      "Epoch 224: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 6795.2085 - reconstruction_loss: 4535.4893 - kl_loss: 2342.0959 - perceptual_loss: 4.7541 - val_loss: 3250.8611 - val_reconstruction_loss: 2373.6306 - val_kl_loss: 874.3701 - val_perceptual_loss: 2.8605 - lr: 6.2500e-06\n",
      "Epoch 225/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6815.3323 - reconstruction_loss: 4538.6670 - kl_loss: 2341.4363 - perceptual_loss: 4.7382\n",
      "Epoch 225: val_reconstruction_loss did not improve from 2247.94434\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6815.9585 - reconstruction_loss: 4538.6670 - kl_loss: 2341.4363 - perceptual_loss: 4.7382 - val_loss: 3257.5249 - val_reconstruction_loss: 2381.5688 - val_kl_loss: 873.0475 - val_perceptual_loss: 2.9085 - lr: 6.2500e-06\n",
      "Epoch 226/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6838.9728 - reconstruction_loss: 4539.1885 - kl_loss: 2337.5779 - perceptual_loss: 4.7290\n",
      "Epoch 226: val_reconstruction_loss improved from 2247.94434 to 2231.80200, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 169ms/step - loss: 6839.3558 - reconstruction_loss: 4539.1885 - kl_loss: 2337.5779 - perceptual_loss: 4.7290 - val_loss: 3111.9194 - val_reconstruction_loss: 2231.8020 - val_kl_loss: 877.3915 - val_perceptual_loss: 2.7258 - lr: 6.2500e-06\n",
      "Epoch 227/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6864.0308 - reconstruction_loss: 4511.2959 - kl_loss: 2348.5972 - perceptual_loss: 4.7388\n",
      "Epoch 227: val_reconstruction_loss did not improve from 2231.80200\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 6864.0362 - reconstruction_loss: 4511.2959 - kl_loss: 2348.5972 - perceptual_loss: 4.7388 - val_loss: 3186.3916 - val_reconstruction_loss: 2304.6826 - val_kl_loss: 878.8235 - val_perceptual_loss: 2.8854 - lr: 6.2500e-06\n",
      "Epoch 228/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6673.9696 - reconstruction_loss: 4519.0737 - kl_loss: 2343.8589 - perceptual_loss: 4.7372\n",
      "Epoch 228: val_reconstruction_loss did not improve from 2231.80200\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6675.7147 - reconstruction_loss: 4519.0737 - kl_loss: 2343.8589 - perceptual_loss: 4.7372 - val_loss: 3190.9626 - val_reconstruction_loss: 2313.9771 - val_kl_loss: 874.3405 - val_perceptual_loss: 2.6450 - lr: 6.2500e-06\n",
      "Epoch 229/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7059.5298 - reconstruction_loss: 4531.8423 - kl_loss: 2341.3088 - perceptual_loss: 4.7188\n",
      "Epoch 229: val_reconstruction_loss improved from 2231.80200 to 2182.89014, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 169ms/step - loss: 7057.8933 - reconstruction_loss: 4531.8423 - kl_loss: 2341.3088 - perceptual_loss: 4.7188 - val_loss: 3065.8269 - val_reconstruction_loss: 2182.8901 - val_kl_loss: 880.1500 - val_perceptual_loss: 2.7869 - lr: 6.2500e-06\n",
      "Epoch 230/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6759.6881 - reconstruction_loss: 4522.8530 - kl_loss: 2352.3965 - perceptual_loss: 4.7140\n",
      "Epoch 230: val_reconstruction_loss did not improve from 2182.89014\n",
      "110/110 [==============================] - 17s 158ms/step - loss: 6760.7717 - reconstruction_loss: 4522.8530 - kl_loss: 2352.3965 - perceptual_loss: 4.7140 - val_loss: 3296.4194 - val_reconstruction_loss: 2418.7432 - val_kl_loss: 875.0355 - val_perceptual_loss: 2.6409 - lr: 6.2500e-06\n",
      "Epoch 231/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6927.6740 - reconstruction_loss: 4526.4292 - kl_loss: 2337.8269 - perceptual_loss: 4.6939\n",
      "Epoch 231: val_reconstruction_loss improved from 2182.89014 to 2175.27393, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 169ms/step - loss: 6927.1449 - reconstruction_loss: 4526.4292 - kl_loss: 2337.8269 - perceptual_loss: 4.6939 - val_loss: 3050.6167 - val_reconstruction_loss: 2175.2739 - val_kl_loss: 872.6912 - val_perceptual_loss: 2.6516 - lr: 6.2500e-06\n",
      "Epoch 232/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6558.3678 - reconstruction_loss: 4486.4717 - kl_loss: 2335.3809 - perceptual_loss: 4.7171\n",
      "Epoch 232: val_reconstruction_loss did not improve from 2175.27393\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6560.7840 - reconstruction_loss: 4486.4717 - kl_loss: 2335.3809 - perceptual_loss: 4.7171 - val_loss: 3166.8306 - val_reconstruction_loss: 2291.6685 - val_kl_loss: 872.4113 - val_perceptual_loss: 2.7510 - lr: 6.2500e-06\n",
      "Epoch 233/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6701.4415 - reconstruction_loss: 4522.5586 - kl_loss: 2333.5559 - perceptual_loss: 4.7019\n",
      "Epoch 233: val_reconstruction_loss did not improve from 2175.27393\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6702.8773 - reconstruction_loss: 4522.5586 - kl_loss: 2333.5559 - perceptual_loss: 4.7019 - val_loss: 3189.0557 - val_reconstruction_loss: 2316.0908 - val_kl_loss: 870.0848 - val_perceptual_loss: 2.8801 - lr: 6.2500e-06\n",
      "Epoch 234/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7064.4950 - reconstruction_loss: 4530.2124 - kl_loss: 2322.4563 - perceptual_loss: 4.7135\n",
      "Epoch 234: val_reconstruction_loss did not improve from 2175.27393\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 7062.6291 - reconstruction_loss: 4530.2124 - kl_loss: 2322.4563 - perceptual_loss: 4.7135 - val_loss: 3249.4058 - val_reconstruction_loss: 2379.0134 - val_kl_loss: 867.5253 - val_perceptual_loss: 2.8670 - lr: 6.2500e-06\n",
      "Epoch 235/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6835.0384 - reconstruction_loss: 4496.0005 - kl_loss: 2319.8782 - perceptual_loss: 4.6855\n",
      "Epoch 235: val_reconstruction_loss did not improve from 2175.27393\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6834.9080 - reconstruction_loss: 4496.0005 - kl_loss: 2319.8782 - perceptual_loss: 4.6855 - val_loss: 3130.3318 - val_reconstruction_loss: 2259.1018 - val_kl_loss: 868.4834 - val_perceptual_loss: 2.7467 - lr: 6.2500e-06\n",
      "Epoch 236/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6945.8752 - reconstruction_loss: 4530.0894 - kl_loss: 2322.8809 - perceptual_loss: 4.7378\n",
      "Epoch 236: val_reconstruction_loss did not improve from 2175.27393\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 6945.0809 - reconstruction_loss: 4530.0894 - kl_loss: 2322.8809 - perceptual_loss: 4.7378 - val_loss: 3213.7275 - val_reconstruction_loss: 2340.1426 - val_kl_loss: 870.7963 - val_perceptual_loss: 2.7889 - lr: 6.2500e-06\n",
      "Epoch 237/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6808.2667 - reconstruction_loss: 4472.6484 - kl_loss: 2337.8921 - perceptual_loss: 4.7087\n",
      "Epoch 237: val_reconstruction_loss improved from 2175.27393 to 2160.36304, saving model to checkpoints\\fourierFalse_percepTrue_random_8\\checkpoint_fourierFalse_percepTrue_random_8_vae_best\n",
      "110/110 [==============================] - 19s 171ms/step - loss: 6808.3296 - reconstruction_loss: 4472.6484 - kl_loss: 2337.8921 - perceptual_loss: 4.7087 - val_loss: 3038.0356 - val_reconstruction_loss: 2160.3630 - val_kl_loss: 875.0369 - val_perceptual_loss: 2.6356 - lr: 6.2500e-06\n",
      "Epoch 238/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 7082.1003 - reconstruction_loss: 4487.9795 - kl_loss: 2346.5493 - perceptual_loss: 4.6927\n",
      "Epoch 238: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 7079.9122 - reconstruction_loss: 4487.9795 - kl_loss: 2346.5493 - perceptual_loss: 4.6927 - val_loss: 3215.1602 - val_reconstruction_loss: 2329.7446 - val_kl_loss: 882.6129 - val_perceptual_loss: 2.8028 - lr: 6.2500e-06\n",
      "Epoch 239/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6644.7259 - reconstruction_loss: 4462.8662 - kl_loss: 2357.4534 - perceptual_loss: 4.6971\n",
      "Epoch 239: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 6646.3501 - reconstruction_loss: 4462.8662 - kl_loss: 2357.4534 - perceptual_loss: 4.6971 - val_loss: 3182.6421 - val_reconstruction_loss: 2297.6182 - val_kl_loss: 882.2950 - val_perceptual_loss: 2.7289 - lr: 6.2500e-06\n",
      "Epoch 240/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6887.4183 - reconstruction_loss: 4446.1289 - kl_loss: 2349.4504 - perceptual_loss: 4.6709\n",
      "Epoch 240: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6886.6331 - reconstruction_loss: 4446.1289 - kl_loss: 2349.4504 - perceptual_loss: 4.6709 - val_loss: 3137.6394 - val_reconstruction_loss: 2257.9336 - val_kl_loss: 876.9488 - val_perceptual_loss: 2.7572 - lr: 6.2500e-06\n",
      "Epoch 241/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6951.1953 - reconstruction_loss: 4493.3794 - kl_loss: 2331.4421 - perceptual_loss: 4.6940\n",
      "Epoch 241: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6950.0991 - reconstruction_loss: 4493.3794 - kl_loss: 2331.4421 - perceptual_loss: 4.6940 - val_loss: 3117.4941 - val_reconstruction_loss: 2250.9834 - val_kl_loss: 863.7834 - val_perceptual_loss: 2.7274 - lr: 6.2500e-06\n",
      "Epoch 242/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6945.6185 - reconstruction_loss: 4495.3867 - kl_loss: 2340.1125 - perceptual_loss: 4.6746\n",
      "Epoch 242: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 161ms/step - loss: 6944.6686 - reconstruction_loss: 4495.3867 - kl_loss: 2340.1125 - perceptual_loss: 4.6746 - val_loss: 3134.1548 - val_reconstruction_loss: 2251.7485 - val_kl_loss: 879.8081 - val_perceptual_loss: 2.5981 - lr: 6.2500e-06\n",
      "Epoch 243/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6906.8865 - reconstruction_loss: 4425.6099 - kl_loss: 2353.3784 - perceptual_loss: 4.6931\n",
      "Epoch 243: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6905.7765 - reconstruction_loss: 4425.6099 - kl_loss: 2353.3784 - perceptual_loss: 4.6931 - val_loss: 3065.3726 - val_reconstruction_loss: 2185.6113 - val_kl_loss: 877.0079 - val_perceptual_loss: 2.7531 - lr: 6.2500e-06\n",
      "Epoch 244/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6876.8366 - reconstruction_loss: 4410.4829 - kl_loss: 2348.0222 - perceptual_loss: 4.6865\n",
      "Epoch 244: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6875.8127 - reconstruction_loss: 4410.4829 - kl_loss: 2348.0222 - perceptual_loss: 4.6865 - val_loss: 3104.7549 - val_reconstruction_loss: 2222.9907 - val_kl_loss: 878.9000 - val_perceptual_loss: 2.8642 - lr: 6.2500e-06\n",
      "Epoch 245/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6793.0180 - reconstruction_loss: 4457.9673 - kl_loss: 2340.3015 - perceptual_loss: 4.6834\n",
      "Epoch 245: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 6793.1075 - reconstruction_loss: 4457.9673 - kl_loss: 2340.3015 - perceptual_loss: 4.6834 - val_loss: 3165.0022 - val_reconstruction_loss: 2286.7134 - val_kl_loss: 875.6666 - val_perceptual_loss: 2.6224 - lr: 6.2500e-06\n",
      "Epoch 246/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6660.8588 - reconstruction_loss: 4434.0864 - kl_loss: 2330.0679 - perceptual_loss: 4.6895\n",
      "Epoch 246: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6661.8316 - reconstruction_loss: 4434.0864 - kl_loss: 2330.0679 - perceptual_loss: 4.6895 - val_loss: 3114.9578 - val_reconstruction_loss: 2238.7671 - val_kl_loss: 873.3857 - val_perceptual_loss: 2.8049 - lr: 6.2500e-06\n",
      "Epoch 247/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6644.2105 - reconstruction_loss: 4435.4077 - kl_loss: 2338.4390 - perceptual_loss: 4.6597\n",
      "Epoch 247: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 158ms/step - loss: 6645.4204 - reconstruction_loss: 4435.4077 - kl_loss: 2338.4390 - perceptual_loss: 4.6597 - val_loss: 3179.8103 - val_reconstruction_loss: 2299.7610 - val_kl_loss: 877.3320 - val_perceptual_loss: 2.7172 - lr: 6.2500e-06\n",
      "Epoch 248/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6759.9505 - reconstruction_loss: 4458.3311 - kl_loss: 2348.7844 - perceptual_loss: 4.6420\n",
      "Epoch 248: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6760.4172 - reconstruction_loss: 4458.3311 - kl_loss: 2348.7844 - perceptual_loss: 4.6420 - val_loss: 3251.5742 - val_reconstruction_loss: 2368.3965 - val_kl_loss: 880.3539 - val_perceptual_loss: 2.8238 - lr: 6.2500e-06\n",
      "Epoch 249/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6616.5240 - reconstruction_loss: 4410.7446 - kl_loss: 2350.6521 - perceptual_loss: 4.6518\n",
      "Epoch 249: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 160ms/step - loss: 6617.8710 - reconstruction_loss: 4410.7446 - kl_loss: 2350.6521 - perceptual_loss: 4.6518 - val_loss: 3092.0947 - val_reconstruction_loss: 2215.7666 - val_kl_loss: 873.4503 - val_perceptual_loss: 2.8779 - lr: 6.2500e-06\n",
      "Epoch 250/250\n",
      "110/110 [==============================] - ETA: 0s - loss: 6687.8697 - reconstruction_loss: 4419.6045 - kl_loss: 2345.2781 - perceptual_loss: 4.6209\n",
      "Epoch 250: val_reconstruction_loss did not improve from 2160.36304\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 6688.6051 - reconstruction_loss: 4419.6045 - kl_loss: 2345.2781 - perceptual_loss: 4.6209 - val_loss: 3221.6174 - val_reconstruction_loss: 2343.2756 - val_kl_loss: 875.3943 - val_perceptual_loss: 2.9474 - lr: 6.2500e-06\n",
      "‚úÖ VAE training completed. Models saved to checkpoints\\fourierFalse_percepTrue_random_8\n",
      "‚úÖ VAE training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Train VAE model using FLRTrainer\n",
    "train_vae_model = True  # Set to False if you want to skip VAE training\n",
    "if train_vae_model:\n",
    "    print(\"\\nüß† Starting VAE Training with Coordinate-Aware Data...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # IMPORTANT: Clear any existing VAE model to ensure fresh training\n",
    "    if hasattr(trainer, 'vae_model') and trainer.vae_model is not None:\n",
    "        print(\"üîÑ Clearing existing VAE model to retrain from scratch...\")\n",
    "        del trainer.vae_model\n",
    "        trainer.vae_model = None\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Remove any existing VAE checkpoint files to force retraining\n",
    "    import glob\n",
    "    vae_checkpoint_pattern = os.path.join(config['checkpoint_dir'], \"*vae*\")\n",
    "    existing_checkpoints = glob.glob(vae_checkpoint_pattern)\n",
    "    if existing_checkpoints:\n",
    "        print(f\"üóëÔ∏è  Found {len(existing_checkpoints)} existing VAE checkpoints - will retrain from scratch\")\n",
    "        for checkpoint in existing_checkpoints:\n",
    "            print(f\"   Found: {checkpoint}\")\n",
    "    \n",
    "    print(f\"\\nüåä Training VAE with Fourier features: {config['use_fourier']}\")\n",
    "    if config['use_fourier']:\n",
    "        print(\"   VAE will be trained with coordinate-aware data for proper Fourier support\")\n",
    "    else:\n",
    "        print(\"   VAE will be trained with standard field data\")\n",
    "    \n",
    "    print(\"\\nüõ°Ô∏è Note: Checkpoint saving is disabled for the first 30 epochs to avoid Windows file lock issues.\")\n",
    "    print(\"   Saving will automatically start from epoch 31 onward.\")\n",
    "    print(\"   This ensures robust training without checkpoint file access conflicts.\")\n",
    "    \n",
    "    # Train VAE using the trainer with coordinate-aware datasets\n",
    "    vae_model = trainer.train_vae(\n",
    "        train_dataset=vae_train_dataset,\n",
    "        val_dataset=vae_test_dataset,\n",
    "        epochs=config['vae_epochs'],\n",
    "        learning_rate=config['vae_learning_rate'],\n",
    "        latent_dims=config['latent_dims'],\n",
    "        n_base_features=config['n_base_features'],\n",
    "        use_perceptual_loss=config['use_perceptual_loss'],\n",
    "        patience=config['patience'],\n",
    "        reduce_lr_patience=config['reduce_lr_patience']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ VAE training completed successfully!\")\n",
    "    \n",
    "    # Validate that the VAE now works with coordinate data (if using Fourier)\n",
    "    if config['use_fourier'] and vae_model is not None:\n",
    "        print(\"\\nüîç Validating Fourier VAE with coordinate input...\")\n",
    "        try:\n",
    "            # Test with a small batch\n",
    "            test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "            inputs, targets = test_batch\n",
    "            \n",
    "            if isinstance(inputs, tuple):\n",
    "                field_input, coord_input = inputs\n",
    "                print(f\"   Testing VAE with field shape: {field_input.shape}\")\n",
    "                print(f\"   Testing VAE with coord shape: {coord_input.shape}\")\n",
    "                \n",
    "                # Test prediction\n",
    "                prediction = vae_model([field_input[:1], coord_input[:1]])\n",
    "                print(f\"   ‚úÖ VAE prediction successful! Output shape: {prediction.shape}\")\n",
    "                print(f\"   üéâ VAE now properly supports Fourier features with coordinates!\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Unexpected input format: {type(inputs)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå VAE validation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping VAE training. Loading existing model...\")\n",
    "    # The trainer will handle loading existing VAE weights automatically\n",
    "    vae_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33294c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE using the trainer with coordinate-aware datasets\n",
    "vae_model = trainer.train_vae(\n",
    "    train_dataset=vae_train_dataset,\n",
    "    val_dataset=vae_test_dataset,\n",
    "    epochs=50,#config['vae_epochs'],\n",
    "    learning_rate=1e-5,#config['vae_learning_rate'],\n",
    "    latent_dims=config['latent_dims'],\n",
    "    n_base_features=config['n_base_features'],\n",
    "    use_perceptual_loss=True,\n",
    "    patience=config['patience'],\n",
    "    reduce_lr_patience=config['reduce_lr_patience']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687601d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FLRNet model using FLRTrainer\n",
    "if train_flrnet_model:\n",
    "    print(\"\\nüîÑ Starting FLRNet Training using FLRTrainer...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Train FLRNet using the trainer\n",
    "    flr_model = trainer.train_flr_net(\n",
    "        train_dataset=flrnet_train_dataset,\n",
    "        val_dataset=flrnet_test_dataset,\n",
    "        n_sensors=config['n_sensors'],\n",
    "        epochs=config['flr_epochs'],\n",
    "        learning_rate=config['flr_learning_rate'],\n",
    "        pretrained_vae=vae_model,  # Use the VAE model we just trained\n",
    "        latent_dims=config['latent_dims'],\n",
    "        n_base_features=config['n_base_features'],\n",
    "        use_perceptual_loss=config['use_perceptual_loss'],\n",
    "        patience=config['patience'],\n",
    "        reduce_lr_patience=config['reduce_lr_patience']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ FLRNet training completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping FLRNet training. Loading existing model...\")\n",
    "    # The trainer will handle loading existing FLRNet weights automatically\n",
    "    flr_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37445af",
   "metadata": {},
   "source": [
    "## VAE Validation and Visualization\n",
    "\n",
    "After training the VAE, we can evaluate its performance by:\n",
    "1. Visualizing reconstructed flow fields\n",
    "2. Computing reconstruction errors\n",
    "3. Examining the latent space representation\n",
    "\n",
    "This section can be skipped if VAE training was disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a proper Fourier-aware VAE dataset for testing\n",
    "if train_vae_model and trainer.vae_model is not None and trainer.vae_model.use_fourier:\n",
    "    print(\"=== Creating Fourier-Aware VAE Test Dataset ===\")\n",
    "    \n",
    "    # Function to add coordinate grids to VAE data\n",
    "    def add_coordinates_to_vae_data(field_input, field_target):\n",
    "        # Get batch size and shape\n",
    "        batch_size = tf.shape(field_input)[0]\n",
    "        height = tf.shape(field_input)[1]\n",
    "        width = tf.shape(field_input)[2]\n",
    "        \n",
    "        # Create coordinate grid (normalized to [0, 1])\n",
    "        y_coords = tf.linspace(0.0, 1.0, height)\n",
    "        x_coords = tf.linspace(0.0, 1.0, width)\n",
    "        \n",
    "        # Create meshgrid\n",
    "        x_grid, y_grid = tf.meshgrid(x_coords, y_coords, indexing='ij')\n",
    "        \n",
    "        # Stack to create coordinate grid (height, width, 2)\n",
    "        coord_grid = tf.stack([x_grid, y_grid], axis=-1)\n",
    "        \n",
    "        # Expand to batch size (batch_size, height, width, 2)\n",
    "        coord_batch = tf.tile(tf.expand_dims(coord_grid, 0), [batch_size, 1, 1, 1])\n",
    "        \n",
    "        # Return in the format expected by Fourier VAE: ((img, coord), target)\n",
    "        return ((field_input, coord_batch), field_target)\n",
    "    \n",
    "    # Create Fourier-aware VAE dataset\n",
    "    fourier_vae_test_dataset = vae_test_dataset.map(add_coordinates_to_vae_data)\n",
    "    \n",
    "    print(\"‚úÖ Fourier-aware VAE test dataset created\")\n",
    "    \n",
    "    # Test with a small batch\n",
    "    test_batch = next(iter(fourier_vae_test_dataset.take(1)))\n",
    "    \n",
    "    print(f\"Fourier VAE batch structure:\")\n",
    "    inputs, target = test_batch\n",
    "    if isinstance(inputs, tuple):\n",
    "        img_input, coord_input = inputs\n",
    "        print(f\"  Image input shape: {img_input.shape}\")\n",
    "        print(f\"  Coordinate input shape: {coord_input.shape}\")\n",
    "        print(f\"  Target shape: {target.shape}\")\n",
    "        \n",
    "        # Test VAE prediction with proper format\n",
    "        try:\n",
    "            # Take just 4 samples for testing\n",
    "            test_img = img_input[:4]\n",
    "            test_coord = coord_input[:4]\n",
    "            test_target = target[:4]\n",
    "            \n",
    "            print(f\"\\\\nTesting VAE with proper Fourier input format...\")\n",
    "            reconstructed = trainer.vae_model([test_img, test_coord])\n",
    "            \n",
    "            print(f\"‚úÖ Fourier VAE prediction successful!\")\n",
    "            print(f\"   Input shape: {test_img.shape}\")\n",
    "            print(f\"   Coordinate shape: {test_coord.shape}\")\n",
    "            print(f\"   Reconstruction shape: {reconstructed.shape}\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = tf.reduce_mean(tf.square(test_target - reconstructed)).numpy()\n",
    "            mae = tf.reduce_mean(tf.abs(test_target - reconstructed)).numpy()\n",
    "            \n",
    "            print(f\"   Reconstruction MSE: {mse:.6f}\")\n",
    "            print(f\"   Reconstruction MAE: {mae:.6f}\")\n",
    "            \n",
    "            # Visualization\n",
    "            plt.figure(figsize=(16, 8))\n",
    "            \n",
    "            # Show first 4 samples: original and reconstructed\n",
    "            for i in range(4):\n",
    "                # Original\n",
    "                plt.subplot(2, 4, i + 1)\n",
    "                plt.imshow(test_img[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "                plt.title(f'Original {i+1}')\n",
    "                plt.colorbar()\n",
    "                \n",
    "                # Reconstructed\n",
    "                plt.subplot(2, 4, i + 5)\n",
    "                plt.imshow(reconstructed[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "                plt.title(f'Fourier Reconstructed {i+1}')\n",
    "                plt.colorbar()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle('VAE with Fourier Features: Original vs Reconstructed', y=1.02)\n",
    "            plt.show()\n",
    "            \n",
    "            # Error visualization\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            for i in range(4):\n",
    "                plt.subplot(1, 4, i + 1)\n",
    "                error = tf.abs(test_img[i, :, :, 0] - reconstructed[i, :, :, 0]).numpy()\n",
    "                plt.imshow(error, cmap='hot', origin='lower')\n",
    "                plt.title(f'Error {i+1}')\n",
    "                plt.colorbar()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle('VAE Fourier Reconstruction Errors', y=1.02)\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"\\\\nüéâ VAE with Fourier features validation completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Fourier VAE prediction failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"‚ùå Unexpected data format\")\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping Fourier VAE test (VAE not trained or not using Fourier features)\")\n",
    "\n",
    "# VAE Validation and Visualization\n",
    "print(\"=== VAE Model Validation ===\")\n",
    "\n",
    "if train_vae_model and trainer.vae_model is not None:\n",
    "    print(f\"üéØ Validating VAE model (Fourier: {config['use_fourier']})\")\n",
    "    \n",
    "    try:\n",
    "        # Get a test batch from the existing VAE test dataset\n",
    "        test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "        inputs, targets = test_batch\n",
    "        \n",
    "        if config['use_fourier']:\n",
    "            # For Fourier VAE, inputs should be a tuple (field, coordinates)\n",
    "            if isinstance(inputs, tuple) and len(inputs) == 2:\n",
    "                field_input, coord_input = inputs\n",
    "                \n",
    "                print(f\"‚úÖ Fourier VAE dataset structure correct:\")\n",
    "                print(f\"   Field input shape: {field_input.shape}\")\n",
    "                print(f\"   Coordinate input shape: {coord_input.shape}\")\n",
    "                print(f\"   Target shape: {targets.shape}\")\n",
    "                \n",
    "                # Test with first 4 samples\n",
    "                test_field = field_input[:4]\n",
    "                test_coord = coord_input[:4]\n",
    "                test_targets = targets[:4]\n",
    "                \n",
    "                # VAE prediction with coordinates\n",
    "                reconstructed = trainer.vae_model([test_field, test_coord])\n",
    "                \n",
    "                print(f\"‚úÖ Fourier VAE prediction successful!\")\n",
    "                print(f\"   Reconstruction shape: {reconstructed.shape}\")\n",
    "                \n",
    "                # Calculate metrics\n",
    "                mse = tf.reduce_mean(tf.square(test_targets - reconstructed)).numpy()\n",
    "                mae = tf.reduce_mean(tf.abs(test_targets - reconstructed)).numpy()\n",
    "                \n",
    "                print(f\"   Reconstruction MSE: {mse:.6f}\")\n",
    "                print(f\"   Reconstruction MAE: {mae:.6f}\")\n",
    "                \n",
    "                # Visualization\n",
    "                plt.figure(figsize=(16, 8))\n",
    "                \n",
    "                # Show original vs reconstructed\n",
    "                for i in range(4):\n",
    "                    # Original\n",
    "                    plt.subplot(2, 4, i + 1)\n",
    "                    plt.imshow(test_field[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "                    plt.title(f'Original {i+1}')\n",
    "                    plt.colorbar()\n",
    "                    \n",
    "                    # Reconstructed\n",
    "                    plt.subplot(2, 4, i + 5)\n",
    "                    plt.imshow(reconstructed[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "                    plt.title(f'VAE Fourier Recon {i+1}')\n",
    "                    plt.colorbar()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.suptitle('VAE with Fourier Features: Original vs Reconstructed', y=1.02)\n",
    "                plt.show()\n",
    "                \n",
    "                # Error visualization\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                for i in range(4):\n",
    "                    plt.subplot(1, 4, i + 1)\n",
    "                    error = tf.abs(test_field[i, :, :, 0] - reconstructed[i, :, :, 0]).numpy()\n",
    "                    plt.imshow(error, cmap='hot', origin='lower')\n",
    "                    plt.title(f'Error {i+1}')\n",
    "                    plt.colorbar()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.suptitle('VAE Fourier Reconstruction Errors', y=1.02)\n",
    "                plt.show()\n",
    "                \n",
    "                print(\"üéâ FOURIER VAE VALIDATION SUCCESSFUL!\")\n",
    "                print(\"   The VAE has been properly retrained with coordinate-aware data!\")\n",
    "                print(\"   Fourier features are working correctly!\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"‚ùå Unexpected Fourier VAE input format: {type(inputs)}\")\n",
    "                print(\"   Expected tuple (field, coordinates)\")\n",
    "        else:\n",
    "            # For standard VAE, inputs should be just the field data\n",
    "            print(f\"‚úÖ Standard VAE dataset structure:\")\n",
    "            print(f\"   Input shape: {inputs.shape}\")\n",
    "            print(f\"   Target shape: {targets.shape}\")\n",
    "            \n",
    "            # Test with first 4 samples\n",
    "            test_inputs = inputs[:4]\n",
    "            test_targets = targets[:4]\n",
    "            \n",
    "            # VAE prediction without coordinates\n",
    "            reconstructed = trainer.vae_model(test_inputs)\n",
    "            \n",
    "            print(f\"‚úÖ Standard VAE prediction successful!\")\n",
    "            print(f\"   Reconstruction shape: {reconstructed.shape}\")\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = tf.reduce_mean(tf.square(test_targets - reconstructed)).numpy()\n",
    "            mae = tf.reduce_mean(tf.abs(test_targets - reconstructed)).numpy()\n",
    "            \n",
    "            print(f\"   Reconstruction MSE: {mse:.6f}\")\n",
    "            print(f\"   Reconstruction MAE: {mae:.6f}\")\n",
    "            \n",
    "            print(\"‚úÖ STANDARD VAE VALIDATION SUCCESSFUL!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå VAE validation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping VAE validation (VAE training disabled or model not available)\")\n",
    "\n",
    "# Test the retrained VAE with coordinate inputs\n",
    "print(\"=== Testing Retrained VAE with Coordinate Inputs ===\")\n",
    "\n",
    "if trainer.vae_model is not None and config['use_fourier']:\n",
    "    print(\"üåä Testing Fourier-aware VAE...\")\n",
    "    \n",
    "    try:\n",
    "        # Get a test batch from the VAE dataset\n",
    "        test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "        inputs, targets = test_batch\n",
    "        \n",
    "        if isinstance(inputs, tuple) and len(inputs) == 2:\n",
    "            field_input, coord_input = inputs\n",
    "            \n",
    "            print(f\"‚úÖ VAE dataset structure is correct:\")\n",
    "            print(f\"   Field input shape: {field_input.shape}\")\n",
    "            print(f\"   Coordinate input shape: {coord_input.shape}\")\n",
    "            print(f\"   Target shape: {targets.shape}\")\n",
    "            \n",
    "            # Test with just 1 sample for quick verification\n",
    "            test_field = field_input[:1]\n",
    "            test_coord = coord_input[:1]\n",
    "            test_target = targets[:1]\n",
    "            \n",
    "            # Test VAE prediction\n",
    "            print(f\"\\nüîç Testing VAE prediction...\")\n",
    "            reconstructed = trainer.vae_model([test_field, test_coord])\n",
    "            \n",
    "            print(f\"‚úÖ SUCCESS! VAE now works with coordinate input!\")\n",
    "            print(f\"   Input field shape: {test_field.shape}\")\n",
    "            print(f\"   Input coord shape: {test_coord.shape}\")\n",
    "            print(f\"   Reconstruction shape: {reconstructed.shape}\")\n",
    "            \n",
    "            # Calculate error\n",
    "            mse = tf.reduce_mean(tf.square(test_target - reconstructed)).numpy()\n",
    "            print(f\"   Reconstruction MSE: {mse:.6f}\")\n",
    "            \n",
    "            # Visualization of the first sample\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            # Original\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(test_field[0, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "            plt.title('Original Field')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            # Reconstructed\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(reconstructed[0, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "            plt.title('VAE Reconstruction (with Fourier)')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            # Error\n",
    "            plt.subplot(1, 3, 3)\n",
    "            error = tf.abs(test_field[0, :, :, 0] - reconstructed[0, :, :, 0]).numpy()\n",
    "            plt.imshow(error, cmap='hot', origin='lower')\n",
    "            plt.title('Reconstruction Error')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle('VAE Successfully Retrained with Fourier Features!', y=1.05)\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nüéâ MISSION ACCOMPLISHED!\")\n",
    "            print(f\"   The VAE has been successfully retrained with coordinate-aware data!\")\n",
    "            print(f\"   Fourier features are now working properly!\")\n",
    "            print(f\"   The model expects 2-channel coordinate input as designed!\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ùå Unexpected VAE dataset format: {type(inputs)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå VAE test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå VAE model not available or not using Fourier features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ce05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN VAE Validation (No Cached Functions)\n",
    "print(\"=== CLEAN VAE Validation Test ===\")\n",
    "\n",
    "if 'trainer' in locals() and trainer.vae_model is not None:\n",
    "    print(f\"üéØ Testing VAE model (Fourier: {config['use_fourier']})\")\n",
    "    \n",
    "    try:\n",
    "        # Clear any old cached functions by getting fresh data\n",
    "        test_data_iter = iter(vae_test_dataset.take(1))\n",
    "        test_batch = next(test_data_iter)\n",
    "        inputs, targets = test_batch\n",
    "        \n",
    "        if config['use_fourier']:\n",
    "            # Expected format: ((field, coordinates), targets)\n",
    "            if isinstance(inputs, tuple) and len(inputs) == 2:\n",
    "                field_input, coord_input = inputs\n",
    "                \n",
    "                print(f\"‚úÖ VAE input structure correct:\")\n",
    "                print(f\"   Field: {field_input.shape}\")\n",
    "                print(f\"   Coord: {coord_input.shape}\")\n",
    "                print(f\"   Target: {targets.shape}\")\n",
    "                \n",
    "                # Test with just 1 sample\n",
    "                test_field = field_input[:1]\n",
    "                test_coord = coord_input[:1]\n",
    "                test_target = targets[:1]\n",
    "                \n",
    "                # VAE prediction\n",
    "                reconstructed = trainer.vae_model([test_field, test_coord])\n",
    "                \n",
    "                print(f\"‚úÖ VAE prediction successful!\")\n",
    "                print(f\"   Reconstruction shape: {reconstructed.shape}\")\n",
    "                \n",
    "                # Calculate MSE\n",
    "                mse = tf.reduce_mean(tf.square(test_target - reconstructed)).numpy()\n",
    "                print(f\"   MSE: {mse:.6f}\")\n",
    "                \n",
    "                print(\"üéâ SUCCESS: VAE with Fourier features is working correctly!\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"‚ùå Unexpected input format: {type(inputs)}\")\n",
    "                \n",
    "        else:\n",
    "            # Standard VAE test\n",
    "            print(f\"‚úÖ Standard VAE input: {inputs.shape}\")\n",
    "            reconstructed = trainer.vae_model(inputs[:1])\n",
    "            print(f\"‚úÖ Standard VAE working!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå VAE test failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No trainer or VAE model available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a427fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate how the VAE model was actually built during training\n",
    "print(\"=== VAE Model Architecture Investigation ===\")\n",
    "\n",
    "if trainer.vae_model is not None:\n",
    "    print(f\"VAE model use_fourier: {trainer.vae_model.use_fourier}\")\n",
    "    \n",
    "    # Check the encoder structure\n",
    "    encoder = trainer.vae_model.encoder\n",
    "    print(f\"\\\\nEncoder type: {type(encoder)}\")\n",
    "    print(f\"Encoder use_fourier: {encoder.use_fourier}\")\n",
    "    \n",
    "    # Check if encoder has fourier layer\n",
    "    if hasattr(encoder, 'fourier_layer') and encoder.fourier_layer:\n",
    "        fourier_layer = encoder.fourier_layer\n",
    "        print(f\"\\\\nFourier layer found: {fourier_layer}\")\n",
    "        print(f\"  Type: {type(fourier_layer)}\")\n",
    "        if hasattr(fourier_layer, 'built') and fourier_layer.built:\n",
    "            print(f\"  Layer is built: {fourier_layer.built}\")\n",
    "            # Check the projection kernel\n",
    "            if hasattr(fourier_layer, 'proj_kernel'):\n",
    "                proj_kernel = fourier_layer.proj_kernel\n",
    "                print(f\"  Projection kernel: {proj_kernel}\")\n",
    "                if hasattr(proj_kernel, 'kernel') and proj_kernel.kernel is not None:\n",
    "                    kernel_shape = proj_kernel.kernel.shape\n",
    "                    print(f\"  Conv2D kernel shape: {kernel_shape}\")\n",
    "                    print(f\"  Expected input channels: {kernel_shape[2]}\")\n",
    "                    print(f\"  Output channels: {kernel_shape[3]}\")\n",
    "    else:\n",
    "        print(\"\\\\nNo Fourier layer found in encoder\")\n",
    "    \n",
    "    # Test what happens if we try to call the model with a simple input\n",
    "    print(f\"\\\\n=== Testing with Simple Field Input ===\")\n",
    "    try:\n",
    "        # Create a simple test input (just the field)\n",
    "        test_field = tf.zeros((1, 128, 256, 1))\n",
    "        \n",
    "        # Try calling just the encoder with the field (no coordinates)\n",
    "        print(\"Testing encoder with field input only...\")\n",
    "        # encoder_out = encoder(test_field)\n",
    "        # print(f\"Encoder output successful with field only!\")\n",
    "        \n",
    "        # Try calling the full VAE model with just field input\n",
    "        print(\"Testing full VAE with field input only...\")\n",
    "        vae_out = trainer.vae_model(test_field)\n",
    "        print(f\"‚úÖ VAE works with field input only! Output shape: {vae_out.shape}\")\n",
    "        print(\"This suggests the model was trained without proper Fourier coordinates!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Simple field input failed: {e}\")\n",
    "        \n",
    "    # Check how the training dataset actually looked\n",
    "    print(f\"\\\\n=== VAE Training Dataset Analysis ===\")\n",
    "    sample_train_batch = next(iter(vae_train_dataset.take(1)))\n",
    "    print(f\"Training batch structure: {type(sample_train_batch)}\")\n",
    "    if isinstance(sample_train_batch, tuple):\n",
    "        print(f\"  Input shape: {sample_train_batch[0].shape}\")\n",
    "        print(f\"  Target shape: {sample_train_batch[1].shape}\")\n",
    "        print(\"  The training data contains only field data, no coordinates!\")\n",
    "        print(\"  This explains why Fourier features don't work properly.\")\n",
    "        \n",
    "else:\n",
    "    print(\"No VAE model available\")\n",
    "\n",
    "# Verify that the VAE model architecture is now correct for Fourier features\n",
    "print(\"=== VAE Model Architecture Verification ===\")\n",
    "\n",
    "if trainer.vae_model is not None:\n",
    "    print(f\"VAE model use_fourier: {trainer.vae_model.use_fourier}\")\n",
    "    \n",
    "    if trainer.vae_model.use_fourier:\n",
    "        print(\"\\nüåä Analyzing Fourier-aware VAE architecture...\")\n",
    "        \n",
    "        # Check the encoder structure\n",
    "        encoder = trainer.vae_model.encoder\n",
    "        print(f\"Encoder type: {type(encoder)}\")\n",
    "        print(f\"Encoder use_fourier: {encoder.use_fourier}\")\n",
    "        \n",
    "        # Check if encoder has fourier layer\n",
    "        if hasattr(encoder, 'fourier_layer') and encoder.fourier_layer:\n",
    "            fourier_layer = encoder.fourier_layer\n",
    "            print(f\"\\n‚úÖ Fourier layer found: {fourier_layer}\")\n",
    "            print(f\"  Type: {type(fourier_layer)}\")\n",
    "            \n",
    "            if hasattr(fourier_layer, 'built') and fourier_layer.built:\n",
    "                print(f\"  Layer is built: {fourier_layer.built}\")\n",
    "                \n",
    "                # Check the projection kernel\n",
    "                if hasattr(fourier_layer, 'proj_kernel'):\n",
    "                    proj_kernel = fourier_layer.proj_kernel\n",
    "                    print(f\"  Projection kernel: {proj_kernel}\")\n",
    "                    if hasattr(proj_kernel, 'kernel') and proj_kernel.kernel is not None:\n",
    "                        kernel_shape = proj_kernel.kernel.shape\n",
    "                        print(f\"  Conv2D kernel shape: {kernel_shape}\")\n",
    "                        print(f\"  Expected input channels: {kernel_shape[2]}\")\n",
    "                        print(f\"  Output channels: {kernel_shape[3]}\")\n",
    "                        \n",
    "                        # Verify proper 2-channel input for coordinates\n",
    "                        if kernel_shape[2] == 2:\n",
    "                            print(f\"  ‚úÖ Fourier layer correctly expects 2-channel coordinate input!\")\n",
    "                        else:\n",
    "                            print(f\"  ‚ö†Ô∏è  Fourier layer expects {kernel_shape[2]} channels, should be 2\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  Fourier layer not yet built\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå No Fourier layer found in encoder\")\n",
    "        \n",
    "        # Test the model with coordinate input to build the layers\n",
    "        print(f\"\\nüîç Testing model to ensure proper layer construction...\")\n",
    "        try:\n",
    "            # Create test inputs with the right format\n",
    "            test_field = tf.zeros((1, config['input_shape'][0], config['input_shape'][1], 1))\n",
    "            test_coord = tf.zeros((1, config['input_shape'][0], config['input_shape'][1], 2))\n",
    "            \n",
    "            print(f\"  Test field shape: {test_field.shape}\")\n",
    "            print(f\"  Test coordinate shape: {test_coord.shape}\")\n",
    "            \n",
    "            # This should build the layers properly\n",
    "            output = trainer.vae_model([test_field, test_coord])\n",
    "            print(f\"  ‚úÖ Model successfully processes coordinate input! Output shape: {output.shape}\")\n",
    "            \n",
    "            # Now check the Fourier layer again\n",
    "            if hasattr(encoder, 'fourier_layer') and encoder.fourier_layer:\n",
    "                fourier_layer = encoder.fourier_layer\n",
    "                if hasattr(fourier_layer, 'proj_kernel') and fourier_layer.proj_kernel.kernel is not None:\n",
    "                    kernel_shape = fourier_layer.proj_kernel.kernel.shape\n",
    "                    print(f\"  Final Fourier kernel shape: {kernel_shape}\")\n",
    "                    if kernel_shape[2] == 2:\n",
    "                        print(f\"  üéâ SUCCESS: Fourier layer now properly expects 2-channel coordinates!\")\n",
    "                    else:\n",
    "                        print(f\"  ‚ùå ISSUE: Fourier layer still expects {kernel_shape[2]} channels\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Model test failed: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\nüîÑ Standard VAE (no Fourier features)\")\n",
    "        \n",
    "        # Test standard VAE\n",
    "        try:\n",
    "            test_field = tf.zeros((1, config['input_shape'][0], config['input_shape'][1], 1))\n",
    "            output = trainer.vae_model(test_field)\n",
    "            print(f\"‚úÖ Standard VAE works correctly! Output shape: {output.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Standard VAE test failed: {e}\")\n",
    "    \n",
    "    # Show model summary for reference\n",
    "    print(f\"\\nüìã VAE Model Summary:\")\n",
    "    try:\n",
    "        trainer.vae_model.summary()\n",
    "    except:\n",
    "        print(\"Could not display model summary\")\n",
    "        \n",
    "else:\n",
    "    print(\"No VAE model available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e2163b",
   "metadata": {},
   "source": [
    "## FLRNet Validation and Visualization\n",
    "\n",
    "After training the FLRNet model, we can evaluate its performance by:\n",
    "1. Testing sensor-to-field reconstruction accuracy\n",
    "2. Comparing predicted vs ground truth flow fields\n",
    "3. Analyzing reconstruction quality metrics\n",
    "4. Visualizing sensor positions overlaid on predictions\n",
    "\n",
    "This section validates the main FLRNet model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLRNet Validation and Visualization\n",
    "if train_flrnet_model and trainer.flr_model is not None:\n",
    "    print(\"=== FLRNet Model Validation ===\")\n",
    "    \n",
    "    # Get test samples for evaluation\n",
    "    test_batch = next(iter(flrnet_test_dataset.batch(8)))\n",
    "    sensor_readings = test_batch[0]  # Input sensor readings\n",
    "    ground_truth = test_batch[1]     # Target flow fields\n",
    "    \n",
    "    # Get FLRNet predictions\n",
    "    predictions = trainer.flr_model.predict(sensor_readings)\n",
    "    \n",
    "    # Calculate reconstruction metrics\n",
    "    mse = np.mean((ground_truth.numpy() - predictions) ** 2)\n",
    "    mae = np.mean(np.abs(ground_truth.numpy() - predictions))\n",
    "    \n",
    "    # Calculate relative error\n",
    "    relative_error = np.mean(np.abs(ground_truth.numpy() - predictions) / (np.abs(ground_truth.numpy()) + 1e-8))\n",
    "    \n",
    "    print(f\"FLRNet Reconstruction MSE: {mse:.6f}\")\n",
    "    print(f\"FLRNet Reconstruction MAE: {mae:.6f}\")\n",
    "    print(f\"FLRNet Relative Error: {relative_error:.6f}\")\n",
    "    \n",
    "    # Visualize predictions vs ground truth\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Show first 4 samples: sensor readings, ground truth, predictions, and errors\n",
    "    for i in range(4):\n",
    "        # Sensor readings visualization\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        field_viz = np.zeros((h, w))\n",
    "        # Place sensor readings on the field for visualization\n",
    "        for j, (x_pos, y_pos) in enumerate(sensor_positions):\n",
    "            if j < len(sensor_readings[i]):\n",
    "                field_viz[int(y_pos), int(x_pos)] = sensor_readings[i, j]\n",
    "        plt.imshow(field_viz, cmap='RdBu_r', origin='lower')\n",
    "        plt.title(f'Sensor Readings {i+1}')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Ground truth\n",
    "        plt.subplot(4, 4, i + 5)\n",
    "        plt.imshow(ground_truth[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "        plt.title(f'Ground Truth {i+1}')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Predictions\n",
    "        plt.subplot(4, 4, i + 9)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "        plt.title(f'FLRNet Prediction {i+1}')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Error\n",
    "        plt.subplot(4, 4, i + 13)\n",
    "        error = np.abs(ground_truth[i, :, :, 0] - predictions[i, :, :, 0])\n",
    "        plt.imshow(error, cmap='hot', origin='lower')\n",
    "        plt.title(f'Prediction Error {i+1}')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('FLRNet: Sensor Readings ‚Üí Ground Truth vs Predictions', y=0.98)\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional visualization: Overlay sensor positions on predictions\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    for i in range(2):\n",
    "        # Prediction with sensor overlay\n",
    "        plt.subplot(1, 2, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "        \n",
    "        # Overlay sensor positions\n",
    "        sensor_x = sensor_positions[:, 0]\n",
    "        sensor_y = sensor_positions[:, 1]\n",
    "        plt.scatter(sensor_x, sensor_y, c='black', s=100, marker='o', edgecolors='white', linewidth=2)\n",
    "        \n",
    "        # Add sensor value annotations\n",
    "        for j, (x, y) in enumerate(sensor_positions):\n",
    "            if j < len(sensor_readings[i]):\n",
    "                plt.annotate(f'{sensor_readings[i, j]:.2f}', \n",
    "                           (x, y), xytext=(5, 5), textcoords='offset points',\n",
    "                           fontsize=8, color='white', weight='bold')\n",
    "        \n",
    "        plt.title(f'FLRNet Prediction {i+1} with Sensor Positions')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('FLRNet Predictions with Sensor Position Overlay', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"FLRNet validation completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping FLRNet validation (FLRNet training was disabled or model not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742eb60",
   "metadata": {},
   "source": [
    "## Training Summary and Model Saving\n",
    "\n",
    "Complete the training workflow by saving trained models and summarizing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Summary and Model Saving\n",
    "print(\"=== Training Summary ===\")\n",
    "\n",
    "# Print configuration summary\n",
    "print(f\"Configuration: {config_name}\")\n",
    "print(f\"Layout type: {layout_type}\")\n",
    "print(f\"Number of sensors: {n_sensors}\")\n",
    "print(f\"VAE training: {'Enabled' if train_vae_model else 'Disabled'}\")\n",
    "print(f\"FLRNet training: {'Enabled' if train_flrnet_model else 'Disabled'}\")\n",
    "\n",
    "# Save trained models\n",
    "if trainer.vae_model is not None:\n",
    "    vae_filename = f\"vae_model_{layout_type}_{n_sensors}_sensors.h5\"\n",
    "    trainer.vae_model.save(vae_filename)\n",
    "    print(f\"VAE model saved as: {vae_filename}\")\n",
    "\n",
    "if trainer.flr_model is not None:\n",
    "    flrnet_filename = f\"flrnet_model_{layout_type}_{n_sensors}_sensors.h5\"\n",
    "    trainer.flr_model.save(flrnet_filename)\n",
    "    print(f\"FLRNet model saved as: {flrnet_filename}\")\n",
    "\n",
    "print(\"\\n=== Training Complete ===\")\n",
    "print(\"All requested models have been trained and validated.\")\n",
    "print(\"Models have been saved for future use.\")\n",
    "\n",
    "# üîÑ FLRNet Training Continuation\n",
    "# Toggle for enabling FLRNet training continuation (set to True to continue from checkpoint)\n",
    "continue_flrnet_training = False  # Change to True to enable\n",
    "\n",
    "# Toggle for perceptual loss during continuation (can be different from original training)\n",
    "use_perceptual_loss_flrnet_continuation = True  # Change to False to disable perceptual loss\n",
    "\n",
    "if continue_flrnet_training:\n",
    "    if trainer.flr_model is not None:\n",
    "        print(\"üîÑ Continuing FLRNet training from loaded checkpoint...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Configuration for continued training\n",
    "        additional_epochs = 50\n",
    "        learning_rate = 1e-6  # Lower learning rate for fine-tuning\n",
    "        \n",
    "        print(f\"üìã Continuation Configuration:\")\n",
    "        print(f\"   - Additional epochs: {additional_epochs}\")\n",
    "        print(f\"   - Learning rate: {learning_rate} (reduced for fine-tuning)\")\n",
    "        print(f\"   - Perceptual loss: {'‚úÖ ENABLED' if use_perceptual_loss_flrnet_continuation else '‚ùå DISABLED'}\")\n",
    "        print(f\"   - Model weights: ‚úÖ PRESERVED from checkpoint\")\n",
    "        \n",
    "        # Continue training using the proper method that preserves weights\n",
    "        continued_flrnet = trainer.continue_flrnet_training(\n",
    "            train_dataset=flrnet_train_dataset,\n",
    "            val_dataset=flrnet_test_dataset,\n",
    "            epochs=additional_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            patience=config['patience'],\n",
    "            reduce_lr_patience=config['reduce_lr_patience'],\n",
    "            use_perceptual_loss=use_perceptual_loss_flrnet_continuation  # New option for perceptual loss\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ FLRNet training continuation completed!\")\n",
    "        \n",
    "    elif trainer.vae_model is not None:\n",
    "        print(\"‚ùå Cannot continue FLRNet training: No FLRNet model loaded from checkpoint\")\n",
    "        print(\"   First load an FLRNet model using the checkpoint loading cell above\")\n",
    "    else:\n",
    "        print(\"‚ùå Cannot continue FLRNet training: No models loaded from checkpoint\")\n",
    "        print(\"   First load models using the checkpoint loading cell above\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  FLRNet training continuation is disabled\")\n",
    "    print(\"   Set continue_flrnet_training = True to enable\")\n",
    "    if not continue_flrnet_training:\n",
    "        print(f\"   Current perceptual loss setting: {'‚úÖ ENABLED' if use_perceptual_loss_flrnet_continuation else '‚ùå DISABLED'}\")\n",
    "        print(\"   (can be changed via use_perceptual_loss_flrnet_continuation variable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load VAE Model\n",
    "checkpoint_dir = \"E:/Research/Physics-informed-machine-learning/flow_field_recon_parc/checkpoints/fourierTrue_percepTrue_edge_8\"\n",
    "\n",
    "checkpoint_path = Path(checkpoint_dir)\n",
    "vae_checkpoint_path = checkpoint_path / f\"checkpoint_{config['model_name']}_vae_best\"\n",
    "print(f\"\\nüìÅ Looking for VAE checkpoint: {vae_checkpoint_path}\")\n",
    "\n",
    "if vae_checkpoint_path.exists():\n",
    "    print(\"‚úÖ VAE checkpoint found, creating model...\")\n",
    "    \n",
    "    # Create VAE model architecture\n",
    "    vae_model = models_improved.FLRVAE(\n",
    "        input_shape=config['input_shape'],\n",
    "        latent_dims=config['latent_dims'],\n",
    "        n_base_features=config['n_base_features'],\n",
    "        use_fourier=config['use_fourier'],\n",
    "        use_perceptual_loss=config['use_perceptual_loss']\n",
    "    )\n",
    "    \n",
    "    # Build the model by calling it once with dummy input\n",
    "    dummy_input = tf.zeros((1,) + config['input_shape'])\n",
    "    if config['use_fourier']:\n",
    "        dummy_coord = tf.zeros((1, config['input_shape'][0], config['input_shape'][1], 2))\n",
    "        _ = vae_model([dummy_input, dummy_coord])\n",
    "        print(\"üåä VAE model built for Fourier features\")\n",
    "    else:\n",
    "        _ = vae_model(dummy_input)\n",
    "        print(\"üîÑ VAE model built for standard features\")\n",
    "    \n",
    "    # Load weights\n",
    "    vae_model.load_weights(str(vae_checkpoint_path))\n",
    "    print(f\"‚úÖ VAE weights loaded successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå VAE checkpoint not found at: {vae_checkpoint_path}\")\n",
    "    vae_model = None\n",
    "print(\"üé® === VAE Test Visualization ===\")\n",
    "print(f\"Model type: {'Fourier-aware' if trainer.vae_model.use_fourier else 'Standard'} VAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f87895",
   "metadata": {},
   "source": [
    "## VAE Test Visualization\n",
    "\n",
    "Comprehensive visualization of the VAE model performance with detailed comparisons between original and reconstructed flow fields. This section tests the VAE with both coordinate-aware and standard inputs to validate proper Fourier feature integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20fc2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced VAE Test Visualization (replace the existing cell)\n",
    "\n",
    "# Comprehensive VAE Test Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if trainer.vae_model is not None:\n",
    "    print(\"üé® === VAE Test Visualization ===\")\n",
    "    print(f\"Model type: {'Fourier-aware' if trainer.vae_model.use_fourier else 'Standard'} VAE\")\n",
    "    \n",
    "    # Get test data batch directly from the properly formatted dataset\n",
    "    test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "    print(\"Using coordinate-aware VAE test dataset\")\n",
    "    \n",
    "    # Extract inputs and targets\n",
    "    if isinstance(test_batch, tuple) and len(test_batch) == 2:\n",
    "        test_inputs, test_targets = test_batch\n",
    "        \n",
    "        # Handle coordinate-aware inputs for Fourier VAE\n",
    "        if isinstance(test_inputs, (list, tuple)) and len(test_inputs) == 2:\n",
    "            test_fields, test_coords = test_inputs\n",
    "            print(f\"‚úÖ Fourier VAE inputs detected:\")\n",
    "            print(f\"   Field shape: {test_fields.shape}\")\n",
    "            print(f\"   Coordinate shape: {test_coords.shape}\")\n",
    "            input_for_prediction = [test_fields, test_coords]\n",
    "        else:\n",
    "            test_fields = test_inputs\n",
    "            print(f\"‚úÖ Standard VAE input: {test_fields.shape}\")\n",
    "            input_for_prediction = test_fields\n",
    "        \n",
    "        print(f\"   Target shape: {test_targets.shape}\")\n",
    "        \n",
    "        # Generate VAE reconstructions\n",
    "        print(\"\\nüîÆ Generating VAE reconstructions...\")\n",
    "        try:\n",
    "            # Limit to manageable batch size for visualization\n",
    "            max_samples = min(4, test_fields.shape[0])\n",
    "            \n",
    "            if isinstance(input_for_prediction, list):\n",
    "                limited_input = [test_fields[:max_samples], test_coords[:max_samples]]\n",
    "            else:\n",
    "                limited_input = test_fields[:max_samples]\n",
    "            limited_targets = test_targets[:max_samples]\n",
    "            \n",
    "            reconstructions = trainer.vae_model.predict(limited_input, verbose=0)\n",
    "            print(f\"‚úÖ Reconstruction successful! Shape: {reconstructions.shape}\")\n",
    "            \n",
    "            # Calculate reconstruction metrics\n",
    "            mse = np.mean((limited_targets.numpy() - reconstructions) ** 2)\n",
    "            mae = np.mean(np.abs(limited_targets.numpy() - reconstructions))\n",
    "            max_error = np.max(np.abs(limited_targets.numpy() - reconstructions))\n",
    "            \n",
    "            print(f\"\\nüìä VAE Reconstruction Metrics:\")\n",
    "            print(f\"   MSE: {mse:.6f} ({'Excellent' if mse < 0.01 else 'Good' if mse < 0.05 else 'Needs improvement'})\")\n",
    "            print(f\"   MAE: {mae:.6f}\")\n",
    "            print(f\"   Max Error: {max_error:.6f}\")\n",
    "            \n",
    "            # Create comprehensive visualization\n",
    "            n_samples = max_samples\n",
    "            fig, axes = plt.subplots(3, n_samples, figsize=(5*n_samples, 12))\n",
    "            \n",
    "            if n_samples == 1:\n",
    "                axes = axes.reshape(-1, 1)\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                # Original field\n",
    "                im1 = axes[0, i].imshow(limited_targets[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "                axes[0, i].set_title(f'Original Field {i+1}', fontweight='bold')\n",
    "                axes[0, i].set_xlabel('X Position')\n",
    "                axes[0, i].set_ylabel('Y Position')\n",
    "                plt.colorbar(im1, ax=axes[0, i], shrink=0.8)\n",
    "                \n",
    "                # Reconstructed field\n",
    "                im2 = axes[1, i].imshow(reconstructions[i, :, :, 0], cmap='RdBu_r', origin='lower')\n",
    "                axes[1, i].set_title(f'VAE Reconstruction {i+1}', fontweight='bold')\n",
    "                axes[1, i].set_xlabel('X Position')\n",
    "                axes[1, i].set_ylabel('Y Position')\n",
    "                plt.colorbar(im2, ax=axes[1, i], shrink=0.8)\n",
    "                \n",
    "                # Error map\n",
    "                error = np.abs(limited_targets[i, :, :, 0] - reconstructions[i, :, :, 0])\n",
    "                im3 = axes[2, i].imshow(error, cmap='hot', origin='lower')\n",
    "                axes[2, i].set_title(f'Reconstruction Error {i+1}', fontweight='bold')\n",
    "                axes[2, i].set_xlabel('X Position')\n",
    "                axes[2, i].set_ylabel('Y Position')\n",
    "                plt.colorbar(im3, ax=axes[2, i], shrink=0.8)\n",
    "                \n",
    "                # Add error statistics as text\n",
    "                sample_mse = np.mean(error**2)\n",
    "                sample_max = np.max(error)\n",
    "                axes[2, i].text(0.02, 0.98, f'MSE: {sample_mse:.4f}\\nMax: {sample_max:.4f}', \n",
    "                               transform=axes[2, i].transAxes, verticalalignment='top',\n",
    "                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                               fontsize=9, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f'VAE Reconstruction Results - {config_name}', y=1.02, fontsize=16, fontweight='bold')\n",
    "            plt.show()\n",
    "            \n",
    "            # Statistical analysis\n",
    "            print(f\"\\nüìà Statistical Comparison:\")\n",
    "            orig_stats = {\n",
    "                'mean': np.mean(limited_targets.numpy()),\n",
    "                'std': np.std(limited_targets.numpy()),\n",
    "                'min': np.min(limited_targets.numpy()),\n",
    "                'max': np.max(limited_targets.numpy())\n",
    "            }\n",
    "            \n",
    "            recon_stats = {\n",
    "                'mean': np.mean(reconstructions),\n",
    "                'std': np.std(reconstructions),\n",
    "                'min': np.min(reconstructions),\n",
    "                'max': np.max(reconstructions)\n",
    "            }\n",
    "            \n",
    "            print(f\"   Original  - Mean: {orig_stats['mean']:.4f}, Std: {orig_stats['std']:.4f}, Range: [{orig_stats['min']:.4f}, {orig_stats['max']:.4f}]\")\n",
    "            print(f\"   Reconstructed - Mean: {recon_stats['mean']:.4f}, Std: {recon_stats['std']:.4f}, Range: [{recon_stats['min']:.4f}, {recon_stats['max']:.4f}]\")\n",
    "            \n",
    "            # Distribution comparison\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Value distributions\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.hist(limited_targets.numpy().flatten(), bins=50, alpha=0.7, label='Original', density=True, color='blue')\n",
    "            plt.hist(reconstructions.flatten(), bins=50, alpha=0.7, label='Reconstructed', density=True, color='red')\n",
    "            plt.xlabel('Field Value')\n",
    "            plt.ylabel('Density')\n",
    "            plt.title('Value Distribution Comparison', fontweight='bold')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Error distribution\n",
    "            plt.subplot(1, 3, 2)\n",
    "            errors = np.abs(limited_targets.numpy() - reconstructions).flatten()\n",
    "            plt.hist(errors, bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "            plt.xlabel('Absolute Error')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Reconstruction Error Distribution', fontweight='bold')\n",
    "            plt.axvline(np.mean(errors), color='darkred', linestyle='--', linewidth=2, label=f'Mean: {np.mean(errors):.4f}')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Relative error distribution\n",
    "            plt.subplot(1, 3, 3)\n",
    "            relative_errors = errors / (np.abs(limited_targets.numpy().flatten()) + 1e-8)\n",
    "            plt.hist(relative_errors, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "            plt.xlabel('Relative Error')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Relative Error Distribution', fontweight='bold')\n",
    "            plt.axvline(np.mean(relative_errors), color='darkorange', linestyle='--', linewidth=2, label=f'Mean: {np.mean(relative_errors):.4f}')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle('VAE Performance Analysis', y=1.02, fontsize=14, fontweight='bold')\n",
    "            plt.show()\n",
    "            \n",
    "            # Fourier feature analysis (if applicable)\n",
    "            if trainer.vae_model.use_fourier and isinstance(test_inputs, (list, tuple)):\n",
    "                print(f\"\\nüåä Fourier Feature Analysis:\")\n",
    "                print(f\"   Coordinate grid statistics:\")\n",
    "                print(f\"   X coordinates - Range: [{np.min(test_coords[0, :, :, 0]):.3f}, {np.max(test_coords[0, :, :, 0]):.3f}]\")\n",
    "                print(f\"   Y coordinates - Range: [{np.min(test_coords[0, :, :, 1]):.3f}, {np.max(test_coords[0, :, :, 1]):.3f}]\")\n",
    "                \n",
    "                # Show coordinate grids for first sample\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "                \n",
    "                im1 = axes[0].imshow(test_coords[0, :, :, 0], cmap='viridis', origin='lower')\n",
    "                axes[0].set_title('X Coordinate Grid', fontweight='bold')\n",
    "                axes[0].set_xlabel('X Position')\n",
    "                axes[0].set_ylabel('Y Position')\n",
    "                plt.colorbar(im1, ax=axes[0])\n",
    "                \n",
    "                im2 = axes[1].imshow(test_coords[0, :, :, 1], cmap='plasma', origin='lower')\n",
    "                axes[1].set_title('Y Coordinate Grid', fontweight='bold')\n",
    "                axes[1].set_xlabel('X Position')\n",
    "                axes[1].set_ylabel('Y Position')\n",
    "                plt.colorbar(im2, ax=axes[1])\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.suptitle('Fourier Feature Coordinate Grids', y=1.02, fontsize=14, fontweight='bold')\n",
    "                plt.show()\n",
    "            \n",
    "            print(f\"\\nüéâ VAE visualization completed successfully!\")\n",
    "            print(f\"   üìä Reconstruction Quality: {'üèÜ Excellent' if mse < 0.01 else '‚úÖ Good' if mse < 0.05 else '‚ö†Ô∏è Needs improvement'}\")\n",
    "            print(f\"   üåä Fourier Features: {'‚úÖ Working correctly' if trainer.vae_model.use_fourier else '‚ûñ Not used'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå VAE prediction failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    else:\n",
    "        print(f\"‚ùå Unexpected test batch structure: {type(test_batch)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No VAE model available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277de7f",
   "metadata": {},
   "source": [
    "## Load Trained Models from Checkpoint\n",
    "\n",
    "Now let's load the trained models from saved checkpoints using the FLRTrainer's built-in methods. The trainer provides several loading options:\n",
    "\n",
    "- **`load_vae_from_checkpoint()`**: Load only the VAE model\n",
    "- **`load_flrnet_from_checkpoint()`**: Load only the FLRNet model  \n",
    "- **`load_models_from_checkpoint()`**: Load both models at once\n",
    "\n",
    "### Features:\n",
    "- **Automatic checkpoint detection**: Finds the best available checkpoint (best ‚Üí last ‚Üí final_weights)\n",
    "- **Robust path handling**: Uses correct TensorFlow checkpoint format (no file extensions)\n",
    "- **Error handling**: Graceful fallback and clear error messages\n",
    "- **Architecture consistency**: Ensures loaded model matches trainer configuration\n",
    "- **Smart dependencies**: FLRNet loading automatically handles VAE dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d837893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the VAE model using the trainer's built-in method\n",
    "print(\"üöÄ Loading trained VAE model from checkpoint using FLRTrainer...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fix checkpoint path - use the directory, not the specific file\n",
    "checkpoint_directory = Path(config['checkpoint_dir'])\n",
    "print(f\"üìÇ Using checkpoint directory: {checkpoint_directory}\")\n",
    "\n",
    "# List available checkpoints for debugging\n",
    "if checkpoint_directory.exists():\n",
    "    print(f\"üìã Available checkpoint files:\")\n",
    "    for file in sorted(checkpoint_directory.iterdir()):\n",
    "        print(f\"   - {file.name}\")\n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint directory not found: {checkpoint_directory}\")\n",
    "\n",
    "# Use the trainer's built-in checkpoint loading method\n",
    "vae_model = trainer.load_vae_from_checkpoint(\n",
    "    checkpoint_dir=checkpoint_directory,\n",
    "    latent_dims=config['latent_dims'],\n",
    "    n_base_features=config['n_base_features'],\n",
    "    use_perceptual_loss=config['use_perceptual_loss'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if vae_model is not None:\n",
    "    print(\"\\nüéâ VAE model loaded successfully!\")\n",
    "    print(\"üìà Model is ready for inference and visualization\")\n",
    "    \n",
    "    # Verify the model works with a test prediction\n",
    "    try:\n",
    "        # Get a small batch for testing\n",
    "        test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "        test_inputs, test_targets = test_batch\n",
    "        \n",
    "        # Handle different input formats (Fourier vs Standard VAE)\n",
    "        if isinstance(test_inputs, (list, tuple)) and len(test_inputs) == 2:\n",
    "            # Fourier VAE expects [field, coordinates]\n",
    "            test_field, test_coord = test_inputs\n",
    "            test_prediction = vae_model([test_field[:1], test_coord[:1]])\n",
    "            print(f\"‚úÖ Fourier VAE verification successful!\")\n",
    "            print(f\"   - Field input shape: {test_field[:1].shape}\")\n",
    "            print(f\"   - Coordinate input shape: {test_coord[:1].shape}\")\n",
    "        else:\n",
    "            # Standard VAE expects just field data\n",
    "            test_prediction = vae_model(test_inputs[:1])\n",
    "            print(f\"‚úÖ Standard VAE verification successful!\")\n",
    "            print(f\"   - Input shape: {test_inputs[:1].shape}\")\n",
    "        \n",
    "        print(f\"   - Output shape: {test_prediction.shape}\")\n",
    "        print(f\"   - Output range: [{test_prediction.numpy().min():.4f}, {test_prediction.numpy().max():.4f}]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Model verification failed: {str(e)}\")\n",
    "        print(\"Model loaded but may have compatibility issues\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to load VAE model\")\n",
    "    print(\"üîß Troubleshooting steps:\")\n",
    "    print(\"   1. Check if training completed successfully\")\n",
    "    print(\"   2. Verify checkpoint files exist in the directory\")\n",
    "    print(\"   3. Ensure model configuration matches training setup\")\n",
    "    print(\"   4. Try running the training cell again if checkpoints are missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20499860",
   "metadata": {},
   "source": [
    "## üîÑ Training Continuation with Advanced Options\n",
    "\n",
    "### What is Training Continuation?\n",
    "- **Loading for Inference**: Loads model weights for evaluation/visualization only\n",
    "- **Training Continuation**: Loads model weights AND continues training with preserved state\n",
    "- **Key Benefit**: Preserves trained weights instead of resetting to random initialization\n",
    "\n",
    "### üéõÔ∏è Perceptual Loss Control\n",
    "You can now **override the perceptual loss setting** during training continuation:\n",
    "\n",
    "**For VAE Continuation:**\n",
    "- `use_perceptual_loss_continuation = True`: Enable perceptual loss (better visual quality)\n",
    "- `use_perceptual_loss_continuation = False`: Disable perceptual loss (faster training, MSE only)\n",
    "\n",
    "**For FLRNet Continuation:**\n",
    "- `use_perceptual_loss_flrnet_continuation = True`: Enable perceptual loss in VAE component\n",
    "- `use_perceptual_loss_flrnet_continuation = False`: Disable perceptual loss in VAE component\n",
    "\n",
    "### üîÑ Continuation vs New Training\n",
    "| Method | Model Weights | Optimizer State | Use Case |\n",
    "|--------|--------------|----------------|----------|\n",
    "| `train_vae()` | ‚ùå Reset to random | ‚ùå New optimizer | Fresh training |\n",
    "| `continue_vae_training()` | ‚úÖ Preserved | ‚ùå New optimizer | Fine-tuning/Resume |\n",
    "\n",
    "### ‚ö†Ô∏è Important Notes\n",
    "- **Perceptual Loss Override**: When changing perceptual loss settings, the system automatically initializes/removes metric trackers\n",
    "- **Metric Compatibility**: The system ensures metric trackers match the current perceptual loss setting\n",
    "- **Safe Overrides**: You can safely change perceptual loss settings between continuation sessions\n",
    "\n",
    "**Important**: Continuation methods preserve model weights but create fresh optimizers (standard practice for fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aeeab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Pre-Continuation Weight Preservation Verification\n",
    "print(\"üîç === Model Weight Preservation Check ===\")\n",
    "\n",
    "if vae_model is not None:\n",
    "    print(\"Testing if model weights are preserved during continuation setup...\")\n",
    "    \n",
    "    # Get a test sample to establish baseline\n",
    "    test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "    test_inputs, test_targets = test_batch\n",
    "    \n",
    "    if isinstance(test_inputs, tuple):\n",
    "        test_field, test_coord = test_inputs\n",
    "        test_input = [test_field[:1], test_coord[:1]]\n",
    "        input_format = \"Fourier (field + coordinates)\"\n",
    "    else:\n",
    "        test_input = test_inputs[:1]\n",
    "        input_format = \"Standard (field only)\"\n",
    "    \n",
    "    print(f\"üìä Input format: {input_format}\")\n",
    "    \n",
    "    # Test prediction BEFORE any training operations\n",
    "    print(\"\\nüìä BEFORE continuation setup:\")\n",
    "    prediction_before = vae_model.predict(test_input, verbose=0)\n",
    "    mse_before = np.mean((test_targets[:1].numpy() - prediction_before) ** 2)\n",
    "    prediction_range_before = [prediction_before.min(), prediction_before.max()]\n",
    "    \n",
    "    print(f\"   MSE: {mse_before:.6f}\")\n",
    "    print(f\"   Prediction range: [{prediction_range_before[0]:.4f}, {prediction_range_before[1]:.4f}]\")\n",
    "    \n",
    "    # Store some layer weights for comparison\n",
    "    try:\n",
    "        layer_weights_before = []\n",
    "        weight_layer_names = []\n",
    "        for i, layer in enumerate(vae_model.layers):\n",
    "            if hasattr(layer, 'get_weights') and layer.get_weights():\n",
    "                weights = layer.get_weights()\n",
    "                if len(weights) > 0 and weights[0].size > 0:\n",
    "                    layer_weights_before.append(weights[0].copy())\n",
    "                    weight_layer_names.append(f\"Layer_{i}_{layer.name}\")\n",
    "                    if len(layer_weights_before) >= 3:  # Just store first 3 layers with weights\n",
    "                        break\n",
    "        \n",
    "        print(f\"   Stored weights from {len(layer_weights_before)} layers for comparison\")\n",
    "        \n",
    "        # Check if MSE indicates model is working correctly\n",
    "        if mse_before > 1.0:\n",
    "            print(\"‚ùå WARNING: High MSE detected! Model may have been reset or not properly loaded!\")\n",
    "            print(\"   Expected: Low MSE if weights were preserved from checkpoint\")\n",
    "            print(\"   Actual: High MSE suggests random/reset weights\")\n",
    "            print(f\"   Recommendation: Check checkpoint loading process\")\n",
    "        elif mse_before < 0.1:\n",
    "            print(\"‚úÖ EXCELLENT: Very low MSE suggests weights were properly loaded from checkpoint\")\n",
    "        else:\n",
    "            print(\"‚úÖ GOOD: Reasonable MSE suggests weights were loaded correctly\")\n",
    "        \n",
    "        # Store baseline for later comparison\n",
    "        baseline_mse = mse_before\n",
    "        baseline_weights = layer_weights_before\n",
    "        baseline_prediction_range = prediction_range_before\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not extract weights for comparison: {e}\")\n",
    "        baseline_mse = mse_before\n",
    "        baseline_weights = None\n",
    "        baseline_prediction_range = prediction_range_before\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No VAE model available for testing\")\n",
    "    baseline_mse = None\n",
    "    baseline_weights = None\n",
    "    baseline_prediction_range = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adad25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue VAE Training from Checkpoint\n",
    "# Set this to True if you want to continue training from the loaded checkpoint\n",
    "continue_vae_training = True  # Change to True to enable\n",
    "\n",
    "# Toggle for perceptual loss during continuation (can be different from original training)\n",
    "use_perceptual_loss_continuation = True  # Change to False to disable perceptual loss\n",
    "\n",
    "if continue_vae_training and vae_model is not None:\n",
    "    print(\"üîÑ Continuing VAE training from loaded checkpoint...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration for continued training\n",
    "    additional_epochs = 100\n",
    "    learning_rate = 1e-4  # Lower learning rate for fine-tuning\n",
    "    \n",
    "    print(f\"üìã Continuation Configuration:\")\n",
    "    print(f\"   - Additional epochs: {additional_epochs}\")\n",
    "    print(f\"   - Learning rate: {learning_rate} (reduced for fine-tuning)\")\n",
    "    print(f\"   - Perceptual loss: {'‚úÖ ENABLED' if use_perceptual_loss_continuation else '‚ùå DISABLED'}\")\n",
    "    print(f\"   - Model weights: ‚úÖ PRESERVED from checkpoint\")\n",
    "    \n",
    "    # Option 1: Continue training using the trainer (recommended)\n",
    "    # The trainer will handle optimizer state and callbacks properly\n",
    "    print(\"\\nüöÄ Method 1: Using FLRTrainer (Recommended)\")\n",
    "    \n",
    "    # Set the loaded model in the trainer\n",
    "    trainer.vae_model = vae_model\n",
    "    \n",
    "    # Continue training with reduced learning rate\n",
    "    continued_vae = trainer.continue_vae_training(\n",
    "        train_dataset=vae_train_dataset,\n",
    "        val_dataset=vae_test_dataset,\n",
    "        epochs=additional_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        patience=config['patience'],\n",
    "        reduce_lr_patience=config['reduce_lr_patience'],\n",
    "        use_perceptual_loss=use_perceptual_loss_continuation  # New option for perceptual loss\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ VAE training continuation completed!\")\n",
    "    \n",
    "elif continue_vae_training and vae_model is None:\n",
    "    print(\"‚ùå Cannot continue VAE training: No model loaded from checkpoint\")\n",
    "    print(\"   First load a model using the checkpoint loading cell above\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  VAE training continuation is disabled\")\n",
    "    print(\"   Set continue_vae_training = True to enable\")\n",
    "    if not continue_vae_training:\n",
    "        print(f\"   Current perceptual loss setting: {'‚úÖ ENABLED' if use_perceptual_loss_continuation else '‚ùå DISABLED'}\")\n",
    "        print(\"   (can be changed via use_perceptual_loss_continuation variable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Post-Continuation Weight Verification\n",
    "print(\"üîç === Post-Continuation Weight Verification ===\")\n",
    "\n",
    "if 'baseline_mse' in locals() and baseline_mse is not None and trainer.vae_model is not None:\n",
    "    print(\"Verifying that model weights were properly preserved during training continuation...\")\n",
    "    \n",
    "    # Get the same test sample used for baseline\n",
    "    test_batch = next(iter(vae_test_dataset.take(1)))\n",
    "    test_inputs, test_targets = test_batch\n",
    "    \n",
    "    if isinstance(test_inputs, tuple):\n",
    "        test_field, test_coord = test_inputs\n",
    "        test_input = [test_field[:1], test_coord[:1]]\n",
    "    else:\n",
    "        test_input = test_inputs[:1]\n",
    "    \n",
    "    # Test prediction AFTER continuation\n",
    "    print(\"\\nüìä AFTER continuation:\")\n",
    "    prediction_after = trainer.vae_model.predict(test_input, verbose=0)\n",
    "    mse_after = np.mean((test_targets[:1].numpy() - prediction_after) ** 2)\n",
    "    prediction_range_after = [prediction_after.min(), prediction_after.max()]\n",
    "    \n",
    "    print(f\"   MSE: {mse_after:.6f}\")\n",
    "    print(f\"   Prediction range: [{prediction_range_after[0]:.4f}, {prediction_range_after[1]:.4f}]\")\n",
    "    \n",
    "    # Compare with baseline\n",
    "    print(f\"\\nüìà Comparison with baseline:\")\n",
    "    print(f\"   Baseline MSE: {baseline_mse:.6f}\")\n",
    "    print(f\"   After MSE: {mse_after:.6f}\")\n",
    "    print(f\"   MSE change: {mse_after - baseline_mse:.6f}\")\n",
    "    \n",
    "    # Analyze results\n",
    "    if mse_after < baseline_mse:\n",
    "        improvement = ((baseline_mse - mse_after) / baseline_mse) * 100\n",
    "        print(f\"‚úÖ EXCELLENT: Model improved by {improvement:.2f}%!\")\n",
    "        print(\"   Training continuation was successful and preserved weights correctly\")\n",
    "    elif abs(mse_after - baseline_mse) < 0.01:\n",
    "        print(\"‚úÖ GOOD: Model performance maintained (minimal change)\")\n",
    "        print(\"   Weights were preserved correctly during continuation\")\n",
    "    elif mse_after > baseline_mse * 2:\n",
    "        print(\"‚ùå WARNING: Model performance degraded significantly!\")\n",
    "        print(\"   This may indicate:\")\n",
    "        print(\"   - Learning rate too high\")\n",
    "        print(\"   - Training instability\")\n",
    "        print(\"   - Possible weight corruption\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  Model performance changed moderately\")\n",
    "        print(\"   This is normal for continued training\")\n",
    "    \n",
    "    # Check for weight consistency if we stored baseline weights\n",
    "    if 'baseline_weights' in locals() and baseline_weights is not None:\n",
    "        try:\n",
    "            print(f\"\\nüîç Direct weight comparison:\")\n",
    "            current_weights = []\n",
    "            for i, layer in enumerate(trainer.vae_model.layers):\n",
    "                if hasattr(layer, 'get_weights') and layer.get_weights():\n",
    "                    weights = layer.get_weights()\n",
    "                    if len(weights) > 0 and weights[0].size > 0:\n",
    "                        current_weights.append(weights[0].copy())\n",
    "                        if len(current_weights) >= len(baseline_weights):\n",
    "                            break\n",
    "            \n",
    "            weights_changed = False\n",
    "            for i, (baseline_w, current_w) in enumerate(zip(baseline_weights, current_weights)):\n",
    "                if baseline_w.shape == current_w.shape:\n",
    "                    weight_diff = np.mean(np.abs(baseline_w - current_w))\n",
    "                    print(f\"   Layer {i} weight change: {weight_diff:.6f}\")\n",
    "                    if weight_diff > 0.001:  # Weights should change during training\n",
    "                        weights_changed = True\n",
    "            \n",
    "            if weights_changed:\n",
    "                print(\"‚úÖ CONFIRMED: Weights changed during training (expected)\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  WARNING: Weights didn't change much - training may not be effective\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not compare weights directly: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéØ FINAL ASSESSMENT:\")\n",
    "    if mse_after <= baseline_mse + 0.01:\n",
    "        print(\"‚úÖ Training continuation was SUCCESSFUL\")\n",
    "        print(\"   - Weights were preserved from checkpoint\")\n",
    "        print(\"   - Training improved or maintained model performance\")\n",
    "        print(\"   - Model is ready for inference or further training\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Training continuation had MIXED RESULTS\")\n",
    "        print(\"   - Weights were preserved, but performance may have degraded\")\n",
    "        print(\"   - Consider adjusting learning rate or training parameters\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot perform verification - baseline not available\")\n",
    "    print(\"Run the weight preservation check cell before training continuation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6048e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue FLRNet Training from Checkpoint\n",
    "# Set this to True if you want to continue FLRNet training from checkpoint\n",
    "continue_flrnet_training = False  # Change to True to enable\n",
    "\n",
    "if continue_flrnet_training:\n",
    "    print(\"üîÑ Continuing FLRNet training from checkpoint...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # First, ensure we have a VAE model (required for FLRNet)\n",
    "    if vae_model is None:\n",
    "        print(\"üîÑ Loading VAE model first (required for FLRNet)...\")\n",
    "        vae_model = trainer.load_vae_from_checkpoint(\n",
    "            checkpoint_dir=checkpoint_directory,\n",
    "            latent_dims=config['latent_dims'],\n",
    "            n_base_features=config['n_base_features'],\n",
    "            use_perceptual_loss=config['use_perceptual_loss'],\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    if vae_model is not None:\n",
    "        # Try to load existing FLRNet model\n",
    "        print(\"üîÑ Loading FLRNet model from checkpoint...\")\n",
    "        flrnet_model = trainer.load_flrnet_from_checkpoint(\n",
    "            n_sensors=config['n_sensors'],\n",
    "            checkpoint_dir=checkpoint_directory,\n",
    "            pretrained_vae=vae_model,\n",
    "            latent_dims=config['latent_dims'],\n",
    "            n_base_features=config['n_base_features'],\n",
    "            use_perceptual_loss=config['use_perceptual_loss'],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        if flrnet_model is not None:\n",
    "            # Configure training parameters for continuation\n",
    "            additional_epochs = 100  # How many more epochs to train\n",
    "            learning_rate = 1e-6     # Very low learning rate for fine-tuning\n",
    "            \n",
    "            print(f\"\\nüìã FLRNet Training Configuration:\")\n",
    "            print(f\"   - Additional epochs: {additional_epochs}\")\n",
    "            print(f\"   - Learning rate: {learning_rate}\")\n",
    "            print(f\"   - VAE model: ‚úÖ Loaded\")\n",
    "            print(f\"   - FLRNet model: ‚úÖ Loaded\")\n",
    "            \n",
    "            # Set the loaded model in the trainer\n",
    "            trainer.flr_model = flrnet_model\n",
    "            trainer.vae_model = vae_model\n",
    "            \n",
    "            # Continue training with reduced learning rate\n",
    "            continued_flrnet = trainer.train_flr_net(\n",
    "                train_dataset=flrnet_train_dataset,\n",
    "                val_dataset=flrnet_test_dataset,\n",
    "                n_sensors=config['n_sensors'],\n",
    "                epochs=additional_epochs,\n",
    "                learning_rate=learning_rate,\n",
    "                pretrained_vae=vae_model,\n",
    "                latent_dims=config['latent_dims'],\n",
    "                n_base_features=config['n_base_features'],\n",
    "                use_perceptual_loss=config['use_perceptual_loss'],\n",
    "                patience=config['patience'],\n",
    "                reduce_lr_patience=config['reduce_lr_patience']\n",
    "            )\n",
    "            \n",
    "            print(\"‚úÖ FLRNet training continuation completed!\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå Could not load FLRNet model from checkpoint\")\n",
    "            print(\"You may need to train FLRNet from scratch first\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not load VAE model - required for FLRNet training\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  FLRNet training continuation is disabled\")\n",
    "    print(\"Set continue_flrnet_training = True to enable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2328cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Manual Checkpoint Restoration with Optimizer State\n",
    "# This method provides more control over the restoration process\n",
    "use_advanced_restoration = False  # Change to True to enable\n",
    "\n",
    "if use_advanced_restoration:\n",
    "    print(\"üîß Advanced Checkpoint Restoration...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Configuration for restoration\n",
    "    restore_epoch = 50  # The epoch to restore from (if known)\n",
    "    target_total_epochs = 150  # Total epochs you want to reach\n",
    "    \n",
    "    print(f\"üìã Advanced Restoration Configuration:\")\n",
    "    print(f\"   - Restore from epoch: {restore_epoch}\")\n",
    "    print(f\"   - Target total epochs: {target_total_epochs}\")\n",
    "    print(f\"   - Additional epochs: {target_total_epochs - restore_epoch}\")\n",
    "    \n",
    "    # Method 1: TensorFlow's built-in checkpoint manager\n",
    "    def setup_checkpoint_manager(model, optimizer, checkpoint_dir):\n",
    "        \"\"\"Set up TensorFlow checkpoint manager for proper state restoration.\"\"\"\n",
    "        \n",
    "        # Create checkpoint object\n",
    "        checkpoint = tf.train.Checkpoint(\n",
    "            optimizer=optimizer,\n",
    "            model=model,\n",
    "            epoch=tf.Variable(0, dtype=tf.int64)\n",
    "        )\n",
    "        \n",
    "        # Create checkpoint manager\n",
    "        manager = tf.train.CheckpointManager(\n",
    "            checkpoint,\n",
    "            directory=str(checkpoint_dir / \"tf_checkpoints\"),\n",
    "            max_to_keep=3\n",
    "        )\n",
    "        \n",
    "        return checkpoint, manager\n",
    "    \n",
    "    # Example for VAE restoration\n",
    "    if vae_model is not None:\n",
    "        print(\"\\nüîß Setting up VAE checkpoint restoration...\")\n",
    "        \n",
    "        # Create optimizer (same as training)\n",
    "        vae_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=1e-5,  # Reduced for fine-tuning\n",
    "            beta_1=0.9, \n",
    "            beta_2=0.999\n",
    "        )\n",
    "        \n",
    "        # Compile model with optimizer\n",
    "        vae_model.compile(optimizer=vae_optimizer)\n",
    "        \n",
    "        # Set up checkpoint manager\n",
    "        vae_checkpoint, vae_manager = setup_checkpoint_manager(\n",
    "            vae_model, vae_optimizer, checkpoint_directory\n",
    "        )\n",
    "        \n",
    "        # Try to restore latest checkpoint\n",
    "        if vae_manager.latest_checkpoint:\n",
    "            vae_checkpoint.restore(vae_manager.latest_checkpoint)\n",
    "            restored_epoch = int(vae_checkpoint.epoch.numpy())\n",
    "            print(f\"‚úÖ Restored VAE from epoch {restored_epoch}\")\n",
    "            print(f\"üéØ Will continue training from epoch {restored_epoch + 1}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No TensorFlow checkpoint found, using loaded weights\")\n",
    "            restored_epoch = 0\n",
    "        \n",
    "        # Calculate remaining epochs\n",
    "        remaining_epochs = max(0, target_total_epochs - restored_epoch)\n",
    "        \n",
    "        if remaining_epochs > 0:\n",
    "            print(f\"\\nüöÄ Continuing VAE training for {remaining_epochs} more epochs...\")\n",
    "            \n",
    "            # Custom training loop with checkpoint saving\n",
    "            for epoch in range(remaining_epochs):\n",
    "                current_epoch = restored_epoch + epoch + 1\n",
    "                print(f\"\\nEpoch {current_epoch}/{target_total_epochs}\")\n",
    "                \n",
    "                # Train for one epoch\n",
    "                history = vae_model.fit(\n",
    "                    vae_train_dataset,\n",
    "                    validation_data=vae_test_dataset,\n",
    "                    epochs=1,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Update epoch counter\n",
    "                vae_checkpoint.epoch.assign(current_epoch)\n",
    "                \n",
    "                # Save checkpoint every 10 epochs\n",
    "                if current_epoch % 10 == 0:\n",
    "                    save_path = vae_manager.save()\n",
    "                    print(f\"üíæ Saved checkpoint: {save_path}\")\n",
    "                \n",
    "                # Early stopping logic (optional)\n",
    "                val_loss = history.history.get('val_loss', [0])[-1]\n",
    "                if val_loss < 0.001:  # Example threshold\n",
    "                    print(f\"üéØ Early stopping - validation loss {val_loss:.6f} is below threshold\")\n",
    "                    break\n",
    "            \n",
    "            print(\"‚úÖ Advanced VAE training continuation completed!\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è  Target epochs already reached\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No VAE model loaded for advanced restoration\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Advanced checkpoint restoration is disabled\")\n",
    "    print(\"Set use_advanced_restoration = True to enable\")\n",
    "    print(\"This method provides:\")\n",
    "    print(\"   ‚Ä¢ True optimizer state restoration\")\n",
    "    print(\"   ‚Ä¢ Exact epoch continuation\")\n",
    "    print(\"   ‚Ä¢ Custom training loop control\")\n",
    "    print(\"   ‚Ä¢ Proper checkpoint management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8935323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Load both VAE and FLRNet models at once using trainer\n",
    "# Uncomment this section if you want to load both models together\n",
    "\n",
    "# print(\"\\nüîÑ Alternative: Loading both VAE and FLRNet models...\")\n",
    "# vae_model, flrnet_model = trainer.load_models_from_checkpoint(\n",
    "#     n_sensors=config['n_sensors'],\n",
    "#     checkpoint_dir=checkpoint_directory,\n",
    "#     latent_dims=config['latent_dims'],\n",
    "#     n_base_features=config['n_base_features'],\n",
    "#     use_perceptual_loss=config['use_perceptual_loss'],\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# if vae_model is not None and flrnet_model is not None:\n",
    "#     print(\"üéâ Both models loaded successfully!\")\n",
    "#     print(\"üìà Models are ready for inference and visualization\")\n",
    "# elif vae_model is not None:\n",
    "#     print(\"‚ö†Ô∏è  Only VAE model loaded successfully\")\n",
    "# elif flrnet_model is not None:\n",
    "#     print(\"‚ö†Ô∏è  Only FLRNet model loaded successfully\") \n",
    "# else:\n",
    "#     print(\"‚ùå Failed to load both models\")\n",
    "\n",
    "# Example: Load FLRNet separately (uncomment if needed)\n",
    "# print(\"\\nüîÑ Loading FLRNet model separately...\")\n",
    "# flrnet_model = trainer.load_flrnet_from_checkpoint(\n",
    "#     n_sensors=config['n_sensors'],\n",
    "#     checkpoint_dir=checkpoint_directory,\n",
    "#     pretrained_vae=vae_model,  # Use the VAE we just loaded\n",
    "#     latent_dims=config['latent_dims'],\n",
    "#     n_base_features=config['n_base_features'],\n",
    "#     use_perceptual_loss=config['use_perceptual_loss'],\n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16712b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced VAE Model Inference and Visualization\n",
    "if vae_model is not None:\n",
    "    print(\"üé® Performing enhanced VAE inference and visualization...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get multiple test samples for comprehensive evaluation\n",
    "    test_samples = 4\n",
    "    test_batch = next(iter(vae_test_dataset.batch(test_samples).take(1)))\n",
    "    test_inputs, test_targets = test_batch\n",
    "    \n",
    "    # Handle different input formats (Fourier vs Standard VAE)\n",
    "    if isinstance(test_inputs, (list, tuple)) and len(test_inputs) == 2:\n",
    "        # Fourier VAE expects [field, coordinates]\n",
    "        test_fields, test_coords = test_inputs\n",
    "        predictions = vae_model([test_fields, test_coords])\n",
    "        print(f\"üåä Using Fourier VAE with coordinate inputs\")\n",
    "        print(f\"   - Field input shape: {test_fields.shape}\")\n",
    "        print(f\"   - Coordinate input shape: {test_coords.shape}\")\n",
    "    else:\n",
    "        # Standard VAE expects just field data\n",
    "        predictions = vae_model(test_inputs)\n",
    "        test_fields = test_inputs\n",
    "        print(f\"üîÑ Using Standard VAE\")\n",
    "        print(f\"   - Input shape: {test_inputs.shape}\")\n",
    "    \n",
    "    print(f\"   - Output shape: {predictions.shape}\")\n",
    "    \n",
    "    # Convert to numpy for analysis\n",
    "    test_targets_np = test_targets.numpy()\n",
    "    predictions_np = predictions.numpy()\n",
    "    test_fields_np = test_fields.numpy()\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    mse_per_sample = np.mean((test_targets_np - predictions_np)**2, axis=(1,2,3))\n",
    "    mae_per_sample = np.mean(np.abs(test_targets_np - predictions_np), axis=(1,2,3))\n",
    "    \n",
    "    overall_mse = np.mean(mse_per_sample)\n",
    "    overall_mae = np.mean(mae_per_sample)\n",
    "    \n",
    "    print(f\"üìä Model Performance Metrics:\")\n",
    "    print(f\"   - Overall MSE: {overall_mse:.6f}\")\n",
    "    print(f\"   - Overall MAE: {overall_mae:.6f}\")\n",
    "    print(f\"   - Best sample MSE: {np.min(mse_per_sample):.6f}\")\n",
    "    print(f\"   - Worst sample MSE: {np.max(mse_per_sample):.6f}\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(3, test_samples, figsize=(4*test_samples, 12))\n",
    "    \n",
    "    for i in range(test_samples):\n",
    "        # Original field\n",
    "        im1 = axes[0, i].imshow(test_targets_np[i, :, :, 0], cmap='RdBu_r', aspect='equal', origin='lower')\n",
    "        axes[0, i].set_title(f'Original Field {i+1}', fontsize=12)\n",
    "        axes[0, i].axis('off')\n",
    "        plt.colorbar(im1, ax=axes[0, i], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Reconstructed field\n",
    "        im2 = axes[1, i].imshow(predictions_np[i, :, :, 0], cmap='RdBu_r', aspect='equal', origin='lower')\n",
    "        axes[1, i].set_title(f'Reconstructed Field {i+1}', fontsize=12)\n",
    "        axes[1, i].axis('off')\n",
    "        plt.colorbar(im2, ax=axes[1, i], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Error field\n",
    "        error = np.abs(test_targets_np[i, :, :, 0] - predictions_np[i, :, :, 0])\n",
    "        im3 = axes[2, i].imshow(error, cmap='hot', aspect='equal', origin='lower')\n",
    "        axes[2, i].set_title(f'Error Field {i+1}\\nMSE: {mse_per_sample[i]:.4f}', fontsize=12)\n",
    "        axes[2, i].axis('off')\n",
    "        plt.colorbar(im3, ax=axes[2, i], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Add row labels\n",
    "    axes[0, 0].text(-0.2, 0.5, 'Original', transform=axes[0, 0].transAxes, \n",
    "                    rotation=90, va='center', ha='center', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].text(-0.2, 0.5, 'Reconstructed', transform=axes[1, 0].transAxes, \n",
    "                    rotation=90, va='center', ha='center', fontsize=14, fontweight='bold')\n",
    "    axes[2, 0].text(-0.2, 0.5, 'Error', transform=axes[2, 0].transAxes, \n",
    "                    rotation=90, va='center', ha='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'VAE Model Performance - {test_samples} Test Samples\\nOverall MSE: {overall_mse:.6f}, MAE: {overall_mae:.6f}', \n",
    "                 fontsize=16, y=0.98)\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(f\"\\nüìà Statistical Analysis:\")\n",
    "    print(f\"   - Target field range: [{test_targets_np.min():.4f}, {test_targets_np.max():.4f}]\")\n",
    "    print(f\"   - Prediction range: [{predictions_np.min():.4f}, {predictions_np.max():.4f}]\")\n",
    "    print(f\"   - Prediction std: {predictions_np.std():.4f}\")\n",
    "    print(f\"   - Target std: {test_targets_np.std():.4f}\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    correlation = np.corrcoef(test_targets_np.flatten(), predictions_np.flatten())[0, 1]\n",
    "    print(f\"   - Correlation coefficient: {correlation:.4f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ VAE model evaluation complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot perform inference - VAE model not loaded\")\n",
    "    print(\"Please run the model loading cell above first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05ad50",
   "metadata": {},
   "source": [
    "## Workflow Summary\n",
    "\n",
    "This notebook has successfully completed a comprehensive flow field reconstruction training and validation workflow:\n",
    "\n",
    "### ‚úÖ Completed Tasks:\n",
    "\n",
    "1. **Configuration Management**: Used `ConfigManager` for robust configuration handling\n",
    "2. **Dataset Loading**: Created coordinate-aware datasets for Fourier features\n",
    "3. **VAE Training**: Trained VAE with proper coordinate input for Fourier features\n",
    "4. **VAE Validation**: Comprehensive testing and visualization of VAE performance\n",
    "5. **FLRNet Training**: Trained sensor-to-field reconstruction model\n",
    "6. **FLRNet Validation**: Evaluated FLRNet performance with sensor position overlays\n",
    "7. **Model Saving**: Saved trained models for future use\n",
    "\n",
    "### üîß Key Fixes Applied:\n",
    "\n",
    "- **Fourier Bug Fix**: Resolved coordinate input issues for Fourier-aware VAE\n",
    "- **Dataset Coordinate Integration**: Added proper coordinate grid generation\n",
    "- **Robust Error Handling**: Added validation and error checking throughout\n",
    "- **Comprehensive Visualization**: Created detailed analysis plots and metrics\n",
    "\n",
    "### üìä Results:\n",
    "\n",
    "The notebook now provides a complete, debugged workflow for training and validating both VAE and FLRNet models with optional Fourier features. All models are properly coordinate-aware and ready for physics-informed machine learning applications.\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "- Models are saved and ready for inference\n",
    "- Configuration can be easily changed for different sensor layouts\n",
    "- Fourier features are properly integrated and tested\n",
    "- Framework is extensible for additional physics-informed constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
